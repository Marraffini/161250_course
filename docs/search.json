[
  {
    "objectID": "workshops/ws08.html",
    "href": "workshops/ws08.html",
    "title": "Chapter 8 Workshop",
    "section": "",
    "text": "Dataset alfalfa\nThis dataset is from the R package faraway. The alfalfa dataset frame has 25 rows and 4 columns. Data comes from an experiment to test the effects of seed inoculum, irrigation and shade on alfalfa yield.\nThis data frame contains the following columns:\nshade - Distance of location from tree line divided into 5 shade areas\nirrigation - Irrigation effect divided into 5 levels\ninoculum - Four types of seed incolum, A-D with E as control\nyield - Dry matter yield of alfalfa\n\n\nCode\nlibrary(tidyverse)\n\ndata(alfalfa, package=\"faraway\")\n\nhead(alfalfa, 25)\n\n\n\n\nExercise 8.1\nObtain the main effects and interaction plots.\nOld style Main effects plots:\n\n\nCode\nmod1 &lt;- aov(yield ~ shade + irrigation + inoculum, \n            data=alfalfa)\n\nlibrary(effects)\n\nplot(allEffects(mod1))\n\n\nOld style Interaction effects plots:\n\n\nCode\nmod2 &lt;- aov(\n  yield ~ shade*irrigation*inoculum - shade:irrigation:inoculum, \n  data = alfalfa\n  )\n\n\nlibrary(effects)\n\nmod1 &lt;- lm(yield ~ shade * irrigation, data = alfalfa) \n\neffect('shade:irrigation', \n       mod = mod1) |&gt; \n  plot(multiline = TRUE)\n\n\nCode\nmod2 &lt;- lm(yield ~ shade * inoculum, data = alfalfa) \n\neffect('shade:inoculum',\n       mod=mod2) |&gt; \n  plot(multiline=TRUE)\n\n\nCode\nmod3 &lt;- lm(yield ~ irrigation * inoculum, data = alfalfa) \n\neffect('irrigation:inoculum',\n       mod=mod3) |&gt; \n  plot(multiline=TRUE)\n\n\nggplot2 can produce good main effects and interaction plots but the R codes for this task are not short. For main effects plot-\n\n\nCode\nlibrary(ggplot2)\n\nplot1 &lt;- ggplot(alfalfa) + \n  aes(x = shade, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) + \n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of shade\")\n\nplot2 &lt;- ggplot(alfalfa) +\n  aes(x = irrigation, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) +\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of irrigation\")\n\nplot3 &lt;- ggplot(alfalfa) +\n  aes(x = inoculum, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) + \n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of inoculum\")\n\nlibrary(patchwork)\nplot1+plot2+plot3\n\n\nFor interaction plot-\n\n\nCode\n#Interactions Plot\nplot4 &lt;- ggplot(alfalfa) +\n  aes(x = shade, y = yield, \n      group = irrigation, colour = irrigation) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"shade*irrigation interaction\")\n\nplot5 &lt;- ggplot(alfalfa) +\n  aes(x = inoculum, y = yield,\n      group = irrigation, colour = irrigation) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"inoculum*irrigation interaction\")\n\nplot6 &lt;- ggplot(alfalfa) +\n  aes(x = shade, y = yield, \n      group = inoculum, colour = inoculum) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"shade*inoculum interaction\")\n\nplot4 / plot5 / plot6\n\n\n\n\nExercise 8.2\nFit one-way ANOVA models to this dataset.\n\n\nCode\nanova1 &lt;- aov(yield ~ shade, data = alfalfa)\nsummary(anova1)\n\nanova2 &lt;- aov(yield ~ irrigation, data = alfalfa)\nsummary(anova2)\n\nanova3 &lt;- aov(yield ~ inoculum, data = alfalfa)\nsummary(anova3)\n\nplot(TukeyHSD(anova1))\n\n\nCode\nplot(TukeyHSD(anova2))\n\n\nCode\nplot(TukeyHSD(anova3))\n\n\n\n\nExercise 8.3\nFit a three-factor (additive) ANOVA model without interactions.\n\n\nCode\nanova1 &lt;- aov(yield ~ shade + irrigation + inoculum,\n              data = alfalfa)\n\nsummary(anova1)\n\nlibrary(ggfortify)\nautoplot(anova1, 1)\n\n\nFor the base R style four diagnostic plots, use plot(anova1) and set the par.\n\n\nCode\npar(mfrow=c(2,2))\nplot(anova1)\n\n\n\n\nExercise 8.4\nFit the indicator variable regression model of yield ~ inoculum\n\n\nCode\nalfalfa &lt;- alfalfa |&gt; \n  mutate(\n    I.A = as.numeric(inoculum==\"A\"),\n    I.B = as.numeric(inoculum==\"B\"),\n    I.C = as.numeric(inoculum==\"C\"),\n    I.D = as.numeric(inoculum==\"D\")\n  )\n\nindicator.reg &lt;- lm(yield ~ I.A + I.B + I.C + I.D, \n                    data = alfalfa)\n\nsummary(indicator.reg)\n\n\nNote that this regression allows the treatments to be compared with the control.\n\n\nExercise 8.5\nshade is a categorical variable of factor codes but let us (incorrectly) treat it as numerical (and if the actual distances are given, then the distance variable becomes a covariate). Fit ANCOVA of yield on the inoculum factor and shade covariate.\nR:\n\n\nCode\nancova.model &lt;- lm(yield ~ as.numeric(shade) * inoculum, \n                   data = alfalfa)\nsummary(ancova.model)\n\n\n\n\nExercise 8.6\nIn a two-factor experiment, one of the factors was assigned to main plot (main-plot factor), the second factor, called the subplot factor, was assigned into subplots. The dataset https://www.massey.ac.nz/~kgovinda/data/plots.RData gives the experimental set up. Perform the ANOVA for this basic split-plot experiment.\nR:\n\n\nCode\nurl1 &lt;- \"https://www.massey.ac.nz/~anhsmith/data/plots.RData\"\ndownload.file(url = url1, destfile = \"plots.RData\")\nload(\"plots.RData\")\n\nplots\n\n\n\n\nCode\nsp.model &lt;- aov(yield ~ block + A*B + Error(block/A),\n                data=plots)\nsummary(sp.model)\n\n\n\n\nCode\n#Incorrect model\nsummary(aov(yield ~ block + A*B ,\n            data=plots))\n\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws06.html",
    "href": "workshops/ws06.html",
    "title": "Chapter 6 Workshop",
    "section": "",
    "text": "Dataset Prestige\nWe will continue to use dataset Prestige from the car R package.\n\n\nExercise 6.1\nFit a resistant line of prestige against education. Show the fitted line on a scatterplot of prestige ~ education. Obtain the plot of the residuals against fitted values.\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\n\nx &lt;- Prestige$education\ny &lt;- Prestige$prestige\n\n\nMedian-median line Tukey line()\n\n\nCode\nline(x,y)\n\n\n\n\nPlotting Tukey line & residuals\n\n\nCode\nTline &lt;- line(x,y)\n\nPrestigeT &lt;- Prestige |&gt; \n  mutate(Tline.residuals=residuals(Tline),\n         Tline.fits=fitted(Tline)\n         ) \n\np1 &lt;- PrestigeT |&gt; \n  ggplot() +\n  aes(x=education, y=prestige) + \n  geom_point() +\n  geom_path(aes(x=education, y=Tline.fits))\n\np2 &lt;- PrestigeT |&gt; \n  ggplot() +\n  aes(y=Tline.residuals, x=Tline.fits) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\nlibrary(patchwork)\n\np1+p2\n\n\n\n\nCode\n# Old style plotting\nx &lt;- Prestige$education\ny &lt;- Prestige$prestige\n\nplot(y ~ x) \ntitle(main = \"Scatterplot prestige ~ education\")\nz = line(x,y)  # slightly different Rline\nabline(coef(z), lty=2, col= \"blue\")\nlegend(\"topleft\", legend =\"Resistant (Tukey) line\", \n       lty=2, col = \"blue\")\n\n\nCode\nplot(residuals(z) ~ fitted(z))\n\n\nThe robust regression (which is similar to the robust line) function in the MASS package along with the visreg package is done with the following codes:\n\n\nCode\nlibrary(visreg)\nlibrary(car)\nlibrary(MASS)\n\nfit1 &lt;- rlm(prestige ~ education, data = Prestige)\nvisreg(fit1)\n\n\n\n\nExercise 6.2\nFit a simple regression line of prestige against education. Show the fitted regression line on a scatterplot of prestige ~ education. Obtain the plot of the residuals against fitted values.\n\n\nCode\nx &lt;- Prestige$education\ny &lt;- Prestige$prestige\n\nsimplereg &lt;- lm(y~x) # Fits the simple regression\n\nsummary(simplereg)\n\nanova(simplereg)\n\n\n\n\nCode\nPrestigeReg &lt;- Prestige |&gt; \n  mutate(Residuals=residuals(simplereg), \n         Fits=fitted(simplereg))\n\nPrestigeReg |&gt; \n  ggplot() + \n  aes(x=education, y=prestige) + \n  geom_point() + \n  geom_line(aes(x=education, y=Fits))\n\n\nCode\nPrestigeReg |&gt; \n  ggplot() + \n  aes(y=Residuals, x=Fits) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\n\nCode\n# Old style plotting\nplot(y ~ x) \nabline(simplereg, lty=1, col = \"blue\")\nabline(h=mean(y), lty=2)\nabline(v=mean(x), lty=2)\nlegend(\"topleft\", legend =c(\"Regression line\"), \n       lty=1, col = \"blue\")\n\n\nCode\nFitted = fitted.values(simplereg)\nResiduals = residuals(simplereg)\nplot(Fitted, Residuals, xlab = \"Fitted Values\")\nabline(h = 0)\ntitle(\"Residuals Vs Fitted Values\")\n\n\nUsing the visreg package.\n\n\nCode\nfit1&lt;- lm(prestige ~ education, data = Prestige)\nvisreg(fit1)\n\n\nNote:\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws04.html",
    "href": "workshops/ws04.html",
    "title": "Chapter 4 Workshop",
    "section": "",
    "text": "We will again be using a well-known dataset called Prestige from the car R package. This dataset deals with prestige ratings of Canadian Occupations. The Prestige dataset has 102 rows and 6 columns. The observations are occupations.\nThis data frame contains the following columns:\n\neducation - Average education of occupational incumbents, years, in 1971.\nincome - Average income of incumbents, dollars, in 1971.\nwomen - Percentage of incumbents who are women.\nprestige - Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.\ncensus - Canadian Census occupational code.\ntype - Type of occupation. A factor with levels: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar. (includes four missing values).\n\n\n\nPerform a one-sample t-test to test the hypothesis that the true mean prestige is exactly 50.\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\ndata(Prestige)\n\n# Alternative hyp: greater or less than 50\nt.test(Prestige$prestige, mu=50)\n\n# Alternative hyp: greater than 50\nt.test(Prestige$prestige, mu=50, alternative=\"greater\")\n\n\n\n\n\nTest whether the true mean prestige score for professionals is 50% more than the true mean prestige score for white collar occupations.\n\n\nCode\nprof.data = Prestige |&gt; \n  filter(type==\"prof\") |&gt; \n  pull(prestige)\n\nwc.data = Prestige |&gt; \n  filter(type==\"wc\") |&gt; \n  pull(prestige) \n\nt.test(prof.data, \n       wc.data, \n       mu = 0.5 * mean(wc.data),\n       alternative = 'greater')\n\n\n\n\n\nExplore the skewness in the income variable using a boxplot, mids-vs-spread plot. Compute the D-Statistics. Obtain a suitable power transformation to correct the skewness. Compute the 95% confidence interval for the true mean Top measurement using the raw and transformed data.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(income) + \n  geom_boxplot() \n\n\nCode\n# or\nboxplot(Prestige$income, horizontal = TRUE)\n\n\n\n\nCode\n# D-Stat codes under a few shrinking transformations\n\nD1 = function(x) {\n(mean(x) - median(x)) / sd(x)\n}\n\nD2 = function(x) {\n(mean(x) - median(x)) / (fivenum(x)[4] - fivenum(x)[2])\n}\n\nD3 = function(x) {\n((fivenum(x)[4] + fivenum(x)[2]) / 2+-median(x)) / (fivenum(x)[4] - fivenum(x)[2])\n}\n\nx = Prestige$income\n\nVMat &lt;- cbind(\n  Vreci = -1 / x,\n  V = x,\n  VSq = sqrt(x),\n  VLog = log(x)\n  )\n\napply(VMat, 2, D1)\napply(VMat, 2, D2)\napply(VMat, 2, D3)\n\n# or obtain the D-stats individually\nD1(sqrt(x)); D2(sqrt(x)); D3(sqrt(x))\n\n\n\n\nCode\nlibrary(lindia)\n\ngg_boxcox(lm(x ~ 1))\n\n\nCode\n# or\nrequire(MASS)\nb &lt;- boxcox(x ~ 1)\ntitle(\"Log-likelihood curve of boxcox parameter\")\nk &lt;- b$x[which.max(b$y)]\nmtext(paste(\"optimum power=\", formatC(k)))\n\n\n\n\nCode\nt.test(x)\nt.test(log(x))\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws04.html#exercise-4.1",
    "href": "workshops/ws04.html#exercise-4.1",
    "title": "Chapter 4 Workshop",
    "section": "",
    "text": "Perform a one-sample t-test to test the hypothesis that the true mean prestige is exactly 50.\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\ndata(Prestige)\n\n# Alternative hyp: greater or less than 50\nt.test(Prestige$prestige, mu=50)\n\n# Alternative hyp: greater than 50\nt.test(Prestige$prestige, mu=50, alternative=\"greater\")"
  },
  {
    "objectID": "workshops/ws04.html#exercise-4.2",
    "href": "workshops/ws04.html#exercise-4.2",
    "title": "Chapter 4 Workshop",
    "section": "",
    "text": "Test whether the true mean prestige score for professionals is 50% more than the true mean prestige score for white collar occupations.\n\n\nCode\nprof.data = Prestige |&gt; \n  filter(type==\"prof\") |&gt; \n  pull(prestige)\n\nwc.data = Prestige |&gt; \n  filter(type==\"wc\") |&gt; \n  pull(prestige) \n\nt.test(prof.data, \n       wc.data, \n       mu = 0.5 * mean(wc.data),\n       alternative = 'greater')"
  },
  {
    "objectID": "workshops/ws04.html#exercise-4.3",
    "href": "workshops/ws04.html#exercise-4.3",
    "title": "Chapter 4 Workshop",
    "section": "",
    "text": "Explore the skewness in the income variable using a boxplot, mids-vs-spread plot. Compute the D-Statistics. Obtain a suitable power transformation to correct the skewness. Compute the 95% confidence interval for the true mean Top measurement using the raw and transformed data.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(income) + \n  geom_boxplot() \n\n\nCode\n# or\nboxplot(Prestige$income, horizontal = TRUE)\n\n\n\n\nCode\n# D-Stat codes under a few shrinking transformations\n\nD1 = function(x) {\n(mean(x) - median(x)) / sd(x)\n}\n\nD2 = function(x) {\n(mean(x) - median(x)) / (fivenum(x)[4] - fivenum(x)[2])\n}\n\nD3 = function(x) {\n((fivenum(x)[4] + fivenum(x)[2]) / 2+-median(x)) / (fivenum(x)[4] - fivenum(x)[2])\n}\n\nx = Prestige$income\n\nVMat &lt;- cbind(\n  Vreci = -1 / x,\n  V = x,\n  VSq = sqrt(x),\n  VLog = log(x)\n  )\n\napply(VMat, 2, D1)\napply(VMat, 2, D2)\napply(VMat, 2, D3)\n\n# or obtain the D-stats individually\nD1(sqrt(x)); D2(sqrt(x)); D3(sqrt(x))\n\n\n\n\nCode\nlibrary(lindia)\n\ngg_boxcox(lm(x ~ 1))\n\n\nCode\n# or\nrequire(MASS)\nb &lt;- boxcox(x ~ 1)\ntitle(\"Log-likelihood curve of boxcox parameter\")\nk &lt;- b$x[which.max(b$y)]\nmtext(paste(\"optimum power=\", formatC(k)))\n\n\n\n\nCode\nt.test(x)\nt.test(log(x))\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws02.html",
    "href": "workshops/ws02.html",
    "title": "Chapter 2 Workshop",
    "section": "",
    "text": "Dataset Prestige\nWe will be using a well-known dataset called Prestige from the car R package. This dataset deals with prestige ratings of Canadian Occupations. The Prestige dataset has 102 rows and 6 columns. The observations are occupations.\nThis data frame contains the following columns:\n\neducation - Average education of occupational incumbents, years, in 1971.\nincome - Average income of incumbents, dollars, in 1971.\nwomen - Percentage of incumbents who are women.\nprestige - Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.\ncensus - Canadian Census occupational code.\ntype - Type of occupation. A factor with levels: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar. (includes four missing values).\n\nLoad the data:\n\n\nCode\nlibrary(car)\ndata(Prestige)\n\n\n\n\nExercise 2.1\nDraw a bar chart for type:\n\n\nCode\nlibrary(tidyverse)\n\np &lt;- Prestige |&gt; \n  ggplot() +\n  aes(type) + \n  geom_bar()\n\np\n\n\nOr with plotly (which works for HTML, not for PDF)\n\n\nCode\nlibrary(plotly)\n\nggplotly(p)\n\n\n\n\n\n\nOr with old-style R plot\n\n\nCode\n# or\nlibrary(car)\nbarplot(table(Prestige$type))\n\n\n\n\nExercise 2.2\nDraw a histogram of prestige.\nBelow demonstrates the flexibility of ggplot code. You can specify the data argument by piping it into ggplot, or by putting it as an argument to ggplot or a geom_. Likewise, the aes information, which determines which variables are used where, can be added as an extra line or specified inside the ggplot or geom_ function.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(x = prestige) +\n  geom_histogram(bins=10)\n\n\nCode\nggplot(Prestige) +\n  aes(x = prestige) +\n  geom_histogram(bins=10) \n\n\nCode\nggplot() +\n  geom_histogram(\n    data = Prestige,\n    mapping = aes(x = prestige),\n    bins = 10\n    )\n\n\nCode\n# or\n# library(plotly)\n# p &lt;- Prestige |&gt; \n#   ggplot() +\n#   aes(prestige) +\n#   geom_histogram(bins=10)\n# \n# ggplotly(p)\n\n# or\n# hist(Prestige$prestige)\n\n\n\n\nCode\nPrestige |&gt;\n  ggplot() + \n  aes(x = rownames(Prestige), y = prestige) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\nWhat a mess!\nWe can tidy it up by ordering the professions according to prestige. First, we move the professions from rownames to a variable. Then, we fct_reorder the professions using prestige.\nLook at Figure 1.\n\n\nCode\nPrestige |&gt; \n  rownames_to_column(var = \"profession\") |&gt; \n  mutate(\n    profession = fct_reorder(profession, prestige)\n    ) |&gt;\n  ggplot() + \n  aes(x = profession, y = prestige, colour = type) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\nFigure 1: a dot plot\n\n\n\n\n\n\nExercise 2.3\nObtain the summary statistics for prestige:\n\n\nCode\nsummary(Prestige)\n\nlibrary(psych)\n\ndescribe(Prestige)\n\ndescribeBy(education + income + women + prestige ~ type, \n           data = Prestige)\n\n\n\n\nExercise 2.4\nObtain the boxplot of prestige ~ type:\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(y=prestige, x=type) +\n  geom_boxplot()\n\n\nCode\n# or\n# library(plotly)\n# p &lt;- Prestige |&gt; ggplot() + \n#   aes(y=prestige, x=type) + geom_boxplot()\n# ggplotly(p)\n\n# or\n# library(lattice)\n# bwplot(prestige ~ type, data=Prestige)\n\n# as violin plots\nPrestige |&gt; \n  ggplot() +\n  aes(y=prestige, x=type) +\n  geom_violin()\n\n\nCode\n# Or put it all together\nPrestige |&gt; \n  ggplot() +\n  aes(y=prestige, x=type) +\n  geom_violin() + \n  geom_boxplot(col = 2, alpha = .2) +\n  geom_jitter(alpha = .2, width = .2, height = 0, colour = 4)\n\n\n\n\nExercise 2.5\nObtain the Empirical Cumulative Distribution Function (ECDF) graphs of prestige ~ type:\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(prestige, colour=type) +\n  stat_ecdf()\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(prestige) +\n  stat_ecdf() + \n  facet_wrap(~type)\n\n\nCode\n# or\nlibrary(latticeExtra)\necdfplot(~ prestige | type, data = Prestige)\n\n\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(\n    x = prestige, # these aes settings are used\n    col = type    # by both geoms\n    ) +\n  geom_density(\n    aes(fill = type), # the 'fill' aes goes here because \n    alpha = .2        # geom_rug doesn't use 'fill'\n    ) +\n  geom_rug()\n\n\n\n\nExercise 2.6\nObtain the {0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95} quantiles of prestige:\n\n\nCode\npr &lt;- c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)\n\nPrestige |&gt; \n  summarise(\n    probs = pr,\n    quants = quantile(prestige, pr)\n    )\n\n# or simply\nquantile(Prestige$prestige, pr)\n\n\n\n\nExercise 2.7\nObtain the scatter plot (with and without marginal boxplots) prestige vs. education :\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- Prestige |&gt; \n  ggplot() + \n  aes(x=education, y=prestige) +\n  geom_point() + \n  geom_smooth(col = 2) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\nggMarginal(p1, type=\"boxplot\")\n\n\n\n\nCode\nlibrary(car)\n\nscatterplot(education ~ prestige, data = Prestige)\n\n\nThe later plot will show prediction interval ribbon while the first plot will show the confidence interval ribbon.\n\n\nExercise 2.8\nObtain the bubble or balloon plot prestige vs. education vs. income (income forming the bubble size):\n\n\nCode\nlibrary(ggplot2)\n\nPrestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige, size = income) +\n  geom_point()\n\n\nCode\n# or\n\nlibrary(plotly)\n\np &lt;- Prestige |&gt;\n  ggplot() + \n  aes(x = education, y = prestige, size = income) +\n  geom_point()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nExercise 2.9\nObtain the contour plot prestige vs. education vs. income :\n\n\nCode\nlibrary(plotly)\n\nplot_ly(type = 'contour', \n        x = Prestige$education, \n        y = Prestige$income, \n        z = Prestige$prestige)\n\n\nTo add axes labels and titles, try-\n\n\nCode\nlibrary(plotly)\n\nplot_ly(\n  Prestige,\n  type = 'contour',\n  x = Prestige$education,\n  y = Prestige$income,\n  z = Prestige$prestige\n) |&gt; layout(\n  title = 'Contour Plot of prestige scores',\n  xaxis = list(title = 'education'),\n  yaxis = list(title = 'income')\n)\n\n\nWe can also define our own function for the contour approximation.\n\n\nCode\nlibrary(modelr)\n\n# make a smooth model\ny.m = loess(prestige ~ education * income, data = Prestige)\n\n# make a regular grid of all combinations of education and income\nmygrid &lt;- Prestige |&gt; \n  data_grid(\n    education = seq_range(education, 50),\n    income = seq_range(income, 50)\n  ) |&gt; \n  # add predicted prestige using the smooth model\n  add_predictions(y.m, var = \"predicted prestige\")\n\n# make ggplot contour plot\np &lt;- mygrid |&gt; \n  ggplot() + \n  aes(x = education, y = income, z = `predicted prestige`) +\n  geom_contour()\n\np\n\n\nCode\n# make a plotly version\nlibrary(plotly)\nggplotly(p)\n\n# filled contour ggplot\nmygrid |&gt; \n  ggplot() + \n  aes(x=education, y=income, z=`predicted prestige`) +\n  stat_contour_filled()\n\n\nCode\n# or the older-style lattice graphs \n\nlibrary(lattice)\n\ncontourplot(`predicted prestige` ~ education * income, \n            data = mygrid, \n            cuts = 10, region = TRUE,\n            xlab = \"education \", ylab = \"income \")\n\n\nCode\nwireframe(`predicted prestige` ~ education * income, \n          data = mygrid,  \n          cuts = 10, region = TRUE, \n          xlab = \"education \", ylab = \"income \")\n\n\nCode\nlevelplot(`predicted prestige` ~ education * income, \n          data = mygrid,  \n          cuts = 10, region = TRUE, \n          xlab = \"education \", ylab = \"income \")\n\n\nCode\ncloud(`predicted prestige` ~ income * education, \n      data = mygrid) \n\n\n\n\nExercise 2.10\nObtain the 3-D plot prestige vs. education vs. income :\n\n\nCode\nlibrary(car)\n\nscatter3d(prestige ~ education + income, \n          data = Prestige)\n\n\n\n\nExercise 2.11\nCreate prestige ~ education | type graphs. That is, prestige ~ education grouped by type as colours and/or panels.\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige, color = type) +\n  geom_point() + \n  facet_wrap(~ type)\n\n\nCode\n# or\n# library(plotly)\n#\n# p &lt;- Prestige |&gt; \n#   ggplot() + \n#   aes(x = education, y = prestige, color = type) +\n#   geom_point() + \n#   facet_wrap(~ type)\n# \n# ggplotly(p)\n\n\n\n\nCode\np &lt;- Prestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige, color = type) +\n  geom_point()\n\np\n\n\nCode\n# OR\n#\n# library(plotly)\n# ggplotly(p)\n\n\n\n\nCode\nscatterplot(prestige ~ education | type, \n            data=Prestige)\n\n\n\n\nCode\nxyplot(prestige ~ education | type, \n       auto.key = TRUE, \n       data = Prestige)\n\n\n\n\nCode\nxyplot(prestige ~ education, \n       group = type,\n       auto.key = TRUE, \n       data = Prestige)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.12\nTime-series data EDA based on RBNZ house sales data.\n\n\nCode\nurl1 &lt;- \"https://www.massey.ac.nz/~anhsmith/data/housesales.RData\"\ndownload.file(url = url1, destfile = \"housesales.RData\")\nload(\"housesales.RData\")\n\n\n\n\nCode\nlibrary(forecast)\nautoplot(housesales)\n\n\n\n\nCode\n# Dynamic graphing\n# https://rstudio.github.io/dygraphs/index.html\n\nlibrary(dygraphs)\n\ndygraph(housesales) |&gt; \n  dyOptions(drawPoints = TRUE)\n  \ndygraph(housesales) |&gt; \n  dyOptions(fillGraph=TRUE)\n\n\n\n\nCode\nggseasonplot(housesales)\n\n\nCode\nggsubseriesplot(housesales)\n\n\nSeries Decomposition\n\n\nCode\nlibrary(tidyverse)\n\nhousesales |&gt; \n  decompose(type=\"additive\")  |&gt;   \n  forecast::autoplot() + \n  ggtitle(\"\")\n\n\nCode\nhousesales |&gt; \n  decompose(type=\"multiplicative\") |&gt;   \n  forecast::autoplot() + \n  ggtitle(\"\")\n\n\nlag & ACF plots\n\n\nCode\ngglagplot(housesales)\n\n\nCode\ngglagplot(housesales, seasonal=FALSE, lag=1)\n\n\n\n\nCode\nggAcf(housesales)\n\n\nCode\nggPacf(housesales)\n\n\n\n\nCode\nggtsdisplay(housesales)\n\n\nMore graphing examples are here (R codes file)."
  },
  {
    "objectID": "workshops/references.html",
    "href": "workshops/references.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nReferences"
  },
  {
    "objectID": "studyguide/index.html",
    "href": "studyguide/index.html",
    "title": "Preface",
    "section": "",
    "text": "“Statistics is the grammar of science.”\n— Karl Pearson\nIn this course, principles and techniques will be considered of collecting, displaying, and analysing data. The Study Guide covers the basic types of sets of numbers, namely single batches, two or more batches, related batches and data in tables. It contains Exploratory Data Analysis (EDA) techniques for displaying data, fitting a model, and considering transformations of variables so that the data better fit the assumptions of the models. The Study Guide also contains some methods for testing hypotheses, namely \\(t\\), \\(F\\) and \\(\\chi^2\\) tests; and some thoughts on the collection data.\nThe essence of Exploratory Data Analysis, or EDA, is to search for clues by whatever graphical or numerical means seem appropriate and even though these methods are often easy to understand, it is useful to have a computer at hand to perform the computations and draw graphs. Methods for EDA have developed very quickly over the past few decades. John Tukey (1977) likened EDA to detective work where data are studied carefully to provide clues. Some of the techniques employed are quite old and date back to the nineteenth century and beyond when researchers collected information in an organised way and displayed it in tables and graphs. Other techniques are of more recent origin and have been developed to more easily discern peculiarities and trends in data and to provide robust methods of fitting models to data, and these methods not being overly sensitive to restrictive assumptions.\nInferential statistics1 has also seen huge progress in the last 50 or so years since we’ve had fast computers. Inference uses probability models to test hypotheses and estimate parameters while quantifying uncertainty. These procedures lie at the heart of all quantitative science, and can be likened to the way a detective evaluates evidence for competing theories. Regression and Analysis of Variance models are covered in the later part of the study guide, where we will use both EDA and inferential statistics. Topics such as time series analysis, nonparametric methods, and experimental design are presented only briefly. It is intended that these sections will give you an idea of how advanced modelling or analysis can be done.\nNote that you are not expected to study all the topics in depth. Some sections of the study guide are intended for revision of the topics you studied in a first year statistics course. Some topics such as experimental design, correspondence analysis etc will not be examined in depth.\nData analysis is best learned by practice—exploring, summarising, plotting, and testing ideas with real data. Reading a textbook or study guide on data analysis is important, but this activity is more like reading a cook book. In the words of John Tukey:"
  },
  {
    "objectID": "studyguide/index.html#footnotes",
    "href": "studyguide/index.html#footnotes",
    "title": "Preface",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInferential statistics is sometimes called “Confirmatory Analysis”, but I’m not a fan of this term. Real scientists acknowledge that there is always uncertainty, and we can never really “confirm” anything!↩︎"
  },
  {
    "objectID": "studyguide/7-multiple.html",
    "href": "studyguide/7-multiple.html",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "",
    "text": "In this chapter, we consider multiple regression and other models in which there are more than one predictor (or \\(X\\)) variable. Once again our focus is on finding the estimates of coefficients or parameters in multiple linear regression models by the method of least squares. For this we assume that\nWe will continue to use the data set horsehearts of the weights of horses’ hearts and other related measurements."
  },
  {
    "objectID": "studyguide/7-multiple.html#significance-testing-of-type-i-ss",
    "href": "studyguide/7-multiple.html#significance-testing-of-type-i-ss",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Significance testing of Type I SS",
    "text": "Significance testing of Type I SS\nThe significance of the additional variation explained by a predictor can be tested using a \\(t\\) or \\(F\\) statistic. Consider the simple regression model of WEIGHT on EXTDIA. Suppose we decided to add the explanatory variable OUTERDIA to the model, i.e. regress WEIGHT on two explanatory variables EXTDIA and OUTERDIA. Is this new model a significant improvement on the existing one? For testing the null hypothesis that the true slope coefficient of OUTERDIA in this model is zero, the \\(t\\)-statistic is 1.531 (see output below).\n\n\nCode\ntwovar.model &lt;- lm(WEIGHT~ EXTDIA+OUTERDIA, data=horsehearts)\n\ntwovar.model |&gt; tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -1.97     0.551      -3.57 0.000885\n2 EXTDIA         0.226    0.0614      3.68 0.000637\n3 OUTERDIA       0.522    0.341       1.53 0.133   \n\n\nThe \\(t\\) and \\(F\\) distributions are related by the equation \\(t^{2} =F\\) when the numerator df is just one for the \\(F\\) statistic. Hence 1.532 = 2.34 is the \\(F\\) value for testing the significance of the additional SSR due to OUTERDIA. In other words, the addition of OUTERDIA to the simple regression model does not result in a significant improvement in the sense that the reduction in residual SS (= 1.247) as measured by the \\(F\\) value of 2.34 is not significant (\\(p\\)-value being 0.133).\n\n\nCode\nonevar.model &lt;- lm(WEIGHT~ EXTDIA, data=horsehearts)\n\ntwovar.model &lt;- lm(WEIGHT~ EXTDIA+OUTERDIA, data=horsehearts)\n\nanova(onevar.model, twovar.model)\n\n\nAnalysis of Variance Table\n\nModel 1: WEIGHT ~ EXTDIA\nModel 2: WEIGHT ~ EXTDIA + OUTERDIA\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     44 24.115                           \n2     43 22.867  1    1.2472 2.3453  0.133\n\n\nAlthough OUTERDIA is correlated with WEIGHT, it also has high correlation with EXTDIA. In other words, the correlation matrix gives us some indication of how many variables might be needed in a multiple regression model, although by itself it cannot tell us what combination of predictor variables is good or best.\n\n\n\nFigure 5: Issues with multiple predictors\n\n\n\n\n\nFigure 6: Effect of multiple predictors on model summaries\n\n\nFigure 5 and Figure 6 summarise the following facts:\n\nWhen there is only one explanatory variable, \\(R^2\\) = SSR/SST equals the square of the correlation coefficient between that variable and the dependent variable. Therefore if only one variable is to be chosen, it should have the highest correlation with the response variable, \\(Y\\).\nWhen variables are added to a model, the regression sum of squares SSR will increase and the residual or error sum of squares SSE will reduce. The opposite is true if variables are dropped from the model. This fact follows from Figure 6.\nThe other side of the coin to the above remark is that as additional variables are added, the Sums of Squares for residuals, SSE, will decrease towards zero as also shown in Figure 6(c).\nThe overlap of circles in suggests that these changes in both SSR and SST will lessen as more variables are added, see Figure 5(b).\nFollowing on from the last two notes, as \\(R^2\\) = SSR/SST, \\(R^2\\) will increase monotonically towards 1 as additional variables are added to the model. (monotonically increasing means that it never decreases although it could remain the same). This is indicated by Figure 6(a). If variables are dropped, then \\(R^2\\) will monotonically decrease.\nAgainst the above trends, the graph of residual mean square in Figure 6(b) reduces to a minimum but may eventually start to increase if enough variables are added. The residual sum of squares SSE decreases as variables are added to the model (see Figure 5(b)). However, the associated df values also decrease so that the residual standard deviation decreases at first and then starts to increase as shown in Figure 6(b). (Note that the residual standard error \\(s_{e}\\) is the square root of the residual mean square\n\\[s_{e}^{2} =\\frac{{\\text {SSE}}}{{\\text {error degrees of freedom}}},\\]\ndenoted as MSE in Figure 5(b)). After a number of variables have been entered, the additional amount of variation explained by them slows down but the degrees of freedom continues to change by 1 for every variable added, resulting in the eventual increase in residual mean square. Note that the graphs in Figure 5 are idealised ones. For some data sets, the behaviour of residual mean square may not be monotone.\nNotice that the above trends will occur even if the variables added are garbage. For example, you could generate a column of random data or a column of birthdays of your friends, and this would improve the \\(R^2\\) but not the adjusted \\(R^2\\). The adjusted \\(R^2\\) makes adjustment for the degrees of freedom for the SSR and SSE, and hence reliable when compared to the unadjusted or multiple \\(R^2\\). The residual mean square error also partly adjusts for the drop in the degrees of freedom for the SSE and hence becomes an important measure. The addition of unimportant variables will not improve the adjusted \\(R^2\\) and the mean square error \\(s_{e}^{2}\\)."
  },
  {
    "objectID": "studyguide/7-multiple.html#other-ss-types",
    "href": "studyguide/7-multiple.html#other-ss-types",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Other SS types",
    "text": "Other SS types\nThe R anova function anova() calculates sequential or Type-I SS values.\nType-II sums of squares is based on the principle of marginality. Type II SS correspond to the R convention in which each variable effect is adjusted for all other appropriate effects.\nType-III sums of squares is the SS added to the regression SS after ALL other predictors including an intercept term. This SS however creates theoretical issues such as violation of marginality principle and we should avoid using this SS type for hypothesis tests.\nThe R package car has the function Anova() to compute the Type II and III sums of squares. Try-\n\n\nCode\nfull.model &lt;- lm(WEIGHT~ ., data=horsehearts)\n\nanova(full.model)\n\nlibrary(car)\n\nAnova(full.model, type=2)\nAnova(full.model, type=3)\n\n\nFor the horsehearts data, a comparison of the Type I and II sums squares is given below:\n\n\nCode\nfull.model &lt;- lm(WEIGHT~ ., data=horsehearts)\n\nanova1 &lt;- full.model |&gt; \n  anova() |&gt; \n  tidy() |&gt; \n  select(term, \"Type I SS\" = sumsq)  \n\nanova2 &lt;- full.model |&gt; \n  Anova(type=2) |&gt; \n  tidy() |&gt; \n  select(term, \"Type II SS\" = sumsq) \n\ntype1and2 &lt;- full_join(anova1, anova2, by=\"term\")\n\ntype1and2\n\n\n\n\n\n\n \n  \n    term \n    Type I SS \n    Type II SS \n  \n \n\n  \n    INNERSYS \n    34.40 \n    0.20 \n  \n  \n    INNERDIA \n    3.53 \n    0.62 \n  \n  \n    OUTERSYS \n    2.76 \n    1.69 \n  \n  \n    OUTERDIA \n    0.13 \n    0.55 \n  \n  \n    EXTSYS \n    0.06 \n    1.79 \n  \n  \n    EXTDIA \n    1.90 \n    1.90 \n  \n  \n    Residuals \n    14.07 \n    14.07 \n  \n\n\n\n\n\nWhen predictor variables are correlated, it is difficult to assess their absolute importance and the importance of a variable can be assessed only relatively. This is not an issue with the most highly correlated predictor in general."
  },
  {
    "objectID": "studyguide/7-multiple.html#best-subsets-selection",
    "href": "studyguide/7-multiple.html#best-subsets-selection",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Best Subsets Selection",
    "text": "Best Subsets Selection\nAn exhaustive screening of all possible regression models (and hence the name best subsets regression) can also be done using software. For example, there are 6 predictor variables in the horses’ hearts data. If we fix the number of predictors as 3, then \\(\\small {\\left(\\begin{array}{c} {6} \\\\ {3} \\end{array}\\right)} = 20\\) regression models are possible. One may select the ‘best’ 3-variable model based on criteria such as AIC, \\(C_{p}\\), \\(R_{adj}^{2}\\) etc. Software must be employed to perform the conventional stepwise regression procedures. Software algorithms give one or more best candidate models fixing the number of variables in each step.\nOn the basis of our analysis on the horses’ hear data, we might decide to recommend the model with predictor variables EXTDIA, EXTSYS, INNERDIA and OUTERSYS. In particular if the model is to be used for describing relationships then we would tend to include more variables. For prediction purposes, however, a simpler feasible model is preferred and in this case we may opt for the smaller model with only INNERDIA and OUTERSYS. See Table 4 produced using the following R codes:\n\n\nCode\nlibrary(leaps)\nlibrary(HH)\nlibrary(kableExtra)\n\nb.model &lt;- regsubsets(WEIGHT ~ ., data = horsehearts) |&gt;\n  summaryHH() \n\nb.model |&gt; \n  kable(digits = 3) |&gt; \n  kable_styling(bootstrap_options = \"basic\", full_width = F)\n\n\n\n\nTable 4: Subset selection\n\n\nmodel\np\nrsq\nrss\nadjr2\ncp\nbic\nstderr\n\n\n\n\nINNERD\n2\n0.658\n19.450\n0.650\n11.923\n-41.677\n0.665\n\n\nINNERD-OUTERS\n3\n0.715\n16.173\n0.702\n4.838\n-46.335\n0.613\n\n\nINNERD-OUTERS-OUTERD\n4\n0.718\n16.043\n0.698\n6.477\n-42.878\n0.618\n\n\nINNERD-OUTERS-EXTS-EXTD\n5\n0.741\n14.739\n0.715\n4.862\n-42.949\n0.600\n\n\nINNERD-OUTERS-OUTERD-EXTS-EXTD\n6\n0.749\n14.272\n0.718\n5.566\n-40.602\n0.597\n\n\nINNERS-INNERD-OUTERS-OUTERD-EXTS-EXTD\n7\n0.753\n14.067\n0.714\n7.000\n-37.437\n0.601\n\n\n\n\n\n\n\n\nSometimes theory may indicate that a certain explanatory variable should be included in the model (e.g. due to small sample size). If this variable is found to make an insignificant contribution to the model, then one should exclude the variable when the model is to be used for prediction but if the model is to be used for explanation purposes only then the variable should be included. Other considerations such as cost and time may also be taken into account. For every method or algorithm, one could find peculiar data sets where it fouls up. The moral – be alert and don’t automatically accept models thrown up by a program. Note there is never one right answer as different methods and different criteria lead to different models.\nVariable selection procedures can be a valuable tool in data analysis, particularly in the early stages of building a model. At the same time, they present certain dangers. There are several reasons for this:\n\nThese procedures automatically snoop though many models and may select ones which, by chance, happen to fit well.\nThese forward or backward stepwise procedures are heuristic (i.e., shortcut) algorithms, which often work very well but which may not always select the best model for a given number of predictors (here best may refer to adjusted \\(R^2\\)-values, or AIC or some other criterion).\nAutomatic procedures cannot take into account special knowledge the analyst may have about the data. Therefore, the model selected may not be the best (or make sense) from a practical point of view.\nMethods are available that shrink coefficients towards zero. The least squares approach minimises the residual sums of squares or RSS without placing any constraint on the coefficients. The shrinkage methods, which place a constraint on the coefficients, work well when there are large numbers of predictors. A ridge regression shrinks the coefficients towards zero but in relation each other. On the other hand, (Least Absolute Selection and Shrinkage Operator) lasso regression shrinks some of coefficients to zero which means these predictors can be dropped. Note that the ridge regression does not completely remove predictors. By shrinking large coefficients, we obtain a model with higher bias but lower variance. This process is known as regularisation in the literature (not covered in this course)."
  },
  {
    "objectID": "studyguide/7-multiple.html#time-series-regression-with-seasonality-components",
    "href": "studyguide/7-multiple.html#time-series-regression-with-seasonality-components",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Time Series Regression with seasonality components",
    "text": "Time Series Regression with seasonality components\nIndicator variables are used to capture the seasonality such as months and quarters. Time related trends can be picked up with the usual regression. The function tslm() from the forecast package is handy to model the response \\(Y\\) using the time variable and the seasonal indicators. Consider the credit card balance series discussed in Chapter 2. The fitted linear model is shown in Table 10 and the forecasts made the model for 48 months ahead are shown in Figure 11.\n\n\nCode\nlibrary(readxl)\n\nurl &lt;- \"http://www.massey.ac.nz/~anhsmith/data/hc12_daily_average_balances.xlsx\"\ndestfile &lt;- \"hc12.xlsx\"\n\ncurl::curl_download(url, destfile)\n\ncredit.balance &lt;- \n  read_excel(destfile, na = \"-\", skip = 4) |&gt; \n  pull(CRCD.MOA20) |&gt; \n  na.omit() |&gt; \n  ts(start=c(2000,7), frequency=12)\n\n\n\n\nCode\nlibrary(forecast)\n\ncbfit &lt;- tslm(credit.balance ~ trend + season)\n\ncbfit |&gt; forecast(h=48) |&gt; autoplot()\n\n\n\n\n\nFigure 11: tidy() output of the tslm() fit\n\n\n\n\n\n\nCode\ncbfit |&gt; tidy()  \n\n\nWarning: The `tidy()` method for objects of class `tslm` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nTable 10:  tidy() output of the tslm() fit \n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3460.949 \n    122.430 \n    28.269 \n    0.000 \n  \n  \n    trend \n    12.771 \n    0.395 \n    32.358 \n    0.000 \n  \n  \n    season2 \n    -60.206 \n    154.786 \n    -0.389 \n    0.698 \n  \n  \n    season3 \n    -96.978 \n    154.787 \n    -0.627 \n    0.532 \n  \n  \n    season4 \n    -137.532 \n    154.790 \n    -0.889 \n    0.375 \n  \n  \n    season5 \n    -164.651 \n    154.793 \n    -1.064 \n    0.288 \n  \n  \n    season6 \n    -175.292 \n    154.798 \n    -1.132 \n    0.258 \n  \n  \n    season7 \n    -232.007 \n    153.165 \n    -1.515 \n    0.131 \n  \n  \n    season8 \n    -209.839 \n    154.798 \n    -1.356 \n    0.176 \n  \n  \n    season9 \n    -215.827 \n    154.793 \n    -1.394 \n    0.164 \n  \n  \n    season10 \n    -181.642 \n    154.790 \n    -1.173 \n    0.242 \n  \n  \n    season11 \n    -119.718 \n    154.787 \n    -0.773 \n    0.440 \n  \n  \n    season12 \n    14.728 \n    154.786 \n    0.095 \n    0.924 \n  \n\n\n\n\n\n\nTable 10 shows that the seasonal effects are highly significant. Figure 11 shows that the fitted model is not faring well for the year 2020, which was affected by COVID. The forecasts ahead are also untrustworthy.\nNote that the time variable \\(t\\) becomes the predictor in the fitted model but the model is not based on the past or lagged \\(Y\\) data. The smoothing methods discussed below employ such past data."
  },
  {
    "objectID": "studyguide/7-multiple.html#moving-average-smoothing",
    "href": "studyguide/7-multiple.html#moving-average-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\nHere we compute the mean of successive smaller sets of numbers of immediate past data. The period or length (also called span) of the moving average is the number of observations (including the present one) used for averaging. The general expression for the moving average \\(M_t\\) at time \\(t\\) is \\[M_t = [X_t + X_{t-1} + ... + X_{t-N+1}] / N\\] where \\(X_t\\) is the observation at time \\(t\\) and \\(N\\) the moving average length. Figure 12 shows the moving average smoothing for the ‘$20 Notes in public hands’ data. It can be noted that the degree of smoothing is directly related to the length of the moving average (i.e., longer the length, greater the smoothing).\n\n\nCode\nNZnotes20 &lt;- read_table(\n  \"http://www.massey.ac.nz/~anhsmith/data/20dollar.txt\") |&gt; \n  pull(value) |&gt; \n  ts(start=1968, frequency=1)\n\n\n\n\nCode\nMA.centred &lt;- ma(NZnotes20, 2, centre = TRUE)\nMA.noncentred &lt;- ma(NZnotes20, 2, centre = FALSE)\n\nlibrary(forecast)\n\nautoplot(NZnotes20) + \n  autolayer(MA.centred, series = \"2 y MA centred\") + \n  autolayer(MA.noncentred, series = \"2 y MA noncentred\", linetype=2)\n\n\n\n\n\nFigure 12: Centred and Non-centred Moving Averages\n\n\n\n\nWhen placing the moving averages against time, placing them in the middle time period is more appropriate. Strictly speaking the moving average must fall at \\(t = 1.5, 2.5, 3.5\\) etc when the period of the moving average is an even number. Hence we need to smooth again the moving average smoothed values to place the moving averages at \\(t = 2, 3, 4\\) etc. Figure 12 also compares the centred moving average smoothing and non-centred moving average smoothing (length 2) for the ‘$20 Notes in public hands’ data. It is easy to see that centring has stopped the moving averages from drifting below the original series and ‘lined’ them with the original data."
  },
  {
    "objectID": "studyguide/7-multiple.html#exponential-smoothing",
    "href": "studyguide/7-multiple.html#exponential-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nIn moving average smoothing all past observations are given equal weight. In exponential average smoothing, past observations (i.e. as the observations become older) are given exponentially decreasing weights. That is, recent observations are given relatively more weight than the older observations. Hence the exponential smoothing method becomes a representative method to produce a smoothed time series. The average computed using exponentially decreasing weights is known as the Exponentially Weighted Moving average (EWMA). This fitted average is also called level because this method does not allow for trends or seasonality (and everything gets smoothed).\nEWMA smoothing begins by setting \\(S_0\\) to \\(x_1\\) (usually), where \\(S\\) stands for smoothed observation (or EWMA), and \\(x\\) for the observation. The subscript in \\(x\\) refers to the time periods \\(t =1, 2, ... ,n\\). For the second period, \\(S_2 = \\alpha x_2 + (1-\\alpha)x_1\\) and so on. Here the parameter \\(\\alpha\\) is called the smoothing constant, the weight given to the current observation. A general formula is also available to compute the EWMA for any time period \\(t\\). Figure 13 shows the single exponential smoothing on the $20 Notes series for \\(\\alpha=0.5\\). Instead of fixing an \\(\\alpha\\) value such as 0.5, we may leave it to the software to find an optimum value.\n\n\nCode\nsingle.exp &lt;- NZnotes20 |&gt; ses(alpha=0.5) |&gt; fitted()\n\np1 &lt;- autoplot(NZnotes20) + \n  autolayer(single.exp, series =\"alpha=0.5\")\n\nsingle.exp1 &lt;- NZnotes20 |&gt; ses() |&gt; fitted()\n\np2 &lt;- autoplot(NZnotes20) + \n  autolayer(single.exp1, series = \"optimised alpha\")\n\nlibrary(patchwork)\n\np1/p2\n\n\n\n\n\nFigure 13: Single exponential smoothing (fits)\n\n\n\n\nThe rate at which the effect of older observations on the current EWMA will be dampened depends on \\(\\alpha\\), the smoothing constant. Larger the \\(\\alpha\\) value, faster the dampening effect of older observations.\nA naive choice for the initial value for \\(S_0\\) (i.e. at the origin) is \\(x_1\\), the first observation. The other choices include the average of two or more successive observations, estimating using regression methods etc. In this course we will not be concerned with the choice of the initial values very much (and accept the defaults of the R packages)."
  },
  {
    "objectID": "studyguide/7-multiple.html#double-exponential-smoothing",
    "href": "studyguide/7-multiple.html#double-exponential-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Double Exponential Smoothing",
    "text": "Double Exponential Smoothing\nSingle exponential smoothing is improved to double exponential smoothing to account for the trend type of variations. This is achieved by introducing a second smoothing constant say \\(\\beta\\). This weighting constant captures linear trends using the successive differences in the fitted EWMAs. The process of double exponential smoothing is conveniently represented by the following two equations.\n\\(S_t = \\alpha X_t+(1-\\alpha)[S_{t-1} + T_{t-1}]\\) (called level equation)\nwhere\n\\(T_t =\\beta [S_t - S_{t-1}] + (1 -\\beta)T_{t-1}\\) (called trend equation).\nThe second equation for the trend EWMA gives a weight of \\(\\beta\\) to the current differences in the EWMAs (i.e. \\(S_t - S_{t-1}\\)) and the balance weight \\((1-\\beta)\\) to the preceding trend EWMA.\nThe main or usual EWMA (i.e. \\(S_t\\)) gives a weight of \\(\\alpha\\) to the current observation and the balance weight \\((1-\\alpha)\\) to the sum of preceding main and trend EWMAs (i.e. \\(S_{t-1} + T_{t-1}\\)). A naive choice for the initial value for \\(T_0\\) (i.e. at the origin) is \\(x_2-x_1\\), the difference between the first and the second observations. The other choices include the average of two or more successive differences, estimating using regression methods etc. In this course we will not be concerned with the choice of the initial values very much. The smoothing constants \\(\\alpha\\) and \\(\\beta\\) are obtained by non-linear optimisation methods (such as the Marquardt algorithm). In this course, we will just accept the R outputs as the optimised fits. Figure 14 shows the double exponential smoothing on the $20 Notes series with optimum \\(\\alpha\\) and \\(\\beta\\) (as determined by the forecast package).\n\n\nCode\ndouble.exp &lt;- NZnotes20 |&gt; holt() |&gt; fitted()\n\nautoplot(NZnotes20) + \n  autolayer(double.exp, series = \"DEWMA-optimised\")\n\n\n\n\n\nFigure 14: Double exponential smoothing"
  },
  {
    "objectID": "studyguide/7-multiple.html#triple-exponential-smoothing",
    "href": "studyguide/7-multiple.html#triple-exponential-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Triple Exponential Smoothing",
    "text": "Triple Exponential Smoothing\nThis approach developed by Holt and Winter (hence the name Holts-Winter (HW) smoothing) employs a level equation, a trend equation, and a seasonal equation for smoothing at each time period. Hence three weights, or smoothing parameters are needed.\n\\(S_t = \\alpha(X_t-P_{t-p}) + (1-\\alpha)[S_{t-1}+T_{t-1}]\\) (level equation)\n\\(T_t = \\beta [S_t-S_{t-1}]+ 1-\\beta)T_{t-1}\\) (trend equation)\n\\(P_t = \\phi (X_t-S_t)+(1-\\phi)P_{t-p}\\) (seasonal equation of a given period \\(p\\))\nThe smoothing parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\phi\\) are constants and are usually estimated minimising the MSE. In order to proceed with the triple exponential smoothing, we need at least one complete season’s data to determine initial estimates of the seasonal indices. For estimating the trend components, it is preferable to have at least two complete season’s data.\nThe initial trend is usually estimated using the average differences in the corresponding observations in two adjacent seasons. The estimating initial values for seasonal components, we use the averages rather than differences. Regression methods are also employed for estimating the initial values. In this course, we will not study the estimation methods for initial values in any detail but will accept computations and optimisation reported in the forecast R package. Figure 15 shows the triple exponential smoothing to the outstanding credit card balances series.\n\n\nCode\nlibrary(forecast)\n\ntrp.exp &lt;- credit.balance |&gt; hw() |&gt; fitted()\n\nautoplot(credit.balance) +\n  autolayer(trp.exp, \n            series = \"Holt-Winter- optimised\")\n\n\n\n\n\nFigure 15: Triple exponential smoothing"
  },
  {
    "objectID": "studyguide/7-multiple.html#assessment-of-fit",
    "href": "studyguide/7-multiple.html#assessment-of-fit",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Assessment of Fit:",
    "text": "Assessment of Fit:\nForecast accuracy measures such as MSE are useful for fixing the smoothing parameters such as the moving average length or the EWMA smoothing constant. We may minimise MSE (say) to fix a value for the EWMA smoothing constant. This can be done by trial and error or by nonlinear optimisation methods (such as Marquardt’s procedure).\nBy the term forecasting, we mean projecting the present time series for future time points. For example, assume that we used an uncentered two period moving average to smooth the ‘$20 Notes’ time series. The moving average value (non-centred) for 1969 is 18.05. A naive approach to forecasting will be to use the smoothed value at time \\((t-1)\\) to forecast for time \\(t\\). Hence the forecasted value (or simply forecast) of $20 bills for 1970 would be 18.05 as against the actual observed value of 21.76. In the absence of Year 1970 data, the same value 18.05 would be the forecast for 1971 and so on. MAs are not useful for forecasting in general and hence this average is just employed to fit trends or extract trends when seasonal variation is absent.\nFor EWMA Forecasting, the forecast approach is to add an adjustment for the error that occurred in the last forecast. We again consider ‘$20 Notes’ time series and obtain the EWMA smoothed values for \\(\\alpha = 0.4\\). For the year 1969 (say), the EWMA value is 17.78 as against the actual value 19.41 giving an error of 19.41- 17.78= 2.72. This error is given a weight of 0.4 and added to the 1969 forecast (naive estimate) of 16.69 as \\(16.69+0.4\\times2.72\\) giving a forecast value of 17.78 for 1970. The term ‘adjustment error’ will refer to \\(0.4\\times2.72 = 1.088\\). This forecast value is also obtained as\nForecast for \\(1970 = 0.4\\times19.41+0.6\\times16.69= 17.78\\).\n(from the relationship \\(S_{t+1}=\\alpha x_{t+1}+(1-\\alpha)S_t\\) where the unavailable value \\(X_{t+1}\\) is replaced by the naive estimate \\(X_t\\)). This forecasting approach is also not useful in the presence of trend etc. Hence only a forecast of one time period ahead is usually done. For forecasting two or more time periods ahead, methods such as double and triple exponential smoothing are more useful.\nIf we perform the double exponential forecasting for some \\(m\\) periods ahead from a point at time \\(t\\), the trend part of EWMA, \\(T_t\\), will be added \\(m\\) times to the naive level estimate \\(S_t\\). As shown in Figure 16, the double exponential smoothing approach provides no nonsense forecasts compared to the naive single exponential forecasts in the presence of trends. The fit/forecast quality measures such as the MSE, MAD etc will also be smaller for the double exponential smoothing in the presence of trends.\n\n\nCode\nholt1 &lt;- holt(credit.balance)\nholt2 &lt;- hw(credit.balance)\n\np0 &lt;- forecast::autoplot(window(credit.balance, start=2012)) + \n  xlim(2012, 2025) + ylim(4500,7000) + ylab(\"\")\n\np1 &lt;- p0 + autolayer(holt1, series = \"Double exponential\")\n\np2 &lt;- p0 + autolayer(holt2, series = \"Triple exponential\")\n\nlibrary(patchwork)\n\np1/p2\n\n\n\n\n\nFigure 16: Double and triple exponential forecasts for credit balance data\n\n\n\n\nThe forecast accuracy measures can also be obtained using the accuracy() function in the forecast package. This function also give a few other accuracy measures. For the credit card balances data, we obtain-\n\n\nCode\nbind_cols(\n  Method = c(\n    \"Double exponential smoothing\",\n    \"Triple exponential smoothing\"\n  ),\n  bind_rows(\n    accuracy(holt1)[,c(2,3,5)],\n    accuracy(holt2)[,c(2,3,5)]\n    )\n)\n\n\n# A tibble: 2 × 4\n  Method                        RMSE   MAE  MAPE\n  &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Double exponential smoothing  88.5  51.3 0.993\n2 Triple exponential smoothing  72.5  33.5 0.680\n\n\nEvidently the triple exponential smoothing fares well for our credit card balances data."
  },
  {
    "objectID": "studyguide/7-multiple.html#intro-to-autoregressive-modelling",
    "href": "studyguide/7-multiple.html#intro-to-autoregressive-modelling",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Intro to Autoregressive Modelling",
    "text": "Intro to Autoregressive Modelling\nThe concept of stationarity plays in important role for building time series models. In crude terms, a time series is said to be stationary if the mean and variance do not change over time (alternatively the same probability law applies over time). In fact stationarity is defined in a pure mathematical way but we will not worry about this in this course.\nA white noise series is defined as a series with a constant mean and variance, and the true mean and variance remain the same for all \\(t\\). Normal random data is an example of white noise but the normal assumption is not required for a series to be white noise. You can also intuitively guess that a white noise series is stationary.\nA quick collection of EDA displays can be obtained using a single function ggtsdisplay() or tsdisplay() in R. This display is shown for the white noise series in Figure 17.\n\n\nCode\nset.seed(123)\nwht.noise &lt;- arima.sim(list(order=c(0,0,0)),500)\nggtsdisplay(wht.noise)\n\n\n\n\n\nFigure 17: A summary plots for white noise\n\n\n\n\nTime series modelling is not needed for a series such as this one. The above random series must be distinguished from a series whose autocorrelations are not decaying to zero or becoming significant frequently.\nA drifting random walk series is defined as \\[X_t = \\delta + X_{t-1} + W_t \\]\nwhere \\(\\delta\\) is the constant drift, and \\(W_t\\) is white noise which induces the random walk for the series. The mean function depends on \\(t\\) for this series and hence not stationary. This is not of concern because we can model the drift and make the residuals stationary. The trick is to model the difference \\(X_{t} - X_{t-1}\\) or just use the first lag \\(X_{t-1}\\) as a predictor in the usual regression.\n\n\nCode\nset.seed(123)\nrwd &lt;- arima.sim(list(order=c(0,1,0)),500)\nggtsdisplay(rwd)\n\n\n\n\n\nFigure 18: A summary plots for drifting random walk series\n\n\n\n\nIn Figure 18, it should also be noted that the ACFs decay to zero which is a good thing when compared to the case where ACFs are not decaying to zero.\n\\[X_t=\\beta_0+\\beta_1(\\sin(\\frac {2\\pi}{12} t)+\\beta_1(\\cos(\\frac {2\\pi}{12} t)+\\epsilon_t\\] The above model introduces a 12-period seasonal pattern using \\(\\sin\\) and \\(\\cos\\) functions (which are periodic). The time series EDA plots for this function is obtained below:\n\n\nCode\nt=1:500\nset.seed(123)\nXt=sin(t*2*pi/12)+cos(t*2*pi/12)+rnorm(500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 19: Seasonal series EDA plots\n\n\n\n\nNote the periodical patterns in the EDA plots shown in Figure 19. Analysis of a time series using sine and cosine functions is known as frequency domain approach and is popular in meteorology, chemistry and geophysics. Instead of using trigonometric functions, say if indicator variables are used to model seasonality, we stay within the time domain. The autocovariance function in the time domain is analogous to the spectral density function in the frequency domain.\nConsider the model \\[X_t=\\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2}+ \\dots + \\alpha_p X_{t-p} + \\epsilon_t\\]\nThis model is called the auto-regressive model of order \\(p\\) and called the \\(AR(p)\\) process. Under this model, we assume that the present value depends linearly on the immediate past values as well as a random error. Note that this model is very similar to the multiple regression model where the predictors are just the past values of the series. This \\(AR(p)\\) series is stationary if the variance of the terms are finite. When \\(p = 1\\) (the first-order process), the model is known as a Markov process. The EDA plots for random data from this process is shown in Figure 20. Figure 21 shows the \\(AR(3)\\) process EDA plots. Note that the PACF shows a pattern matching the parameters set ar= c(0.8, -0.7, .3). The last PACF in an \\(AR(p)\\) model accounts the excess autocorrelation at lag \\(p\\) that is not accounted for by an \\(AR(p-1)\\) model.\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(1,0,0), ar=.6), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 20: A typical Markov series EDA plots\n\n\n\n\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(3,0,0), ar= c(0.8, -0.7, .3)), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 21: A typical AR(p=3) series EDA plots\n\n\n\n\nThe moving average process for errors is defined by the following equation. \\[X_t=\\beta_0 z_{t}+ \\beta_1 z_{t-1}+ \\dots + \\beta_q z_{t-q}\\] Note that \\(X_t\\) is modelled with errors \\(z_1\\), \\(z_2\\),…, whose means are assumed to be zero and constant variance. The \\(\\beta\\)s are coefficients of the model and \\(q\\) is the order. The mean of this \\(MA(q)\\) process is zero but we can always add some mean \\(\\mu\\) which will not affect the properties such as ACFs. The basic idea behind the \\(MA(q)\\) process is that the current value of the response is due to variety of current and past unpredictable random events. It is proved that the moving average process is a stationary process and that the ACFs at lags greater than \\(q\\) are zero. Figure 22 and Figure 23 show the basic EDA plots for the \\(MA(1)\\) and \\(MA(3)\\) processes.\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(0,0,1), ma=.6), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 22: EDA plots of a typical MA(1) process\n\n\n\n\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(0,0,3), ma=c(.3, .1, -.4)), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 23: EDA plots of a typical MA(3) process\n\n\n\n\nARMA Model\nThe term \\(ARMA(p, q)\\) model refers to the following equation that combines both the \\(AR(p)\\) and \\(MA(q)\\) models.\n\\[X_t=\\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2}+ \\dots + \\alpha_p X_{t-p} + \\beta_o z_{t}+ \\beta_1 z_{t-1}+ \\dots + \\beta_q z_{t-q}\\] It is easy to see that the term \\(\\epsilon_t\\) in the \\(AR(p)\\) model is replaced or expanded with the \\(MA(q)\\) model. You may wonder why to have such a complicated model. In fact the ARMA model requires fewer parameters than using just \\(MA(q)\\) or \\(AR(p)\\) model. \\(ARMA(p, q)\\) model is a stationary model. Figure 24 shows the EDA plots for the simulated series from the \\(ARMA(2,2)\\) process; note the constants fixed under the ar and ma parts of the arima.sim function and compare the ACF and PACF plots.\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(2,0,2), ar=c(.5, -.3), ma=c(.3, .1)), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 24: EDA plots of a typical MA(3) process\n\n\n\n\nFitting ARMA model Fitting an AR model can be done approximately using the multiple regression approach. If we use the sample mean \\(\\bar{X}\\) to estimate the mean \\(\\mu\\) of the process, the AR model becomes the multiple regression model with lags as predictors. However we cannot take the same approach to fitting ARMA models and we need to employ nonlinear optimisation methods.\nAfter fitting the ARMA model, we perform diagnostics of the fitted model. Here we explore the residuals of the fitted model for randomness and periodicity. In order to avoid over fitting, we will also examine the standard errors of the fitted coefficients. The need for transformations such the logarithm or the square root will also be indicated by the residuals.\nIf the residuals are found to be nonstationary (often the case), we opt for differencing of the series. We have briefly seen that differencing can bring stationarity to a drifting process. Formally, the first difference \\(X_t-X_{t-1}\\) is denoted as \\(\\bigtriangledown X_t\\). If we perform the differencing of the differences, we obtain \\(\\bigtriangledown^2 X_t\\) and so on. In order to bring stationarity to residuals, we may do differencing \\(d\\) times. We then fit the model to \\(\\bigtriangledown^2 X_t\\) instead of \\(X_t\\). This model is known as an autoregressive integrated moving average (ARIMA) model and denoted as \\(ARIMA(p, d, q)\\). The term “integrated” means that the stationary model that was fitted based on the differenced data has to be summed (or “integrated”) to provide a model for the original data.\nThe ARIMA model is further generalised to seasonal ARIMA (SARIMA) model. The AR part for seasons (parameter P), differencing part (D) and the MA part (Q) form part of the \\(SARIMA(p,d,q ,P, D, Q)\\). This topic is covered in higher level courses.\nBuilding good ARIMA models of Box and Jenkins (1976) generally requires a reasonable amount of experience compared to building models to cross-section data. In this course you are expected not to build ARIMA models (no exam questions). But it should not be too hard to recognise the situations such as seasonality in the data series using EDA tools.\nThe R package forecast has a convenient function called auto.arima which can quickly fit an ARIMA model. This is just an initial model which must be improved further. For the credit balance data, we obtain the following output:\n\n\nCode\nauto.arima(credit.balance)\n\n\nSeries: credit.balance \nARIMA(0,2,4)(0,0,2)[12] \n\nCoefficients:\n          ma1      ma2     ma3     ma4    sma1    sma2\n      -0.6271  -0.5941  0.0977  0.1389  0.2602  0.1775\ns.e.   0.0608   0.0729  0.0683  0.0636  0.0623  0.0570\n\nsigma^2 = 5817:  log likelihood = -1581.39\nAIC=3176.78   AICc=3177.2   BIC=3202.1\n\n\nThis package can also generate forecasts easily, see Figure 25. This plot also shows the confidence bands for the forecasts.\n\n\nCode\nfit &lt;- auto.arima(credit.balance)\nforecast::autoplot(forecast(fit,h=24))\n\n\n\n\n\nFigure 25: Forecasts for credit balance series"
  },
  {
    "objectID": "studyguide/5-tabulated.html",
    "href": "studyguide/5-tabulated.html",
    "title": "Chapter 5: Tabulated Counts",
    "section": "",
    "text": "“….the Chi-square statistical test is one of twenty important scientific breakthroughs of the 20th century…”\n— Ian Hacking (1984)"
  },
  {
    "objectID": "studyguide/5-tabulated.html#goodness-of-fit-for-distributions",
    "href": "studyguide/5-tabulated.html#goodness-of-fit-for-distributions",
    "title": "Chapter 5: Tabulated Counts",
    "section": "Goodness of Fit for Distributions",
    "text": "Goodness of Fit for Distributions\nWhen testing for the goodness of fit for a distribution, observations in various non-overlapping ranges are classified into categories and a frequency count is obtained for each category or range. These (observed) frequency counts are compared to the counts which would be obtained if the hypothesised distribution fitted the data exactly. We call these counts the expected counts. In general one assumes that the parameters of the hypothesised distribution equal the estimates obtained from the data. Thus we are effectively just testing whether the shape of the hypothesised distribution is correct.\nFor example if a set of data has a sample mean of 10 and sample standard deviation of 2 and we wish to test whether a Normal distribution is appropriate for these data, we hypothesise a Normal distribution with a mean of 10 and a standard deviation of 2. The following Chi-squared statistic can be used to test whether the hypothesised distribution describes the data reasonably well. The idea is to split the number line into, say, \\(c\\) non-overlapping intervals spanning the range of the data. The observed and expected frequency of data in each interval is then compared, using\n\\[\\chi ^{2} =\\sum _{1}^{c}\\frac{\\left({\\text   {Observed-Expected}}\\right)^2 }{{\\text   {Expected}}}  =\\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E},\\]\nwhere again we sum over all \\(c\\) categories. For example, for \\(n=100\\) data we would expect 15.87% to be \\(\\leq 8\\), 34.13% to be greater than 8 and \\(\\leq 10\\) , 34.13% to be greater than 10 and \\(\\leq 12\\), and 15.87% to be \\(&gt;12\\). We would compare our observed counts with these expected ones. Note that expected counts need not be integers, and should not be rounded in the calculation of \\(\\chi^2\\), especially if they are small as that can introduce substantial round-off error into \\(\\chi^2\\).\nThe degrees of freedom for this test equals the number of data categories minus one more than the number of parameters estimated \\(c-1-2 = c-3\\) in this case). These tests were covered in your first-year paper. You should revise your notes on them. Note that we have seen in Chapter 4 that it is possible to test for normality using an tests such as Shapiro-Wilk test."
  },
  {
    "objectID": "studyguide/5-tabulated.html#example-porcine-stress-syndrome-pss",
    "href": "studyguide/5-tabulated.html#example-porcine-stress-syndrome-pss",
    "title": "Chapter 5: Tabulated Counts",
    "section": "Example: Porcine Stress Syndrome (PSS)",
    "text": "Example: Porcine Stress Syndrome (PSS)\nPorcine Stress Syndrome (PSS) can result in the development of pale, soft meat in pigs and, under conditions of stress, sudden death. It can therefore result in severe economic loss. It is believed to be controlled by a single gene and its incidence could therefore be reduced by a selective breeding program. In a survey of its incidence, the following results were obtained for four major breeds. (The presence of PSS can be detected by a positive reaction to the breathing of halothane). The observed counts are shown in Table 4.\n\n\nCode\ndpss &lt;- tibble(\n  Breed = c(\"Large White\", \"Hampshire\", \"Landrace(B)\", \"Landrace(S)\"),\n  `Halothane positive` = c(2, 3, 11, 16),\n  `Halothane negative` = c(76, 86, 73, 76)\n) \n\nlibrary(janitor)\n\ndpss_with_totals &lt;- dpss |&gt; \n  adorn_totals(\n    name = c(\"Row total\", \"Column total\"),\n    where = c(\"row\", \"col\")\n    )\n\ndpss_with_totals\n\n\n\n\n\n\nTable 4: Porcine Stress Syndrome (PSS) data\n\n\nBreed\nHalothane positive\nHalothane negative\nColumn total\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nRow total\n32\n311\n343\n\n\n\n\n\n\n\n\nExpected values are calculated assuming independence between the rows and columns. This gives the numbers in parentheses in Figure 1.\n\n\n\nFigure 1: Manual computation of ChSq statistic\n\n\n\n\n\n\n\n\n\n\nFor example the expected value in the (3, 2) cell, the number of British Landrace pigs which gave a negative test for halothane, is \\[E_{3,2} = T_3 \\times T_2 /n = 84 \\times 311/343 = 76.2.\\] The Chi-squared statistic is calculated as before using the formula \\(\\chi^2 = \\sum(O_{ij} - E_{ij})^2/E_{ij}\\). The Chi-square of 16.43 is associated with \\((4-1)\\times (2-1) = 3\\) degrees of freedom, and is therefore significant at the 5% level and at the 0.5% level. (Note that the tabulated value or critical value equals 7.81 at 5% significance level; and 12.8 at the 0.5% level).\nHence, there is strong statistical evidence that there are differences between breeds in the incidence of porcine stress syndrome. In other words, the breed is not independent of the result of the halothane test.\nNotice that the large counts in the second column (halothane negative) lead to large expected values but low contributions to the Chi-squared statistics (0.38, 0.35, 0.13 and 0.66). This is not surprising as each of these terms is divided by a large expected value. On the other hand, the small observations of the first column lead to small expected values but larger contributions to the Chi-squared statistic. Any Chi-squared contribution in excess of four (as explained previously) should be interpreted. In this example it is clear that Landrace (Swedish) pigs are more prone to porcine stress syndrome than are the other three breeds.\nAnother view of the idea of degrees of freedom can be obtained by noting that if the three expected values at the top of Column 2 are calculated (70.7, 80.7 and 76.2) then the remaining expected value can be found by subtraction from the appropriate row or column totals. Thus if the row and column totals are known, then only three numbers inside the contingency table are free before the whole table of numbers is then determined."
  },
  {
    "objectID": "studyguide/5-tabulated.html#example-paturition-in-sows",
    "href": "studyguide/5-tabulated.html#example-paturition-in-sows",
    "title": "Chapter 5: Tabulated Counts",
    "section": "Example: Paturition in Sows",
    "text": "Example: Paturition in Sows\nConsider the data on the effects of parturition in sows shown in Table 5.\n\n\nCode\ndsow &lt;- tibble(\n  Condition = c(\"Starvation\", \"Overlain\", \"Anaemia\", \"Infections\", \"Birth Defect\"),\n  `Control` = c(8, 8, 0, 3, 6),\n  `Injected` = c(10, 9, 12, 7, 2)\n  ) \n\ndsow_with_totals &lt;- dsow |&gt; \n  adorn_totals(\n    name = c(\"Row total\", \"Column total\"),\n    where = c(\"row\", \"col\")\n    )\n\ndsow_with_totals\n\n\n\n\n\n\nTable 5: Parturition in sows\n\n\nCondition\nControl\nInjected\nColumn total\n\n\n\n\nStarvation\n8\n10\n18\n\n\nOverlain\n8\n9\n17\n\n\nAnaemia\n0\n12\n12\n\n\nInfections\n3\n7\n10\n\n\nBirth Defect\n6\n2\n8\n\n\nRow total\n25\n40\n65\n\n\n\n\n\n\n\n\nIn an experiment to consider the effects of parturition in sows, 19 pregnant sows were given intramuscular injections of the drug prostaglandin while another 19 pregnant sows served as controls. Of the piglets born to the control sows, 25 died and were autopsied in the first 3 days and, for the treated sows, 40 died and were autopsied in the first 3 days. The chi square test is shown in Figure 2 and then the R output follows.\n\n\n\nFigure 2: Manual computation of ChSq statistic\n\n\n\n\nCode\n# Make the variable \"Condition\" the row names \n# so that the data frame contains only the \n# two columns and 10 data points \ndsow_dataonly &lt;- dsow |&gt; \n  column_to_rownames(\"Condition\")\n\ndsow_dataonly\n\n\n             Control Injected\nStarvation         8       10\nOverlain           8        9\nAnaemia            0       12\nInfections         3        7\nBirth Defect       6        2\n\n\nCode\ndsow_dataonly |&gt; chisq.test()\n\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  dsow_dataonly\nX-squared = 13.118, df = 4, p-value = 0.01071\n\n\nNotice that there are four cells whose expected values are less than 5 (4.6, 3.8, 3.1 and 4.9) and their contributions to the Chi-squared statistic are (4.62, 0.19, 2.78 and 1.74). Without these contributions, the Chi-squared statistic would not be significant. The conclusion should then be that there is not enough evidence to conclude that the Chi-squared statistic is significant. So there is not sufficient evidence to conclude that the cause of death depends on whether the injection was given.\nIf the Chi-square test involves many small expected counts, then we may obtain the \\(p\\)-value for the chi square statistic by Monte Carlo simulation (i.e. without relying on the Chi-square approximation). This simulation procedure involves random sampling from the set of all possible contingency tables having the same row and column totals. The R function chisq.test() can do this for you.\n\n\nCode\ndsow_dataonly |&gt; chisq.test(simulate.p.value = TRUE)\n\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  dsow_dataonly\nX-squared = 13.118, df = NA, p-value = 0.006997\n\n\nAs mentioned earlier, the Chi-square test is for actual counts and if you scale them you will get different results. For example, if we multiply Parturition counts by 100, and do the test, the conclusions will change.\n\n\nCode\ndsow_dataonly_100 &lt;- dsow_dataonly |&gt; \n  mutate(Control = 100 * Control,\n         Injected = 100 * Injected) \n\ndsow_dataonly_100\n\n\n             Control Injected\nStarvation       800     1000\nOverlain         800      900\nAnaemia            0     1200\nInfections       300      700\nBirth Defect     600      200\n\n\nCode\ndsow_dataonly_100 |&gt; chisq.test()\n\n\n\n    Pearson's Chi-squared test\n\ndata:  dsow_dataonly_100\nX-squared = 1311.8, df = 4, p-value &lt; 2.2e-16\n\n\nWe should also be careful when we combine two or more contingency tables into one. The association seen in a particular table may disappear after amalgamation. Simpson (1951) provided the theory on why this happens for contingency tables data and hence this paradox is known as Simpson’s paradox even though this was observed by others before him. Consider the following R outputs showing the ChiSq test for two separate groups and then the amalgamated group.\nFor Group 1 counts, the Chi-square test is shown below:\n\n\nCode\ngroup1 &lt;- cbind(c(80,30), \n                c(120,80)) \ngroup1\n\n\n     [,1] [,2]\n[1,]   80  120\n[2,]   30   80\n\n\nCode\nchisq.test(group1)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group1\nX-squared = 4.4809, df = 1, p-value = 0.03428\n\n\nFor group 2 counts, the Chi-square test is shown below:\n\n\nCode\ngroup2 &lt;- cbind(c(20,25),\n                c(75,20))\ngroup2\n\n\n     [,1] [,2]\n[1,]   20   75\n[2,]   25   20\n\n\nCode\nchisq.test(group2)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group2\nX-squared = 15.122, df = 1, p-value = 0.0001008\n\n\nFor the amalgamated contingency table, we obtain-\n\n\nCode\nall &lt;- cbind(c(100,55),\n             c(195,100))\nall\n\n\n     [,1] [,2]\n[1,]  100  195\n[2,]   55  100\n\n\nCode\nchisq.test(all)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  all\nX-squared = 0.053808, df = 1, p-value = 0.8166\n\n\nEvidently the association seen in the subgroups is lost after amalgamation.\nThe opposite can also happen. That is, when populations are separated somewhat parallelly by a factor like Gender, the overall association may not filter to the same level of association for the two gender groups."
  },
  {
    "objectID": "studyguide/3-probability.html",
    "href": "studyguide/3-probability.html",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "",
    "text": "“Misunderstanding of probability may be the greatest of all impediments to scientific literacy”\n— Stephen Jay Gould"
  },
  {
    "objectID": "studyguide/3-probability.html#introduction",
    "href": "studyguide/3-probability.html#introduction",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Introduction",
    "text": "Introduction\nThe concept of probability is the foundation of statistical inference. In other words, making inferences about a populations based on a sample relies on probability notions, and any uncertainty in our conclusions can be expressed in probabilistic terms. We also informally used the probability concepts in data collection, particularly with the EPSEM sampling methods.\nThe concepts we cover in this Chapter must be largely familiar to you from a first year course. Note that we are covering probability concepts and distributions at a very basic level without much maths."
  },
  {
    "objectID": "studyguide/3-probability.html#examples",
    "href": "studyguide/3-probability.html#examples",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Examples",
    "text": "Examples\n\nBlood group example\nThere are a large number of genetic-based blood-type systems that have been used for categorising blood. Two of these are the Rh system (Rh+ and Rh–) and the Kell system (K+ and K–). It is found that for any person, their blood type in any one system is independent of their blood type in any other.\nFor Europeans in New Zealand, about 81% are Rh+ and about 8% are K+. So, we can form a Table of counts as shown in Figure 2:\n\n\n\nFigure 2: Categorising blood types\n\n\nIf a European New Zealander is chosen at random, what is the probability that they are (Rh+ and K+) or (Rh– and K–)?\nThe answer is 0.0648 + 0.1748 = 0.2396 (verify).\nSuppose that a murder victim has a bloodstain on him with type (Rh– and K+), presumably from the assailant. What is the probability that a randomly selected person matches this type?\nThe answer is 0.0152 (verify).\n\n\nBeached whales example\nAssume that 0.63 is the probability of a randomly selected beached whale is a male. So the probability of a beached whale being female is \\(1-0.63=0.37\\). Suppose that we also observe whether beached whales are juvenile or adult, and the probability of a whale being juvenile, given that it is female, is 0.8, and the probability it is juvenile, given that it is male, is 0.4.\nLet us denote the random variables and their outcomes as follows:\nSex (S) = either female (\\(f\\)) or male (\\(m\\))\nAge (A) = either juvenile (\\(j\\)) or adult (\\(a\\))\nWe are given\n\\(P(S=m) = 0.63\\)\n\\(P(A=j | S=f) = 0.8\\)\n\\(P(A=j | S=m) = 0.4\\)"
  },
  {
    "objectID": "studyguide/3-probability.html#probability-tree-diagram",
    "href": "studyguide/3-probability.html#probability-tree-diagram",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Probability tree diagram",
    "text": "Probability tree diagram\nEasy to visualise the probabilities with this representation as shown in Figure 3.\n\n\n\nFigure 3: Probability tree diagram\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches.\n\nWhat is the overall probability that a whale is juvenile? That is, what is \\(P(A = j)\\) (or \\(P(j)\\) for short)?\n\\[P(j) = P(m \\textbf{ and } j) + P(f \\textbf{ and } j)= 0.63\\times 0.4+0.37 \\times 0.8 = 0.548 \\]"
  },
  {
    "objectID": "studyguide/3-probability.html#bayes-rule",
    "href": "studyguide/3-probability.html#bayes-rule",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Bayes rule",
    "text": "Bayes rule\nWhat is the probability of the whale being female given that it is juvenile? i.e., what is \\(P(f | j)\\) ?\n\\(P(f | j)\\) doesn’t appear in the tree anywhere, so we will have to go back to first principles.\nwe know that\n\\[P(f \\textbf{ and }  j)  = P(f | j) × P(j)\\] and\n\\[P(f \\textbf{ and }  j)  = P(j | f) × P(f)\\]\nThus, \\[P(f | j) × P(j) = P(j | f) × P(f)\\]\n\\[P(f | j) = \\frac {P(j | f) × P(f)} {P(j)}\\] This is known as the Bayes rule or theorem.\nFor our question \\[P(f | j) = \\frac {P(j | f) × P(f)} {P(j)}= \\frac{0.8 \\times 0.37}{0.548}= 0.54\\]"
  },
  {
    "objectID": "studyguide/3-probability.html#normal-quantile-quantile-plots",
    "href": "studyguide/3-probability.html#normal-quantile-quantile-plots",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Normal quantile-quantile plots",
    "text": "Normal quantile-quantile plots\nTo visually check whether a given set of data are normally distributed, one can create a normal quantile-quantile (Q-Q) plot. In this plot the quantiles of the sample are plotted against the theoretical quantiles of a standard normal distribution N(0,1). If the data, \\(Y_{i}\\), are normally distributed N(\\(\\mu, \\sigma\\)) then \\(Z_{i} = \\left(Y_{i} -\\mu \\right)/\\sigma\\) has a standard normal distribution. If we plot the quantiles of the data against corresponding quantiles of the standard normal distribution, the points should roughly lie on the straight line: \\[Y_{i} =\\mu +\\sigma Z_{i}\\] that is, a line with intercept \\(\\mu\\), and slope \\(\\sigma\\).\nWe should note that the mean and standard deviation do not appear in the formula \\(Y_{i} =\\mu +\\sigma Z_{i}\\) by coincidence. Rather, the mean and standard deviation are intimately connected to the normal distribution: if the data had not been normal then the line with these parameters would not pass through the data.\n\n\n\n\n\nFigure 22: Normal Quantile-Quantile Plot for Checking Normality\n\n\n\n\nFigure 22 shows the normal Q-Q plot for 50 random values from N(80,12). Note that the points in the middle largely plot on a line. It is natural that the tail part of the distribution shows more variation, and hence we will ignore such departures.\nThe normal Q-Q plot is presented as a normal probability plot in some software where the theoretical quantiles are replaced by the associated normal probabilities. Such normal probability plots show the sample data in the \\(X\\)-axis and the normal probabilities in the \\(Y\\)-axis. In both plots, we mainly look whether or not the points plot roughly on a straight line.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\",\n  destfile = \"rangitikei.RData\")\n\nload(\"rangitikei.RData\")\n\np1 &lt;- ggplot(rangitikei) +\n  aes(sample = people) + \n  stat_qq() + stat_qq_line()\n\np2 &lt;- ggplot(rangitikei) + aes(y=people, x=\"\") +\n  geom_boxplot() +\n  xlab(\"\") + \n  coord_flip()\n\ngridExtra::grid.arrange(p1, p2, ncol=1) \n\n\n\n\n\nFigure 23: Normal Quantile-Quantile Plot for Checking Normality\n\n\n\n\nFigure 23 shows the normal Q-Q plot for the number of people who made use of a recreational facility (rangitikei). This plot clearly reveals that the data are not normal. The plotted points show a curved but asymmetric pattern. This pattern matches with the right skewness seen in the data (see the boxplot given in Figure 23.\nNote that normality and symmetry are not the same. If the true distribution of the data is symmetric such as uniform, rectangular or triangular distributions, then the normal Q-Q plot will show a symmetric pattern for the plotted points but they will not plot on a straight line. As an example, the normal Q-Q plots for batches of random data from Student’s \\(t\\) (to be covered later in the next Chapter) and few other symmetric distributions is shown in Figure 24. Clearly normality is rejected for other symmetric non-normal distributions.\n\n\n\n\n\nFigure 24: Normal Quantile-Quantile Plot for Checking Normality"
  },
  {
    "objectID": "studyguide/1-data-collection.html",
    "href": "studyguide/1-data-collection.html",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "",
    "text": "“Data! data! he cried impatiently, I can’t make bricks without clay.”\n— Sir Arthur Conan Doyle, The Vopper Beeches"
  },
  {
    "objectID": "studyguide/1-data-collection.html#categorical-or-qualitative-data",
    "href": "studyguide/1-data-collection.html#categorical-or-qualitative-data",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Categorical or Qualitative Data",
    "text": "Categorical or Qualitative Data\n\nNominal Data\nAt this level, objects are classified by qualities or characteristics, and are placed in categories. Often these categories are really only names, for example Ford Focus, Mondeo, or Mustang; Boeing 737, 747 or 767, etc. Hence we describe the data as ‘nominal’. Sometimes confusion arises because the names are replaced (coded) by numbers as in the aeroplane example, or to facilitate data entry into a computer. An example of the latter would be a person’s marital status, coded as 1 (never married), 2 (currently married), 3 (widowed) or 4 (separated or divorced). It would make no sense to perform any mathematical manipulation on these numbers for they are only abbreviations for the categories. Notice that categories should cover all possibilities so that a further category may be needed, namely 5 (in a de facto relationship).\n\n\nOrdinal Data\nOrdinal data are categorical but the levels have an intrinsic order to them. Typical examples are a 3-point scale for burns - mild, moderate, severe, or a 5-point agreement scale - strongly disagree; disagree; no opinion; agree; strongly agree.\nThe distances between any two responses may differ. For example, disagree may be perceived by some respondents, although not others, as closer to strongly disagree than to no opinion. For this reason, there are problems in aggregating such responses. There are ‘nonparametric’ statistical methods to cope with these problems. It may be feasible, however, to add or average responses if the sample size is reasonably large and the assumption can be made that the differences in usage of the measures will average out over the sample."
  },
  {
    "objectID": "studyguide/1-data-collection.html#quantitative-i.e.-measured-or-counted-data",
    "href": "studyguide/1-data-collection.html#quantitative-i.e.-measured-or-counted-data",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Quantitative (i.e., measured or counted) data",
    "text": "Quantitative (i.e., measured or counted) data\n\nDiscrete Data\nDiscrete data usually arises by counting events or objects. Because of this discrete data are usually integers, but not always. For example, one may count the number of accidents last year at a particular intersection. On the one hand you can’t have fractions since you can’t have half an accident. On the other hand a further analysis may require you to state the proportion of the accidents that were nose-to-tail. Then if there were n accidents, the proportion of nose-to-tail ones could be \\(\\frac{0}{n}, \\frac{1}{n}, \\frac{2}{n}, ... ,\\frac{n}{n}\\) - a finite range of values, so the proportion is still discrete.\n\n\nContinuous Data\nWithout going into the mathematical definition of continuity, we tend to visualise continuous data as having a flowing nature, like water flowing under a bridge, or time passing in a continuous fashion. So one could in theory measure amounts (of water or time) to any level of precision, with many decimal places. Of course in practice, the measured responses will rarely be exactly continuous for, even if the underlying variables are assumed continuous, the instrument used to take readings will only be able to do so on a discrete scale. If the variable of interest is time, responses may be integers (say) 1, 2, … seconds or if the instrument can distinguish to the nearest \\(\\frac{1}{100}\\) of a second, the response will still be discrete but is now 1.00, 1.01 or 0.99. What we can say is that if the unit of measurement is sufficiently small then each individual possible reading will have an infinitesimal probability of occurring and we can basically ignore such individual points and only talk about probability in terms of intervals (e.g. one-second intervals rather than individual 10.99 seconds, say). Thus the practical difference between discrete and continuous data is that with real discrete data, outcomes each have their own separate probability of occurring, but with continuous data, probability is only thought of as associated with intervals.\n\n\nInterval and ratio scales\nStatisticians sometimes further distinguish among quantitative variables as being interval or ratio data. In fact some theoreticians write at length about the ‘four levels of measurement’: nominal, ordinal, interval or ratio.\nFor interval scale responses are assumed to be points on a linear scale. For example on a temperature scale the difference between 10 and 20 degrees is the same as between 30 and 40 degrees. On the usual Fahrenheit or Celsius scale there is a problem with respect to the origin, or zero point, as 20 degrees cannot be assumed to be twice as hot as 10 degrees. It would be meaningful to take differences between measurements or add them together but not to divide or multiply them. In many statistical formulae, deviations from means are involved which obviates the need for an absolute zero, and ratios such as slopes are quite acceptable.\nRatio scales are sometimes regarded the highest level of measurement, and are usually referred to by saying there is a fixed (or absolute) zero. An example would be the number of people in a room (can’t have negatives!) or the time taken to complete a given task. In this case, a reading of 4.2 seconds is twice that of 2.1 seconds. That is, it is meaningful to perform all the mathematical operations of addition, subtraction, multiplication and division.\nFrom a practical point of view, the type of data (categorical - nominal or ordinal; discrete; or continuous) may be crucial to the statistical analysis. It may be convenient to use a statistical method appropriate to ratio level data in the case mentioned above of the time to complete a given task, but the results may also be analysed by a non-parametric technique which actually only requires ordinal data. Similarly it may be satisfactory as a first step to analyse ordinal data by assigning scores {1, 2, 3, 4, and 5} to the categories and running the data through a regression package that assumes Normal data (better approaches are available!). Indeed, it may be difficult in some circumstances to clearly decide on the appropriate level, and the distinction particularly between interval and ratio levels is often unimportant."
  },
  {
    "objectID": "studyguide/1-data-collection.html#measuring-devices-or-instruments",
    "href": "studyguide/1-data-collection.html#measuring-devices-or-instruments",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Measuring Devices or Instruments",
    "text": "Measuring Devices or Instruments\nAny device used to collect measurements is called an instrument. It may be a physical device such as a measuring rule to gauge the heights of plants or a counting device such as a Geiger-counter for measuring radioactive material. In the social sciences or marketing, the instrument may be such a physical device or it may be a questionnaire which requires a more subjective response.\n\nMeasurement Error\nMeasurement error may arise if the instrument tends to give a reading which is too high or too low. Instruments should be checked as often as possible and standardised by checking them against known standards. This is not really possible with questionnaires but one should note the need for pilot testing the questionnaire on a group of subjects who are not in the frame for the survey.\n\n\nIndirect Measures\nIt is a common mistake to assume that the values reported for a variable are direct measurements of that variable. It is more usually the case that the recorded value reflects the desired value but is only an indirect measure of it. A doctor may wish to have some measure of the health of a patient, and, for this reason, a blood pressure measurement is taken, but blood pressure is not a direct measure of health for there are many inherited, climatic and individual effects which also play a part in the blood pressure reading. Even then a typical blood pressure reading would be required, but the mere fact that a measurement is being made will have an effect on the patient and possibly affect the reading. The stress of being in a hospital or a doctor’s surgery may increase the measurement.\nIt should be noted that the instrument used will only rarely be able to give a direct reading. Temperature is often gauged by the expansion of mercury in glass tube which is an indirect measure and will depend on a number of factors and assumptions, perhaps the greatest being that the expansion of the mercury is linear with temperature. In most experiments the measurements, even if indirect, are closely correlated with the variable under study but such assumptions should not be accepted too readily.\nAnother question which should be asked of data is who did the recording? Often, questionnaires require that people provide the information themselves, that is by self-reporting. Conscious or sub-conscious influences may have some bearing on the numbers provided. If someone is asked his/her income or workload, the answer may depend on what is perceived by the respondent as the aim of the survey, who is asking the question and who will have access to the results. A seemingly direct question may evoke an indirect answer. If the data is collected by a researcher or technician there may be some possible hiccups on the interface between the respondent and the researcher or the instrument and researcher."
  },
  {
    "objectID": "studyguide/1-data-collection.html#range-of-instruments",
    "href": "studyguide/1-data-collection.html#range-of-instruments",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Range of Instruments",
    "text": "Range of Instruments\nThe range of instruments is immense and in the remainder of this section a few examples will be given.\n\nObservation\nIt is common in the study of animals or birds to observe them in their natural settings. This may be done directly such as counting the number of wingbeats per minute or it may be of an indirect approach such as counting the frequency of spore in order to determine the number of animals in a given area. This method of data gathering is often costly and time-consuming. It may help to answer what is occurring rather than why.\nAs there is a high labour content with this instrument, errors may occur due to tiredness and boredom and coding errors may be a problem depending on how well defined are the categories to be observed. If many observers are involved, there is the added difficulty that the same behaviour may be perceived differently and hence coded differently by different observers.\nObservation does have an important part to play in a preliminary study to decide on the areas which require further study. Some researchers tend to bypass or shorten this phase by using a multivariate package on the computer hoping that it will sort out important relationships from a morass of complex data. It is surprising how many novel and correct scientific processes were discovered by such painstaking workers as Darwin and Mendel using only visual observation.\n\n\nInterviews\nThis method can suffer from some of the problems of observation in that it can be time consuming, involve coding errors and be influenced by different interviewers making different judgements.\nInterviewers need to be carefully trained so that they allow the person being interviewed to express his or her own views rather than interposing their own feelings or prejudices. On the positive side, a trained interviewer can sense nuances behind supplied answers in a way that observations or questionnaires cannot. A much richer range of responses can then be recorded which makes the interview ideal for pilot studies in particular as it can open up new avenues of study.\n\n\nQuestionnaires\nQuestionnaires are a common method used to collect data and they are very versatile as they can be administered by mail, telephone or face to face. Mailed questionnaires have the advantage of being efficient time-wise for a researcher and may be less threatening to the respondent as they can be answered when and where the respondent chooses. They are appropriate if the respondent needs to give some thought to the questions or where the answers involve sensitive information. Unfortunately, this flexibility can lead to respondents procrastinating or not replying at all. The problem of non-response will be considered later.\nHow long should the questionnaire be? Almost invariably, it is too long as the researcher strives to obtain as much information as possible. A long and tiresome questionnaire may be counter-productive as the quality of the data may suffer. There has been a lot of research on questionnaire design. Some points to note are that straightforward questions should be asked near the beginning so that the respondent feels at ease while questions requiring judgements or moral decisions should come later."
  },
  {
    "objectID": "studyguide/1-data-collection.html#the-human-interface",
    "href": "studyguide/1-data-collection.html#the-human-interface",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "The Human Interface",
    "text": "The Human Interface\nMention has been made of some problems associated with respondents giving replies and researchers seeking to understand those replies and to code them. It is worthwhile taking more time to reconsider these processes for the quality of data will invariably be greatly affected by the human interface with the research instrument.\nThe yields of different varieties of field grasses would seem to be a clear cut and well defined and measured quantity but the yield would be influenced by many factors such as the height of the cut, perhaps the speed at which the mower is operated and the ability of the assistant to mow within the specified boundaries.\nWith more subjective instruments and human respondents other problems may arise and affect the quality of the data.\n\nHuman Respondents\nA number of possible difficulties may arise:\n\nA question, which is clear-cut and unambiguous to the researcher, may not be so to the respondent who may not understand the question or is not too sure about it because it is framed in unfamiliar language or relates to something outside his or her experience.\nThe response may depend on a number of extraneous factors such as how cheerful and at ease the respondent feels.\nQuestions may require remembering certain facts or feelings and memory can be very selective.\nThe situation may involve many variables while the respondent is asked to respond on only one dimension, which may be perceived as a composite of these other variables. The respondent may not be able to make the judgements necessary to give an accurate response. In one questionnaire, some questions may be answered very accurately while others may be of doubtful value. If responses are added in some way to give a total, there may be a problem in that some of the components have been measured accurately but others are less precise.\nQuestionnaires can be self-fulfilling prophecies in that the structure and language of the questions suggest the correct or desired responses. Socialising factors may prejudice the results in that most people are hesitant to be rude or to reveal to strangers their deep desires and true feelings, particularly if these are thought to be socially undesirable.\n\n\n\nResearchers\nThe human interface also arises when someone administers the instrument of the research and some points to note are:\n\nThe competence of the researchers and assistants is important. Judgement may be required to understand the response and to make a note of it. In an interview situation, the researcher must listen and try to understand the responses in terms of his or her framework of judgements and in relation to the aim of the interview. With more objective instruments, they must be read accurately but even with such measuring scales one assistant may tend to read high or to record even calibrations whereas another would not do so, which introduces an unwanted variation in the data collection process.\nThe feelings and beliefs of the assistant may intrude into the collection process. The interaction with the subject in this way may introduce other unwanted variation.\nIt is a salutary point to note that usually the individual who collects the raw data is an underpaid and possibly under-trained junior who is not highly motivated to produce high quality data.\n\n\n\nSelf-Administered Tests\nIn some situations, the subject records the response as in self-administered questionnaires, for example, tick a box or circle a number. There may be advantages in this for the respondent is presumably the best person to know what response should be given. On the other hand, the respondent is usually not trained so that there may be a higher incidence of coding errors and questions which are left unanswered. On the other hand, if the researcher does the coding, unwarranted assumptions may be made.\nIt is conceivable that many human interventions and possible errors may occur in an experiment. For example, a field worker may make certain observations which are studied by another assistant who decides on categories for them while a further assistant codes these categories and finally another person enters them into a computer. Errors are possible at each step along the way.\nAt the other extreme, it is possible for information to be relayed directly to the computer without human interference. Microprocessors can be used to collect and transmit a whole variety of information. In experiments on animals, sensors can be placed on different parts of their bodies to relay information to the computer. These facilities open up new vistas and challenges for statisticians to monitor large amounts of data, for it may not be obvious how to cope with them."
  },
  {
    "objectID": "studyguide/1-data-collection.html#non-response-bias",
    "href": "studyguide/1-data-collection.html#non-response-bias",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Non-response Bias",
    "text": "Non-response Bias\nNon response is very common in all types of data collection and is probably the largest source of non-sampling error. It can occur at three stages of the process. An element may be selected but not found. For example, sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey. It may be hiding or perhaps has died. If the elements are people, some may not be at home at the time, or, if it is a telephone survey, the telephone may be cut off from certain selected individuals.\nIt may not be possible to take a measurement. This happens quite often with people as some may forget, or refuse, to answer the questionnaire. The measuring instrument may not work or give a ludicrous response which may be deleted by the technician. An element may be contacted, a measurement taken but the result not written down in the correct place, or could be so illegible that it cannot be used.\nWherever possible, nonresponse bias should be reduced by making every possible effort to obtain a response. When interviewing people about a particular product, market researchers will often make two call backs (that is three attempts in all) to reach a subject if that person is not at home on the first call. Cost factors will often prohibit further attempts. Surveys by mail often include a stamped addressed envelope or even money or a token for merchandise to arouse moral scruples and force a reply.\nThe problems arising from nonresponse can be illustrated with a simple example: Suppose that 200 people are asked their opinion on a certain issue, only 100 reply of whom 70 are in favour, 30 opposed.\nWhat percentage of the original 200 are in favour?\nThe answer depends on the assumptions made.\n\nSuppose the nonrespondents feel the same about the issue as the respondents. (This is the usual assumption made). This gives:\n\n\n\n\nCase\nIn favour\nOpposed\nTotal\n\n\n\n\nRespondents\n70\n30\n100\n\n\nNonrespondents\n70\n30\n100\n\n\nTotal\n140\n60\n200\n\n\n\nThus the estimate of the number in favour is then 140/200 or 70%.\n\nSuppose that those who did not respond did so because they were opposed to the issue. Thus\n\n\n\n\nCase\nIn favour\nOpposed\nTotal\n\n\n\n\nRespondents\n70\n30\n100\n\n\nNonrespondents\n0\n100\n100\n\n\nTotal\n70\n130\n200\n\n\n\nThe estimate of the number in favour is 70/200 or 35%.\n\nIt may be reasonable in other circumstances to assume that the nonrespondents were quite happy with the status quo and were in favour. Then the estimate of the number in favour is 170/200 or 85%.\nIt may be possible to make other reasonable assumptions if additional information is at hand on the whole batch of 200. We can massage the results in different ways but none of them is very convincing. The moral is: Try to reduce nonresponse by all legitimate means."
  },
  {
    "objectID": "studyguide/1-data-collection.html#some-terminology",
    "href": "studyguide/1-data-collection.html#some-terminology",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Some Terminology",
    "text": "Some Terminology\nIn a census, every element of the population is contacted, counted and other quantities collected or evaluated. Conceptually, the procedure is straightforward and perhaps trivial. In practice and particularly if the population is large, difficulties often arise. There are three critical operations in census taking, namely identifying an element as belonging to the population under study, making contact with that element and obtaining the appropriate information in a suitable form.\nIt may be helpful, at this point, to introduce some terminology to formalise the process of taking a census. The population at which the study is aimed is termed the target population . To obtain information from this population it is necessary to have a means of operationalising the data collection. This involves a frame, which may be a listing of the elements of the population or it may be an operating method such as a geographical map, suitably divided into smaller, manageable areas. A further step is necessary to contact each element of the frame and collect the appropriate information. The resulting set of elements on which completed, usable data has been collected could be called the actual population. It is possible to consider these populations as consisting of the measurements under study rather than the elements, be they objects, animals or people. More will be said on this later.\nCensus taking can then be represented by the sequence\ntarget population \\(\\Rightarrow\\) frame \\(\\Rightarrow\\) actual population.\nIdeally, these three populations would consist of the same elements but in practice they could be represented in Figure 1).\n\n\n\nFigure 1: Relationship between the target population, frame and actual population\n\n\nA simple example of a census would be the exercise of collecting information from the students in a particular school. Suppose the school authorities want to discover, from each student, the number of younger siblings who may wish to attend the school in future years. At first glance, the target population is well defined, as the current students in the school. However queries could arise such as whether adult or part-time pupils should be included. The most suitable frame would, no doubt, be the school roll. Hopefully, the roll would accurately reflect the students currently at the school but it may not exactly match the target population. Some students may be temporarily visiting the school, either staying with relatives or a parent who has a local, temporary job. On the other hand, some students on the roll may be visiting another part of the country, or may be in a hospital or other institution, and there may be some doubt whether their absence is temporary or permanent.\nFrame inefficiencies may well occur to the extent that the frame does not match the target population. The collection of information on siblings may encounter snags as some students may not respond due to absence, or parents or student may refuse to divulge the required information. The actual population may then fall short of the frame, which contains the maximum possible elements, which in this case, are students."
  },
  {
    "objectID": "studyguide/1-data-collection.html#elements-measures-and-associated-variables",
    "href": "studyguide/1-data-collection.html#elements-measures-and-associated-variables",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Elements, Measures and Associated Variables",
    "text": "Elements, Measures and Associated Variables\nWith a census, there is usually some ambivalence as to whether the objects under study are the elements of the population (such as plants, animals or people) or the measurements on these elements. The measurements may simply be the count, or may be such variables as the length, weight, salary or any other measure. The selection process will focus mainly on the elements, but from then on attention will be focussed on measurement and then analysis. These two ideas can be incorporated into the notation \\[U=\\left(L,Y,X\\right)\\] Here \\(U\\) denotes the population, \\(L\\) is the set of labels \\((i = 1,2,...,N)\\) attached to the \\(N\\) elements of the population, \\(Y\\) is the measurement, or response, under study from each element in the population, and \\(X\\) is an associated variable measured on each element in the population.\nFor example, for the census carried out on the students of a high school, the labels \\(i\\) could be the identification numbers of the students on the roll \\(L\\). The measurement on the \\(i^{th}\\) student, represented by \\(y_i\\), may be a variable such as the daily expenditure at the school tuck shop. The associated variable, \\(x_i\\), for the \\(i^{th}\\) student may be the student’s weight. On the other hand, the mark a student gets on a standardised test when entering high school may be \\(x_i\\), with \\(y_i\\) the school certificate mark of that student in mathematics.\nIn this example the survey may, in practice, be carried out as a sample rather than a census but conceptually it would be possible, to carry out such a census. Another point which may need discussion and clarification is whether the population represented by \\(U\\) should refer to the target population, the frame or the actual population; usually \\(U\\) would refer to the target population."
  },
  {
    "objectID": "studyguide/1-data-collection.html#non-sampling-errors",
    "href": "studyguide/1-data-collection.html#non-sampling-errors",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Non-sampling Errors",
    "text": "Non-sampling Errors\nA number of errors may occur in collecting information even when a census is being taken. The elements of the census must be contacted, a measurement or observation taken and this information recorded. As no sampling procedure is involved, it seems reasonable to label errors as non-sampling errors.\nThe goal of the survey will be to produce summary statistics such as the population mean and standard deviation. If errors are committed in the collection process, the summary statistics may be incorrect. It is possible that these errors will cancel out but we cannot rely on this. The errors may lead to a tendency for the calculated mean to be too low and the mean is said to be biased. Alternatively, the bias may be on the high side and, at times, it may be difficult even to decide the direction of the bias although we know there is a strong possibility of it occurring."
  },
  {
    "objectID": "studyguide/1-data-collection.html#selection-bias",
    "href": "studyguide/1-data-collection.html#selection-bias",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Selection Bias",
    "text": "Selection Bias\nIt may seem a contradiction in terms to speak of errors in selecting elements of a census for the aim is to contact every element in the population. From the discussion in the above section, it is clear that some difficulties may arise in the sequence from target population to frame and these difficulties are often called frame inefficiencies.\nA survey may be carried out on all exporting firms in New Zealand. The frame may be a published list of exporting firms but the list may be out of date as some firms have gone broke or been taken into receivership. There may be firms not on the list but which, nevertheless, do engage in some form of exporting besides their other activities.\nOnce the elements of the frame have been located, there remains the step of extracting information from them. For some reasons particular firms in the frame may not wish to take part in the survey so that errors may occur in this second step also."
  },
  {
    "objectID": "studyguide/1-data-collection.html#inference",
    "href": "studyguide/1-data-collection.html#inference",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Inference",
    "text": "Inference\nIt is not appropriate to carry out any statistical test or form a confidence interval on census data because they are measurements on the whole population. Statistical inference is using data from a sample to make statements about the broader population from which the sample was taken. When we do a census, we have the whole population, so we are no longer making inferences.\nNext, we’ll consider sampling from the population instead of taking a census."
  },
  {
    "objectID": "studyguide/1-data-collection.html#introduction-1",
    "href": "studyguide/1-data-collection.html#introduction-1",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Introduction",
    "text": "Introduction\nSampling as a scientific exercise is a peculiarly twentieth century phenomenon despite the fact that sampling has been used in an ad hoc manner from earliest times. Sampling, as in sample surveys, was accepted only after many years of argument and counter-argument. The most intensive and extensive discussion took place at the International Statistical Institute (ISI) meeting in Berne in 1895. Kiaer explained the ‘representative method’ he had used in Norway. Further meetings of the ISI enabled Kiaer to expand and clarify his approach. Four principles have been identified in Kiaer’s reasoning:\n\nRepresentativeness in terms of adequate representation of identifiable groups in the population.\nSelection of objects to be as objective as possible.\nThe reliability of the results should be assessed. Each survey should be divided into a number of distinct parts using a different representative method for each. Comparison of results of these parts would provide evidence as to how much faith could be placed in the results of the survey.\nFinally, Kiaer insisted on a complete specification of the method of selection.\n\nBowley (1906) provided a theory of inference for survey samples which helped to answer the question of how accurate an estimate from a large sample was. Using a Bayesian argument, his approach was only valid for the case where the chances were the same for all items of the groups to be sampled.\nIt was not until Neyman (1934) that random, rather than purposeful, sampling was given a strong mathematical justification. Neyman showed that it did not matter if equal or unequal probabilities were used provided that probabilities were known in advance. His arguments were not based on specific distributions as in Bayesian approaches so that the touchstone of randomness seemed to have finally won the day. For an interesting discussion on the controversies in the history of survey sampling, see Brewer (2013)."
  },
  {
    "objectID": "studyguide/1-data-collection.html#why-sample",
    "href": "studyguide/1-data-collection.html#why-sample",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Why Sample?",
    "text": "Why Sample?\nThere is a widespread belief that a census, rather than a sample, should be taken if at all possible. Most people feel secure with a census but are somewhat uneasy about only collecting a sample and using mathematical means to estimate properties of the population. Lack of sufficient resources of money, trained manpower and so on is often the motivation to reject a complete enumeration. A compromise, of course, is necessary in that a cheaper, quicker collection method using a sample may result in less accurate estimates. Often, however, the reduction in accuracy is slight compared with the considerable reduction in cost. Time is also a factor in that a census may require officials to collect data over a long period which may raise difficulties in that the responses would be meant to relate to a particular point in time.\nAn important point which is often overlooked is that a complete enumeration is only superior to a sample if the data is collected and processed at the same high standard. Often, researchers try to collect too much data for the resources and trained man-power available which results in a large amount of data whose accuracy and validity is suspect. A well collected sample will always be of more use than a shoddily taken census. Another common fault is the attempt to collect too much information from every element, be it an object, animal or person. A questionnaire has a tendency to expand to the point that it frustrates or annoys the recipient or the personnel administering it. In countries where monthly household surveys are held, their results are invariably more accurate than the less frequent population censuses.\nOne classical case in which a sample is preferable to a census is that of destructive sampling in which each object is tested until it breaks or expires. For example, testing the number of hours a light bulb shines before it burns out. The selected items are useless after the test so that there are compelling reasons for limiting the size of the sample. It is salutary to realise that many people selected to give information or to take part in tests or experiments will be affected in some ways. The process of collecting data will itself affect the elements in the sample so that they may no longer be representative of the population about which inferences are to be made. For example, in some quarters, there is a fear that political opinion polls do not merely reflect public opinion but to some extent form it.\nNonsampling errors, nonresponse in particular may introduce biases. Unfortunately, the size and/or direction of these biases are generally unknown. Due to the smaller size of a sample, it may be much easier to reduce nonresponse than in a much more cumbersome enumeration. Call-backs and other procedures which have been devised to reduce nonresponse are very time consuming so that they are rarely feasible when the number of elements is large.\nTo reduce unwanted effects such as interviewer biases, statistical tests can be employed. These require certain assumptions about the population and selection method which have been used. We can never be sure that the assumptions associated with these tests are valid. If the sample suggests that they may not be true, some action must be taken such as a data transformation to make the results more ‘Normal’."
  },
  {
    "objectID": "studyguide/1-data-collection.html#the-case-for-randomisation",
    "href": "studyguide/1-data-collection.html#the-case-for-randomisation",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "The Case for Randomisation",
    "text": "The Case for Randomisation\n\n“Randomization is too important to be left to chance.”\n– J. D. Petruccelli\n\nThe sample should be similar to the population. It is easier to state this than it is to define what is meant by the statement and how to achieve this similarity. Randomisation can be used to obtain a sample which, on average, has similar properties to the population. Not only is this so but it is possible to estimate how closely the sample reflects the population and attach a probability to the possible discrepancy. It is not possible to state with certainty that a particular measurement from a sample is peculiar although it would be possible to attach a probability to such an unusual value. Similarly, it would be possible to make a probability statement about the sample itself if the sampling process was replicated.\nIf the distribution of the variables is known, randomisation enables the distributions of some combinations of the sample values to be calculated. In common with other areas of statistics, it is more likely that the distributions of these variables are not known although these distributions may be hypothesised. The validity of any conclusions will be subject to the proviso that the assumptions are correct. If the distributions are not known the Central Limit Theorem (CLT) may be invoked so that the approximate Normal distributions may be assumed for some estimators.\nOn the other hand, there are enticing reasons for placing restrictions on the sample to make it similar to the population in many respects such as proportions in certain subgroups. Surveys are rarely carried out as a theoretical exercise and results may only be believed if those who commissioned and financed the survey believe in its accuracy. If the sample appears peculiar in relation to some known variables, the validity of the survey may be called in question. To obtain a representative sample to fit the known, or assumed, distribution in the population, the elements may be selected purposely. The resulting estimators of variables may be very accurate but there is no way of determining how accurate they are.\nAs a compromise, it is possible to use a randomisation scheme with certain restrictions. A simple example is that of stratified sampling with different probabilities of selection for each stratum. This procedure can be extended to allow each element in the population to have its own probability of inclusion into the sample. Alternatively, a randomisation approach can be used to draw a sample which may be rejected, however, if certain restrictions are not fulfilled. An important point is that the sample should be chosen with a known probability. Another point to note is that the known information about a population will involve a variable, for example \\(X\\), whereas the measurements to be collected are from a another variable, say \\(Y\\). The efficacy of the survey will depend heavily on the accuracy of \\(X\\) and the relationship between \\(X\\) and \\(Y\\).\nIt should be noted that restrictions may limit the number of elements selected from particular subgroups so that it may not be reasonable to invoke the Central Limit Theorem and assume that estimators in these subgroups follow a Normal distribution."
  },
  {
    "objectID": "studyguide/1-data-collection.html#terminology-and-notation-for-samples",
    "href": "studyguide/1-data-collection.html#terminology-and-notation-for-samples",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Terminology and Notation for Samples",
    "text": "Terminology and Notation for Samples\nA sample is a subset of a population. There are innumerable ways of drawing a sample so that there is a richness and variety in sampling methods which do not exist in census methods. The first step in obtaining a sample is the same as for taking a census in that a frame must be set up. The sample is then drawn from the frame so that the sequence is target population \\(\\Rightarrow\\) frame \\(\\Rightarrow\\) sample.\nThe relationship between population and sample is shown in Figure 2 (which is almost the same as Figure 1).\n\n\n\nFigure 2: Relationship between the target population, frame and sample\n\n\nIt was pointed out above that a population, in practice the frame, could be written as \\(U=\\left(L,Y,X\\right)\\). By contrast probability samples could be denoted by \\(S = \\{P_r, l, y, x\\}\\). The curly brackets, {.} are meant to indicate that there are many samples which can be drawn from the one population. Note that \\(S\\) is the set of all possible samples, \\(s\\), selected under the probability scheme \\(P_r\\). The labels in the sample as well as the sample values for \\(Y\\) and \\(X\\) are denoted by lower case letters. In general, the convention is to use capitals when referring to the population but lower case for the sample. The term \\(P_r\\) indicates the probability, \\(P_r(s)\\), of the sample \\(s\\) being selected.\nAnother way to look at this is to consider the probability that a particular element, or label, be included in the sample. Let \\(\\pi_{i}\\) be the inclusion probability for the \\(i^{th}\\) label. A special but common occurrence is the equality of the \\(\\pi_{i}\\) for \\(i\\) from \\(1\\) through \\(N\\) and, in this case, the \\(P_r(s)\\) will be the same for all \\(s\\) in \\(S\\). In this case, the selection is said to be EPSEM, standing for equal probability of selection, or it is said that the elements are self weighing as the probabilities do not depend on the particular labels which have already been selected. The most obvious example of a self weighting sample is that of a simple random sample but multistage samples can also be arranged with unequal probabilities at each stage but such that the inclusion probabilities are equal."
  },
  {
    "objectID": "studyguide/1-data-collection.html#errors-in-samples",
    "href": "studyguide/1-data-collection.html#errors-in-samples",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Errors in Samples",
    "text": "Errors in Samples\nSamples will be liable to the same kind of biases as a census as well as others inherent in the sampling process. Major sources of biases in samples are classified in Figure 3. On the one hand, the biases can be split into those which occur at the time of selecting the sample, while others occur at the stage of collecting and operation on the data. On the other hand, a division can be made into sampling and nonsampling, depending on whether the bias occurs in a random or fixed manner. The nonsampling errors can be further subdivided according to whether an observation is made or not.\n\n\n\nFigure 3: Classification of Sources of Survey Biases\n\n\nBriefly, the cells in Figure 3 refer to the following problems:\n\nFrame inefficiencies: To select a sample, a frame, which is often a listing but may be such as a map, is used. If certain items are missing or occur more than once, these items and the others in the frame will have a probability of selection which is different from what it should be.\nBiased estimation: In this course, only unbiased estimates such as the sample mean are considered.\nNon-observation: Non-observation could be due to frame inefficiencies or not being able to locate the item.\nNon-response: The item, say a particular person, may be located but he or she may be not at home or refuse to answer all or some of the questions.\nPurposive Selection: When selection occurs deliberately rather than at random then biases will often intrude. In quota sampling, the interviewer is told to contact so many people over 45, or male, or in some other category. The personal choice involved in this selection could introduce bias.\nInterviewer bias, coding errors: Certain characteristics of the interviewer may incline the subject to respond in a certain way. With another interviewer, the subject may respond differently. Once the data is collected, errors can occur in an amazing number of ways in coding the answer, transcribing to summary sheets, entering the data into a computer, as well as error in computer programming.\n\nOther errors are termed random or stochastic in that probabilities are involved so that samples, but not censuses, may suffer from these errors. The assumption is made that the measurement is correct but the estimator varies depending on the sample which has been drawn. Rather than giving the name error to this outcome it would be preferable to speak of sampling variation.\nOur aim in sampling and in all statistical estimation, is to find an estimate which is close to the population parameter. Of course, we have a problem in defining “close” as we do not know if our sample is representative of the whole population or whether it is an unusual one. If the sample yields an estimate of the population parameter we can at least talk about the expected or average distance of the estimate from the parameter.\nAn illustration of the ideas of bias and variance is that of shots fired at a target. If the shots are widely scattered, they exhibit a large variance. If they are spread around the target so that on average they are equally scattered around the bulls-eye (neither too high nor too low etc.), we could say they are unbiased shots of the bulls-eye. The four possible cases are shown in Figure 4).\n\n\n\nFigure 4: Bias and variance\n\n\nNotice that bias can occur in census as well as sample data, but we can only talk about variance in a probability sample. From a single sample yielding one value of a statistic, it is not possible to estimate the bias in the statistic but an estimate of the variance of the statistic can be obtained from the variation in the sample.\nEach sample is not an exact match of the population so that the resulting estimator will invariably differ from the population parameter it is estimating. As the parameter is unknown, the actual difference between the estimator and the parameter is also unknown so that it is important to have certain assumptions about the distributor of the variable under study and distribution of the resulting estimator. Mathematical theory is often available to indicate the amount of bias and variance in the estimator.\nUsually, a sampling method is chosen to ensure that the resulting estimator is unbiased. A peculiar sample does not necessarily mean that the estimator is biased but it may be difficult to convince others that this is so. For example, it may turn out that the sample consists only of females even though the population is evenly divided between the sexes. If females and males give similar responses to the variable under question, the strange composition of the sample in this case would not affect the value obtained for the estimator. On the other hand, a sample which seems to be representative of the population on certain variables may yield a strange value for the estimator. One must assume, for there is no proof as the parameters under study are unknown, that if the sample is chosen carefully, which usually means that an appropriate randomisation scheme is employed, the measurements and hence the estimate obtained will be representative of the population.\nOften, the sample and the estimator are chosen in such a way that the estimator is unbiased. The sampling scheme and the estimator can be considered as two sides of the same coin so that the estimator is chosen to fit the sampling scheme. Furthermore, the sampling scheme can be chosen in such a way that the variance of the estimator is a minimum subject to certain constraints such as cost and time. At times an estimator is chosen deliberately to be biased if in so doing the variance is made smaller. To compensate for this for it may be that a biased estimator will ensure that it is closer to the parameter of interest than an unbiased estimator. These considerations are of more interest to the analysis of data than to their collection; so this topic will not be considered at length here.\nIn sample surveys, an estimator may turn out to be biased because the probability of selection of certain elements is different to what the researcher thought. If the frame used to select people is a list of homeowners, wealthy owners would have a higher probability of selection for they are more likely than less well off people to own more than one house. Consequently, an owner of multiple houses would have a high probability of selection which would bias the estimate towards such people unless the multiple ownership is taken into account. This source of error is termed frame inefficiency and it is likely to arise whenever there are discrepancies with the sampling frame."
  },
  {
    "objectID": "studyguide/1-data-collection.html#simple-random-sample",
    "href": "studyguide/1-data-collection.html#simple-random-sample",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\nIn our lottery example, each of the 1,000 tickets has the same probability (1 in 1,000) of winning any one prize and, indeed, of winning any one of the 3 prizes (3 chances in 1,000, that is, 1 chance in 333).\nIf the researcher has no information about the sampling frame, except perhaps the number of elements \\(N\\) present, simple random sampling may be a feasible approach. It is common in experimental design when treatments are allocated to experimental units. In quality control, simple random sampling is often used.\nIf the population is large, at least in relation to the sample size, it would usually be inefficient to use simple random sampling for it would require considerable time and cost to select and contact the elements of such a sample. If the sampling fraction is large, (say) more than ten percent, it may be reasonable to use simple random sampling as it is likely that the sample will be spread reasonably evenly throughout the population.\nThe more additional information available, the more structured the sampling design can be. The additional information may be of an exact dichotomous or polychotomous nature such as the natural division of the frame into three disparate geographical areas. The additional variable may be continuous such as the age structure of the frame. In other cases, the information may be assumed rather than known for certain. For example, the educational level achieved by every person in a frame may be listed but this information may be a few years out of date. In these cases, a sample survey may be taken based on the age structure of the frame, but the conclusions will only be valid to the extent that there has been no change in this age structure.\nThe additional information can be the basis for the logistics of the survey as different parts of the frame may require more intensive sampling or different methods of organisation. For example, if the frame covers rural and urban areas, the urban sectors may be reached by public transport, but respondents may not be at home during the day as they are likely to be at work. The rural areas may involve other difficulties due to the scattered nature of the population.\nThe obvious danger is that sampling decisions made for administrative ease may hinder accumulation of data and may discredit inferences drawn from the resulting sample. For example, one must query whether the responses collected by one interviewer are likely to be considerably different from those of another, whether data collected by face-to-face methods can be aggregated with data collected by telephone interviews; whether data collected on some elements can be assumed to be similar to other data collected at a later time; whether responses to questions which must be translated into different languages, or explained in simpler English to someone of limited education or facility in English, can give responses which can be aggregated with responses to these same questions when not translated. There are no easy answers to questions such as these but researchers should be aware of such difficulties. Whenever possible, the peculiarities in the collection method should be noted and made available to the reader so that he/she can better judge the conclusions of the survey.\nEstimators with a low variance are likely to be more accurate in that they reflect population values more closely. The use of additional information can lead to a reduction in the variance of estimators such as sample means and sample proportions. This gain in efficiency will depend on the strength of the relationship between the additional information and the variable under study. At times, this relationship may be estimated but, at other times, it must be taken for granted.\nIf the population is small but there is no information available on other aspects of the population, a simple random sample may be appropriate. Other sampling schemes may be used, though, for some of the following reasons:\n\nAdministrative problems\nIf the population size is large, it will be rather clumsy to try to select a simple random sample. If the population is spread out geographically, then considerable cost may be involved unless the sample is grouped in some way.\n\n\nImprove accuracy\nUnder certain conditions, or assumptions, it may be possible to use a scheme which leads to more accurate estimates - accurate in the sense that the variance and/or bias is reduced.\n\n\nPolitical reasons\nA simple random sample of (say) supermarkets in New Zealand may, by chance, consist only of stores in Auckland. The results of this survey may be viewed very sceptically by readers and, more importantly, by those who are funding the survey."
  },
  {
    "objectID": "studyguide/1-data-collection.html#stratified-sampling",
    "href": "studyguide/1-data-collection.html#stratified-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nStratified sampling is a very versatile approach. It can be easy to implement, and can also lead to more efficient estimation of parameters. With stratified sampling, the population is divided into groups called “strata”, and then random samples are taken from each stratum. The advantages of this approach are:\n\nIt is a flexible approach, as strata can be designated in different ways.\nEach stratum can be treated as a separate population so that strata could be surveyed at different times and even different ways. For example, a survey in NZ could be divided into north and south island, urban and rural; the rural sample could be contacted by telephone but the urban by face-to-face interviews. One stratum could involve systematic sampling; other involve simple random sampling.\nIf we have information regarding the variation in strata then the resulting estimator (of the population mean for instance) could be more precise (less variance) than in simple random sampling if the more variable strata are sampled more intensely.\nIt is probably the most common method employed in sample surveys.\nThe sample is spread throughout the population.\nThe variance of an estimate from stratified sampling is usually equal to, or less than, the variance of an estimate from simple random sampling. It would be sensible to sample more heavily the larger strata. This is the approach taken in proportional (stratified) sampling in which the sample size in each stratum is proportional to the size of the stratum. Proportional sampling is appropriate when all the strata are equally variable. When this is not the case the more variable strata are sampled more heavily."
  },
  {
    "objectID": "studyguide/1-data-collection.html#cluster-sampling",
    "href": "studyguide/1-data-collection.html#cluster-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nFor some investigations, it is advisable to consider the population as being composed of clusters or groups and to proceed with the sample survey by selecting certain clusters and collecting measurements from (usually) a random selection of the elements within the chosen clusters. Cluster and stratified sampling differ in that all the strata are sampled whereas only some of the clusters are sampled. It is usual to have a large number of small clusters in cluster sampling but a small number of strata in stratified sampling. Consider a sample survey of the pupils in a school. The classes within the school may be considered as possible strata or possible clusters. There are advantages for cluster sampling in this case.\n\nEase of administration. It will be easier to select certain classes in a school (say) and interview a random sample or all of the students in those classes rather than selecting students from all the classes in the school.\nA listing of all the children is only necessary for the selected classes.\nConfidentiality and/or independence of response may be aided by this approach. It would take longer to collect information from all the classes in the school and the longer it takes the more likely it is that respondents will talk among themselves about the questions before the researcher can collect his/her information. If the whole class is presented with the questionnaire at the same time, confidentiality is more likely to be preserved.\nLocating elements (students) is easier, travel (between classrooms) will be reduced, and it is easy to divide responsibility between interviewers.\n\nIn some cases, it makes more sense to collect measurements on whole clusters rather than just a few individuals within each selected cluster. This is probably true in the case of the above example. Surveys on TV watching collect information on a family is another example where this is probably true. If the TV set is on, it may be difficult to pin down which members are watching as some may claim not to be interested (although they may know quite a lot about that program). Alas, there must be some payment for these benefits and this is reflected in the higher variances of cluster sampling estimators over those of simple random sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#systematic-sampling",
    "href": "studyguide/1-data-collection.html#systematic-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn this section, we shall only consider a particular type of systematic sampling, namely that which includes a component of randomness, as this allows probability statements to be made.\nIn the simplest presentation, the frame is assumed to consist of \\(N\\) elements where \\(N\\) = \\(kn\\) with \\(n\\) being the required sample size. A random digit, \\(i\\), in the range of l through \\(k\\) is chosen as a starting point and then every \\(k^{th}\\) element from then on is placed in the sample. The sample then consists of the elements \\(i, i + k, i + 2k,\\) etc. Clearly this approach is likely to produce a sample which is evenly distributed over the entire sampling frame, and it is therefore likely that the sample will be representative of the population. This is particularly true when the sampling frame is sorted geographically. However, when this is not the case, the variance of the estimators obtained using systematic sampling will generally be greater than those obtained for simple random sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#probability-proportional-to-size-pps-sampling",
    "href": "studyguide/1-data-collection.html#probability-proportional-to-size-pps-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Probability-proportional-to-size (PPS) Sampling",
    "text": "Probability-proportional-to-size (PPS) Sampling\nProbability-proportional-to-size sampling, or PPS sampling, can be thought of as an extension of stratified sampling. Stratified sampling allows the flexibility for the probabilities of selection to vary with the size and characteristics of the strata. PPS sampling takes this a step further so that each element has its own probability of selection.\nThe procedure requires that an associated variable be known so that each element in the population has a measure of size associated with it. This variable may be a related variable whose values are known from a previous census. For example, auditors usually check only a sample of all the accounts in existence. The size of each account is crucial. So the large value accounts should be checked more intensely than the smaller accounts. This is achieved by giving each account a probability of selection which is proportional to its value. This is called Dollar-Unit Sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#estimates-for-common-sampling-methods",
    "href": "studyguide/1-data-collection.html#estimates-for-common-sampling-methods",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Estimates for Common Sampling Methods",
    "text": "Estimates for Common Sampling Methods\nAssociated with each sampling method is a particular form of estimator for the population parameters of concern. If an element has a high probability of selection, its effect must be reduced when an estimate is formed or, otherwise, the estimate will tend to be too large.\nIf the \\(i^{th}\\) observation is \\(y_{i}\\) and the probability that this element is chosen into the sample is \\(\\pi _{i}\\), it turns out that:\nEstimate of population total = \\(\\sum \\frac{y_{i} }{\\pi _{i} }\\) = sum(each observation in sample/its probability of selection)\nThis is easy to show with simple random sampling when \\(\\pi _ i =\\frac{n}{N}\\) for \\(i=1,2,...n\\). Let \\(Y\\) = population total, \\(\\bar{Y}\\) = population mean, and \\(Y=N\\bar{Y}\\). We estimate \\(\\bar{Y}\\) by \\(\\bar{y}\\) = mean of the sample and we estimate \\(Y\\) by \\(N\\bar{y}\\). If we write the estimate of the population total as \\(\\hat{Y}\\), we have: \\[\\hat{Y}=N\\bar{y}= N\\sum \\frac{y_{i} }{n} =\\sum \\frac{y_{i} }{\\left({\\tfrac{n}{N}} \\right)}\\] Notice that the summation is over all observations in the sample. For simple random samples, the probability that each observation is selected is \\(\\frac{n}{N}\\) so that the estimate of the total is, indeed, the sum of each observation divided by its probability of selection.\nFor stratified sampling with \\(k\\) strata, the estimated total and mean are, respectively \\[\\hat{Y}=N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} +....+N_{k} \\bar{y}_{k}\\] and \\[\\bar{Y}=\\left(N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} +....+N_{k} \\bar{y}_{k} \\right)/N\\] Assume that stratum \\(j\\) contains \\(N_{j}\\) elements and that the sample size for this stratum is \\(n_{j}\\), then the probability that an element in the \\(j^{th}\\) stratum is selected is \\(\\frac{n_{j} }{N_{j} }\\). As an example, suppose that a stratified sample of adult males, in full time employment in NZ, was taken to determine the number of working days lost in the previous year and also their party political preference. In this hypothetical example, the strata are urban versus rural. The results were shown in Figure 5).\n\n\n\nFigure 5: STRS estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate of total days lost \\(\\hat{Y}=N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} = 400,000\\times 5 + 100,000\\times 2 = 2,200,000.\\)\nThe estimate of the total number who prefer Labour is \\[\\hat{Y}=N_{1} p_{1} +N_{2} p_{2} = 400,000 \\times 0.5 + 100,000\\times 0.3 = 230,000.\\] Notice that proportions are means and fit into this structure quite naturally by defining\n\\[x_i = \\left\\{ {\\begin{array}{*{20}c} 1 & {{\\text{if the element has the property understudy}}} \\\\\n0 & {{\\text{otherwise}}} \\\\ \\end{array}} \\right.\\]\nNotice also that we could estimate \\(\\bar{y}\\) by \\(\\hat{Y}/N\\) so that:\nEstimate of total days lost per person = \\(\\hat{Y}/N\\) = 2,200,000/500,000 = 4.4.\nEstimate of proportions who prefer Labour = \\(\\hat{Y}/N\\) = 230,000/500,000 = 0.46.\nThese same formulae for estimating the population total and the population mean are appropriate for probability proportional to size (PPS) sampling. However, sampling without replacement makes the formulae rather complicated so we will not consider the details here."
  },
  {
    "objectID": "studyguide/1-data-collection.html#multistage-designs-and-epsem",
    "href": "studyguide/1-data-collection.html#multistage-designs-and-epsem",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Multistage Designs and EPSEM",
    "text": "Multistage Designs and EPSEM\nLarge, and even moderately sized, sample surveys are often multistage surveys. The probability that an element is selected becomes the product of the probabilities of it being selected at each stage. As a simple example, consider 3 schools having 100, 200 and 300 students, respectively, and suppose that 12 students are to be selected from one of these schools. This could be carried out in a two-stage survey with the first stage being the selection of one school by probability proportional to size (PPS) sampling. In the second stage, 12 students are selected from the chosen school. Let \\(\\pi _{a}\\) be the probability of choosing a particular school and \\(\\pi _{b}\\) the probability of choosing a particular student (given that his/her school has been chosen). The probability of selecting a particular student is \\(\\pi_{a} \\times \\pi_{b}\\); see Figure 6.\n\n\n\nFigure 6: PPS example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, each student has the same probability of being selected which has been brought about by the PPS sampling of the schools. It would be possible, in this small example, to use only a one stage procedure such as a simple random sample of 12 students from a list of the 600. Alternatively each school could be viewed as a separate stratum and a stratified sample of students could be chosen. Of course, if 4 students are chosen from each school, those in the smallest school would have a better chance (4 chances in 100) of being selected than those in the largest school (4 chances in 300). On the other hand, if 2 are chosen from the first school, 4 from the next and 6 from the largest then each student will have the same probability of selection.\nThese EPSEM, equal probability of selection methods, have the advantage that it seems “fair” to allow each person the same probability of selection but, also, it makes for simpler and neater formulae for estimates and their variances."
  },
  {
    "objectID": "studyguide/1-data-collection.html#other-sampling-methods",
    "href": "studyguide/1-data-collection.html#other-sampling-methods",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Other Sampling Methods",
    "text": "Other Sampling Methods\n\nVolunteers\nSamples are often selected because they are easy to obtain. Many psychology experiments referred to in the literature make use of long suffering students in introductory psychology courses. Quite often, volunteers are sought for experiments. Indeed, even when a probability sampling scheme is used, some subjects will withdraw so that the actual respondents are, in a sense, volunteers. Sometimes, people are accosted in the street or supermarket by market researchers or media reporters. The sample is by and large an opportunity sample for it is difficult to obtain a random sample under such conditions. Any generalisations from such a sample should be treated with some suspicion.\n\n\nSnowball samples\nA snowball sampling approach is used by medical authorities to track down possible sufferers of sexually transmitted diseases. All former contacts of a known sufferer are contacted, if possible, and former contacts of theirs also contacted. Both volunteer and snowball sampling are purposive methods in that no randomisation is employed.\n\n\nQuota samples\nAnother purposive sampling method is that of quota sampling. The idea is to make the sample representative of the population at least for certain variables. The quotas required in the sample may be decided by partitioning the sample into males and females, and also partitioning it by age groupings. The interviewer would be required to find the stipulated number of respondents in each cell of the sample. Although the sample is representative on the given variables, one is not sure how representative it is in terms of the responses obtained. Furthermore, being a purposive sample, it is difficult to make probability statements about estimators.\n\n\nEstimating wild life populations\nIt would be difficult to estimate the number of deer in a large untamed area such as some parts of the South Island. The target population of deer is very large but also is very hard to locate. One sampling approach would be to use recent deer droppings as a sampling frame, and for researchers to walk in a straight line noting the number of occurrences of droppings in (say) a 2 meter wide path. If the researchers are spaced at intervals of 20 meters then one could assume actual number of droppings of 10 times that obtained in the sample. Further assumptions would be needed to extrapolate from the number of droppings to the number of deer. A more direct approach would be afforded by a capture-recapture method in which a sample of animals is captured, tagged, released and then at a later date a further sample is captured. The proportion of tagged animals in this second sample indicates the size of the population. A number of assumptions must be made such as the uniform spread of the tagged animals throughout the areas of the second sampling. The initial capturing may affect the animals in unusual ways, by making them shy tending to avoid capture in future or making them tamer so that they do not fear capture as much on a subsequent occasion. The method of tagging may also be disadvantageous causing stress. Other assumptions need to be made about the distribution of the untagged and tagged animals. A number of different estimators have been suggested to obtain unbiased and efficient estimators of the number of animals in the population."
  },
  {
    "objectID": "studyguide/1-data-collection.html#sample-size-some-practical-issues",
    "href": "studyguide/1-data-collection.html#sample-size-some-practical-issues",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Sample Size & Some Practical issues",
    "text": "Sample Size & Some Practical issues\nFigure 7 shows a fictitious example of how samples of four people are drawn from a population of 20 people.\n\n\n\nFigure 7: Methods of Sampling: A toy example\n\n\nIn reality, sampling issues are not as simple as in the above example. Most large studies require quite a complex form of sampling involving layered sampling frames and combinations of design strategies. A typical design for an urban study is described below. The first layer consists of 20 regional areas; the second layer consists of all the municipalities within each regional area; the third layer consists of all the suburbs within each municipality; the fourth layer consists of all the city blocks within each suburb and the fifth layer consists of all the households within each city block. Judgement may be used to select only 5 of the 20 regional areas, one typical of the central regions, one each typical of the northern, southern, eastern and western regions. Next a systematic sample of municipalities is chosen (from a list sorted by postal code) for each of these regional areas, a sample of suburbs is chosen (stratified by socio-economic level) within each municipality, a simple random sample of city blocks is chosen within each suburb and a simple random sample of households is chosen within each city block. Such designs aim to be cost effective in terms of producing a representative sample at reasonable cost. However, complex designs require complex analyses, so try to keep things as simple as possible. The important criteria for the selection of an appropriate sample design are stratification, randomisation and cost. Note that non-response also has cost implications in that a survey with a high non-response rate is more costly than a survey with a good response rate. Of course the sample size also has an influence on cost and this provides opportunities for sample design/sample size trade-off. In the following discussion we assume that there is a 100% response rate. In other words sample size refers to the number of (good) sample elements. When only a 50% response rate is expected for a mail survey it means that the number of questionnaires distributed should be double the required sample size.\n\nSample Size\nUnfortunately, it is the available funds rather than research requirements that often determine what the sample size will be. We will therefore not ask the question How large a sample do we need? Instead we will ask What can a certain sample size \\((n)\\) do for us? The answer to this question depends on the sample design that is to be used. A quantity called a design effect \\((d)\\), related to the variance of the estimate, is used to measure the quality of a sample design. The larger the design effect (variance), the worse the design in terms of producing representative samples. The design effect is used to compute an effective sample size \\((e)\\) from the actual sample size \\({n}\\) using the formula \\({e=}\\frac{n}{d}\\). The effective sample size provides a more meaningful indication of the number of independent (useful) responses than the actual sample size. Figure 8 roughly shows the typical ranges for the design effect \\((d)\\) associated with the various sample designs.\n\n\n\nFigure 8: Crude comparison of effective sample sizes\n\n\nFor a simple random sample the design effect is treated as one. So, for a simple random the effective sample size is the same as the actual sample size. Each observation in the sample is doing a full job. You will see that a quota sample typically has a design effect of 2, producing an effective sample size, which is half the actual sample size. This means that each observation in a quota sample is only doing half a job. However, in the case of a stratified random sample, the design effect is typically less than one indicating a very good design and producing an effective sample size that is greater than the actual sample size \\((n)\\). In this case each observation is working overtime. Clearly a design effect of more than one means that the sample is not as efficient as a random sample would be, while a design effect of less than one means that the sample if more efficient than a random sample would be.\nMost opinion polls give a margin of error (this term will be formally defined later on) when they give the result of their survey. This is the level of inaccuracy associated with the sample result. For example, we hear that a poll indicates that a proportion \\(p\\) of 40% of voters support National but the margin of error could be 6.1% if the effective sample size is only 300. When the purpose of the research is to estimate some proportion (i.e. percentage) then the margin of error can be calculated using an approximate formula (namely \\(2\\sqrt{p(1-p)/e}\\) where \\(p\\) is the expected proportion and e is the effective sample size). As indicated in Figure 9, the margin of error declines as the effective sample size increases. By setting \\(e\\) equal to the actual sample size divided by the design effect we can estimate the margin of error for any design. If the margin of error is too big it means that the sample size needs to be increased. Unfortunately this is usually the case. This example is hypothetical but it illustrates the general relationship between the margin of error and the effective sample size.\n\n\nCode\nlibrary(tidyverse)\n\ndfm &lt;- tibble(\n  \"Sample size\" = 30:1000,\n  \"Margin of error\" = 100/sqrt(`Sample size`)\n)\n\ndfm |&gt; \n  ggplot() +\n  aes(x = `Sample size`,\n      y = `Margin of error`) +\n  geom_path() +\n  theme_minimal()\n\n\n\n\n\nFigure 9: Margin of Error and Sample Size\n\n\n\n\nIn Figure 9, the effective sample size required for a margin of error of 10% is 100. Since the actual sample size equals the effective sample size for a simple random sample this means that the size of the simple random sample must be 100 if the margin of error is 10% when a population proportion is estimated. However the effective sample size depends on the sample design. The actual sample size required for a given margin or error also depends on the sampling design. The actual sample size required in order for the margin of error to be 10 is highest for quota samples at 200 and lowest for a random stratified sample at 80. For the random cluster sample, an actual sample size of 120 will produce this margin of error.\nThe following rule of thumb can be used when sample results are to be generalised to a population. Firstly, if fairly sophisticated (multivariate analysis) is required then the effective sample size should be at least 200. Secondly, there should be at least four sample elements for every variable included in the study. However, this rule and the above formula are irrelevant when the population is relatively small or when we do not wish to generalise our findings to the population. In studies of this nature smaller sample sizes are sufficient. But the relative percentage to the size of the population should be large. For example if the population is of size only 100, an effective sample of size 80 is needed to achieve a certain degree of margin of error when the true population proportion is about 0.5. However, only an effective sample of size 370 is needed for the same margin of error if the population size is 10,000. The effective sample size needs to be increased only to 385 if the population size is 100,000. The moral is that percentage sampling is a fallacy, and that the absolute sample size is more important than sampling a fixed percentage of the population."
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova",
    "href": "slides/Chapter08.html#one-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\nfabric burn-time data\n\n\n\n\nfabric 1\nfabric 2\nfabric 3\nfabric 4\n\n\n\n\n17.8\n11.2\n11.8\n14.9\n\n\n16.2\n11.4\n11\n10.8\n\n\n17.5\n15.8\n10\n12.8\n\n\n17.4\n10\n9.2\n10.7\n\n\n15\n10.4\n9.2\n10.7\n\n\n\n\nCan we regard the mean burn times of the four fabrics as equal?\n\n\nfabric 1 seems to take longer time to burn"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model\n\n\\(H_0\\): The mean burn times are equal for the four fabrics\n\\(H_a\\): The mean burn time of at least one fabric is different."
  },
  {
    "objectID": "slides/Chapter08.html#anova-table",
    "href": "slides/Chapter08.html#anova-table",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\n\nobservation = mean + effect + error\n\n\nSS Total = SS Factor + SS Error\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfabric effect on burntime is highly significant.\n\nIn other words, the null hypothesis of equal mean burntime is rejected.\n\nOr alternatively the mean burntime is different for at least one fabric"
  },
  {
    "objectID": "slides/Chapter08.html#graphical-comparison-of-means",
    "href": "slides/Chapter08.html#graphical-comparison-of-means",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphical comparison of means",
    "text": "Graphical comparison of means\n\nThe graph below shows individual 95% confidence intervals for the fabric means\n\nPooled SD (\\(\\sqrt{MSE}\\)) is NOT used"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "href": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD (Interval Plot)",
    "text": "Tukey HSD (Interval Plot)\n\nThe ANOVA model assumes equal SD for the treatments.\nTukey HSD (Honest Significant Differences) plot allows pairwise comparison of treatment means"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "href": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA model Assumptions",
    "text": "One-way ANOVA model Assumptions\n\nResiduals are randomly and normally distributed\nResiduals must be independent of means.\n\nIf SD increases with mean, try square root or logarithmic transformation.\n\nIf experimental errors are more in some subgroups, divide the problem into separate ones.\nPositive correlation among residuals leads to under estimation of error variance; negative correlation leads to overestimation.\n\n\nThese assumptions are harder to validate to small experimental design data"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova",
    "href": "slides/Chapter08.html#two-way-two-factor-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA",
    "text": "Two way (two factor) ANOVA\n\nTwo factors are present.\nExample: Twelve tanks, six each with water from one of the two lakes were set up. Three nutrient supplements were added to each tank. The count of zooplankton in a unit volume of water was then noted.\n\n\n\n Zooplankton Supplement     Lake\n          34          1     Rose\n          43          1     Rose\n          57          1 Dennison\n          40          1 Dennison\n          85          2     Rose\n          68          2     Rose\n          67          2 Dennison\n          53          2 Dennison\n          41          3     Rose\n          24          3     Rose\n          42          3 Dennison\n          52          3 Dennison"
  },
  {
    "objectID": "slides/Chapter08.html#main-effect-plots",
    "href": "slides/Chapter08.html#main-effect-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Main effect plots",
    "text": "Main effect plots\n\nSimply plot of response means for factor levels"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit",
    "href": "slides/Chapter08.html#two-way-model-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nModel:\nObservation = Overall Mean + Row Effect + Column Effect + Error\n\n\n\\(H_0\\): Row means are equal; Column means are equal\n\n\n\n                   Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfactor(Supplement)  2 1918.5   959.2   6.486 0.0212 *\nLake                1   21.3    21.3   0.144 0.7140  \nResiduals           8 1183.2   147.9                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSupplement effect is significant at 5% level but not the Lake effect"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-effect",
    "href": "slides/Chapter08.html#interaction-effect",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction effect",
    "text": "Interaction effect\n\nWhether Factor A (row) effects are constant over Factor B (column) effects or Factor B effects are constant over Factor A effects?\n\nIf the answer is no, then there is an interaction between A & B.\n\nExample:\n\nTemperature and pressure are factors affecting the yield in chemical experiments.\nThey do interact in a mechanistic sense.\n\n\nInteraction may or may not have physical meaning."
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plots",
    "href": "slides/Chapter08.html#interaction-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plots",
    "text": "Interaction Plots\n\nIn the absence of interaction, the plotted means of factor crossings will be roughly parallel\n\nsee Plot 1. A & B do not interact.\n\nIf the the plotted means of factor crossings are far from parallel, then there is interaction\n\nPlot 2 shows extreme (antagonistic) interaction between A & B."
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "href": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plot for Zooplankton data",
    "text": "Interaction Plot for Zooplankton data\n\nattach(zooplankton)\nlibrary(ggplot2)\np1= ggplot(data = zooplankton, aes(x = factor(Supplement), y = Zooplankton, group=Lake, colour=Lake)) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(Zooplankton), slope=0)+ \n  theme_bw()+ggtitle(\"Lake*Supplement Interaction effect\")\np2= ggplot(data = zooplankton, aes(x = Lake, y = Zooplankton, group=factor(Supplement), colour=factor(Supplement))) +stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(Zooplankton), slope=0)+ \n  theme_bw()+ggtitle(\"Lake*Supplement Interaction effect\")\nlibrary(patchwork)\np1+p2\n\n\n\nInteraction effect may be present"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-1",
    "href": "slides/Chapter08.html#two-way-model-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nobservation = mean + row effect + column effect +interaction effect + error\n\n\nThe above model is known as multiplicative model\nIf interaction effect is ignored, we deal with an additive model\n\nExample: zooplankton data\n\n\n                Df Sum Sq Mean Sq F value Pr(&gt;F)  \nSupplement       2 1918.5   959.2   9.253 0.0147 *\nLake             1   21.3    21.3   0.206 0.6660  \nSupplement:Lake  2  561.2   280.6   2.707 0.1453  \nResiduals        6  622.0   103.7                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#residual-diagnostics",
    "href": "slides/Chapter08.html#residual-diagnostics",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics"
  },
  {
    "objectID": "slides/Chapter08.html#indicator-variables",
    "href": "slides/Chapter08.html#indicator-variables",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nIndicator variables are used if the predictor is qualitative rather than quantitative.\n\nConsider gender, a categorical variable.\n\nLet \\(I_1\\) be an indicator variable that takes a value 1 for males and 0 for females.\n\nLet \\(I_2\\) takes 1 for females and 0 for males.\nNote only one of \\(I_1\\) & \\(I_2\\) is sufficient.\n\n\nThe minimum number of indicator variables needed is related to degrees of freedom."
  },
  {
    "objectID": "slides/Chapter08.html#anova-through-regression",
    "href": "slides/Chapter08.html#anova-through-regression",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA through regression",
    "text": "ANOVA through regression\n\nConsider the burn-time data for the four fabrics.\n\nThe four fabric types are categorical.\nDefine-\n\n\\(I_1\\) = 1 for fabric 1 and 0 otherwise\n\\(I_2\\) = 1 for fabric 2 and 0 otherwise\n\\(I_3\\) = 1 for fabric 3 and 0 otherwise\n\\(I_4\\) = 1 for fabric 4 and 0 otherwise\n\n\nNote that any THREE indicator variables are sufficient for the four fabrics.\n\n3 df for 4 fabrics"
  },
  {
    "objectID": "slides/Chapter08.html#regression-summary",
    "href": "slides/Chapter08.html#regression-summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression summary",
    "text": "Regression summary\n\nRegress the burn-time response on the indicator variables \\(I_1\\), \\(I_2\\) & \\(I_3\\)\n\n\n\n\nCall:\nlm(formula = burntime ~ I1 + I2 + I3, data = fabric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.780 -1.205 -0.460  0.775  4.040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.9800     0.7604  15.754 3.65e-11 ***\nI1            4.8000     1.0754   4.463 0.000392 ***\nI2           -0.2200     1.0754  -0.205 0.840485    \nI3           -1.7400     1.0754  -1.618 0.125206    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.7 on 16 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.6706 \nF-statistic: 13.89 on 3 and 16 DF,  p-value: 0.0001016\n\n\n\nCompare with the one-way output\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "href": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Analysis of Covariance (ANCOVA)",
    "text": "Analysis of Covariance (ANCOVA)\n\nIndicator variables are used as additional regressors along with a quantitative predictor (covariate).\n\n\n\n Restaurant  Sales Households Location I1 I2\n          1 135.27        155  Highway  0  0\n          2  72.74         93  Highway  0  0\n          3 114.95        128  Highway  0  0\n          4 102.93        114  Highway  0  0\n          5 131.77        158  Highway  0  0\n          6 160.91        183  Highway  0  0\n          7 179.86        178     Mall  1  0\n          8 220.14        215     Mall  1  0\n          9 179.64        172     Mall  1  0\n         10 185.92        197     Mall  1  0\n         11 207.82        207     Mall  1  0\n         12 113.51         95     Mall  1  0\n         13 203.98        224   Street  0  1\n         14 174.48        199   Street  0  1\n         15 220.43        240   Street  0  1\n         16  93.19        100   Street  0  1\n\n\n\nPlot of data. Do we need three separate models?\n\n\n\nANCOVA fit\n\n\n\n                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 -6.203     11.818  -0.525    0.611\nHouseholds                   0.909      0.083  10.906    0.000\nLocationMall                39.223     16.451   2.384    0.038\nLocationStreet               8.036     16.273   0.494    0.632\nHouseholds:LocationMall     -0.074      0.104  -0.710    0.494\nHouseholds:LocationStreet   -0.012      0.101  -0.120    0.907"
  },
  {
    "objectID": "slides/Chapter08.html#explanation-of-fit",
    "href": "slides/Chapter08.html#explanation-of-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Explanation of fit",
    "text": "Explanation of fit\n\nIn order to allow for different slopes for each location, we define the product (or interaction) variables \\(I_1*X\\), and \\(I_2*X\\) (\\(X\\) being the covariate, Households)\n\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -6.203     11.818  -0.525    0.611\nX              0.909      0.083  10.906    0.000\nI1            39.223     16.451   2.384    0.038\nI2             8.036     16.273   0.494    0.632\nX:I1          -0.074      0.104  -0.710    0.494\nX:I2          -0.012      0.101  -0.120    0.907\n\n\n\nFor \\(I_1=0\\) & \\(I_2=0\\), the model becomes the fit for Highway\n\nSo this set-up compares Mall/Street with Highway\n\nSignificant \\(I_1\\) coefficient means that Mall location has a constant level of higher sales\nMall location model has a higher intercept but the slopes are the same for all three locations."
  },
  {
    "objectID": "slides/Chapter08.html#graphing-the-model",
    "href": "slides/Chapter08.html#graphing-the-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphing the model",
    "text": "Graphing the model"
  },
  {
    "objectID": "slides/Chapter08.html#summary",
    "href": "slides/Chapter08.html#summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary",
    "text": "Summary\nANOVA models study categorical predictors (factors). - Interaction between factors is important. - ANOVA models and regression models are related and fall under a general family of linear models.\nANCOVA models employs both numerical variables (covariates) and qualitative factors for modelling.\n\nInteraction between factors and covariates is important."
  },
  {
    "objectID": "slides/Chapter08.html#shiny-apps",
    "href": "slides/Chapter08.html#shiny-apps",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.simple.experimental.ANOVA.models/\nhttps://shiny.massey.ac.nz/anhsmith/demos/fit.multiple.regression/\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter06.html#what-is-a-statistical-model",
    "href": "slides/Chapter06.html#what-is-a-statistical-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\n\nOBSERVATION = FIT + RESIDUAL\n\nFIT- to explain systematic (non-random) variation in the data\nRESIDUAL - to explain random variation in the data\n\nWith paired (related) data (X,Y)\n\nFitted Model: \\(\\hat{Y}=a+bX\\) (True Model: \\(Y=\\alpha+\\beta X+\\epsilon\\))\nresidual error: \\(e= Y-\\hat{Y}\\) (\\(\\epsilon\\) is not the same as \\(e\\))\n\nWhile fitting models to data\n\nit is required fit the model to the data as closely as possible (e.g. using least squares technique)\nwe need to make residuals free of patterns or trends.\nwe assume a probability distribution of residuals for statistical inference.\n\nA regression equation is a function that indicates how the average value of one response variable for given values of one or more predictor variables varies with these predictor variables; that is, \\(E(Y|X_1, X_2, ..., X_k)\\)"
  },
  {
    "objectID": "slides/Chapter06.html#simple-regression",
    "href": "slides/Chapter06.html#simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Simple regression",
    "text": "Simple regression\n\nThe term simple means there is a single predictor\nThe fitted model remains as: \\(\\hat{Y}=a+bX\\)\n\nThe method of least squares is employed to obtain the estimates \\(a\\) and \\(b\\)\n\nThe sum of squared residuals is minimized in the least squares method.\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.least.squares/\n\nR regression outputs are bulky\n\n\nLEARN HOW TO INTERPRET THE REGRESSION OUTPUTS"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n\n country Alcohol Death\n       1    22.9  30.0\n       2    15.2  23.6\n       3    12.3  18.9\n       4    11.9   5.0\n       5    10.8  12.3\n       6     9.9  14.2\n       7     8.3   7.4\n       8     7.2   3.0\n       9     6.6   7.2\n      10     5.8  10.6\n      11     5.7   3.7\n      12     5.6   3.4\n      13     4.2   4.3\n      14     3.9   3.6\n      15     3.1   5.4"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data"
  },
  {
    "objectID": "slides/Chapter06.html#r-base-output",
    "href": "slides/Chapter06.html#r-base-output",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "R Base Output",
    "text": "R Base Output\n\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nThis is a bulky output. So let us tidy it."
  },
  {
    "objectID": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "href": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitted model and testing its coefficients",
    "text": "Fitted model and testing its coefficients\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n\nFocus on the model, its coefficients\n\nAre these coefficient estimates meaningful in the context?\n\nTests of significance\n\n\\(t\\) tests of intercept and slope\n\\(H_0=true~slope=0\\); \\(H_0=true~intercept=0\\)\n\nFor Alcohol~deaths model, the slope is highly significant but not the intercept"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures",
    "href": "slides/Chapter06.html#model-quality-measures",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\n\nout1 &lt;- glance(mod1) |&gt; \n  select(r.squared, sigma, statistic, p.value) |&gt; \n  mutate_if(is.numeric, round, 2)\n\nout1 |&gt; t() |&gt; \n  kable(caption = \"Model summary measures\") |&gt; \n  kable_classic(full_width = T) \n\n\n\nModel summary measures\n\n\nr.squared\n0.79\n\n\nsigma\n3.94\n\n\nstatistic\n48.35\n\n\np.value\n0.00\n\n\n\n\n\n\n\n\n\nModel summary (or Quality) measures\n\n\\(R^2\\) is the proportion of variation explained by the fitted model\n\nA meaningful model must have at least 50% \\(R^2\\)\nA large \\(R^2\\) is important to explain the relationship(s)\n\nResidual standard deviation (error) \\(S\\) has to be small\n\nHow small? Difficult to say. Compare \\(S\\) with the overall spread in \\(Y\\) or with the mean of \\(Y\\)\nA small \\(S\\) is important for prediction"
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test",
    "href": "slides/Chapter06.html#anova-f-test",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n\nFor the straight line model, the F-test is equivalent to testing the hypothesis that the true slope is zero.\n\nHighly significant F ratio need not always imply that the straight line is the best fit to the data.\nFor Alcohol~deaths model, the \\(F\\) statistic is highly significant which means that the fitted model explains significant variation\n\n\n\n\nout1 &lt;- anova(mod1) |&gt; tidy() |&gt; mutate_if(is.numeric, round, 2)\n\noptions(knitr.kable.NA = \" \")\n\nkable(out1, caption = \"ANOVA table\") |&gt; \n  kable_classic(full_width = F) \n\n\n\nANOVA table\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nAlcohol\n1\n751.44\n751.44\n48.35\n0\n\n\nResiduals\n13\n202.03\n15.54"
  },
  {
    "objectID": "slides/Chapter06.html#anova-table-construction",
    "href": "slides/Chapter06.html#anova-table-construction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA table construction",
    "text": "ANOVA table construction\n\nEach source has an associated degrees of freedom.\nFor regression, DF = \\(1\\) as there are two parameters \\(a\\) and \\(b\\) fitted in the model\nFor total, DF = \\(n-1\\) since there are n observations\nFor error, DF = by subtraction = \\((n-1)-1 = n-2\\)\nMean Square (MS) values are obtained as MS = SS/DF\nF ratio for regression =MS(Regression)/MS(Error)\nF ratio follows the \\(F\\) distribution with \\((1, n-2)\\) d.f and provides the significance of the model fitted.\n\nthe ratio of two sample variances (or MS) follows the \\(F\\) distribution (normal case)"
  },
  {
    "objectID": "slides/Chapter06.html#prediction",
    "href": "slides/Chapter06.html#prediction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Prediction",
    "text": "Prediction\n\nThe predicted response at value \\(x=x_0\\) is obtained using the fitted regression equation.\nConfidence & prediction intervals can also be constructed.\nNote that prediction intervals are for individual observations whereas the confidence intervals are for the expected (mean) response for a given \\(x_0\\)\n\n\n\n       fit      lwr      upr\n1 11.72778 9.476416 13.97915\n\n\n       fit      lwr      upr\n1 11.72778 2.918701 20.53686"
  },
  {
    "objectID": "slides/Chapter06.html#types-of-residuals",
    "href": "slides/Chapter06.html#types-of-residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Types of residuals",
    "text": "Types of residuals\n\nraw or ordinary residual is just (observation-fit)\nStandardized Residual= Residual/Std.Dev of residual\nThe regression model is influenced by outliers or unusual points because the slope estimate of the regression line is sensitive to these outliers.\nSo we define Studentised or deleted t Residual\nSimilar to Standardized Residual without the observation under consideration. That is,\n\nStudentised residual = residual/std. dev of residual (after omitting the particular observation)."
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\nFor small sample sizes, residual diagnostics is difficult"
  },
  {
    "objectID": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "href": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals showing need for transformation",
    "text": "Residuals showing need for transformation\n\nNon-constant Residual Variation"
  },
  {
    "objectID": "slides/Chapter06.html#adding-more-predictors",
    "href": "slides/Chapter06.html#adding-more-predictors",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Adding more predictors",
    "text": "Adding more predictors"
  },
  {
    "objectID": "slides/Chapter06.html#subgrouping-patterns",
    "href": "slides/Chapter06.html#subgrouping-patterns",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Subgrouping patterns",
    "text": "Subgrouping patterns"
  },
  {
    "objectID": "slides/Chapter06.html#outliers",
    "href": "slides/Chapter06.html#outliers",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/Chapter06.html#autocorrelation",
    "href": "slides/Chapter06.html#autocorrelation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nNeighbouring residuals depend on each other"
  },
  {
    "objectID": "slides/Chapter06.html#assumptions",
    "href": "slides/Chapter06.html#assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\n\nModel forming assumptions\n\n\n\\(X\\) is known without error\n\\(Y\\) is related to \\(X\\) according to a straight line.\nThere is a random variability of \\(Y\\) about this line.\n\n\nMore assumptions to form \\(t\\) and \\(F\\) statistics:\n\n\nVariability in \\(Y\\) about the line is constant and independent of \\(X\\) variable.\nThe variability of \\(Y\\) about the line follows normal distribution.\nThe distribution of \\(Y\\) given \\(X = X_i\\) is independent of \\(Y\\) given \\(X = X_j\\)."
  },
  {
    "objectID": "slides/Chapter06.html#improving-simple-regression",
    "href": "slides/Chapter06.html#improving-simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Improving simple regression",
    "text": "Improving simple regression\n\nUse a different predictor or explanatory variable\nTransform the \\(Y\\) variable\nAdd other explanatory variables to the model\nDeletion of invalid (as opposed to outlier) observations\nReconsider the linear relationship"
  },
  {
    "objectID": "slides/Chapter06.html#outlier-effect-on-regression",
    "href": "slides/Chapter06.html#outlier-effect-on-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outlier effect on regression",
    "text": "Outlier effect on regression\n\nScatter plot of People vs Vehicle"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance\n\nA distant \\(x\\) value has a higher leverage.\n\nThis leverage is often measured by the \\(h_{ii}\\) or hi value\nCheck \\(h_{ii}\\) &gt; \\(\\frac{3p}{n}\\) or not\n\nInfluence of a point on the regression is measured using the Cook’s distance \\(D_i\\)\n\nrelated to difference between the regression coefficients with and without the \\(i^{th}\\) data point.\n\\(D_{i} &gt;0.7\\) can be deemed as being influential (for \\(n&gt;15\\))"
  },
  {
    "objectID": "slides/Chapter06.html#tukey-line",
    "href": "slides/Chapter06.html#tukey-line",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Tukey Line",
    "text": "Tukey Line\nAlso called Median-median line, Resistant line or Rline\nAdvantages:\n- Easy to fit\n- Outlier or peculiar values do not affect Rline to  the extent they influence a regression line\nRline fitting\n\nSort the values of X first and copy the corresponding Y values\nDivide the X values equally into three groups with corresponding Y values\nCompute the medians of the X values from the lowest and the uppermost subgroups, and the corresponding medians of Y values\nThe resistant line is then fitted with these two median points"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\nObtain the estimated slope using low and upper group median pairs as \\(b = (18.9-3.7)/(12.3-4.2) = 1.88\\).\nObtain the y-intercept using either the low or upper group median points \\(a = Y-bX = 3.7-1.88*4.2 = -4.2\\).\n\n\n\n\n\n\n\n\nR function\n\nline(cirrhosis$Death,cirrhosis$Alcohol) \n\n\nCall:\nline(cirrhosis$Death, cirrhosis$Alcohol)\n\nCoefficients:\n[1]  4.0797  0.4379\n\n\n\n\n\n\n\n\n\n\n\nThe line() function gives a slightly different slope & intercept."
  },
  {
    "objectID": "slides/Chapter06.html#two-more-robust-models",
    "href": "slides/Chapter06.html#two-more-robust-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Two more Robust models",
    "text": "Two more Robust models\nWe can also fit a robust linear model using the functions MASS::rlm() robustbase::lmrob()."
  },
  {
    "objectID": "slides/Chapter06.html#orthogonal-regresion",
    "href": "slides/Chapter06.html#orthogonal-regresion",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Orthogonal regresion",
    "text": "Orthogonal regresion\n\nAlso called Deming regression.\nThe sum of squared perpendicular distance between the line and actual point is minimised."
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv",
    "href": "slides/Chapter06.html#cross-validation-cv",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nSplit the sample data randomly into k (equal) folds (parts) by resampling.\nFit the model for \\((k − 1\\)) folds of the data\nPredict for the omitted fold\nCompare prediction errors\nRoot mean square error (RMSE) is often used\nR has many packages for cross validation"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data-2",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data-2",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\nlibrary(caret);  library(MASS,  exclude  =  \"select\")\nset.seed(123)\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 100)\nlmfit &lt;- train(Death ~Alcohol,  data= cirrhosis, \n                 trControl = fitControl, method=\"lm\")\nlm.rmses &lt;- lmfit$resample[,1]\nrlmfit &lt;- train(Death ~Alcohol,  data = cirrhosis, \n                  trControl=fitControl, method = \"rlm\")\nrlm.rmses &lt;- rlmfit$resample[,1]\ndfm  &lt;-  cbind.data.frame(lm.rmses,rlm.rmses)\nlibrary(patchwork)\nqplot(data=dfm,  lm.rmses,  geom=\"boxplot\")  /\nqplot(data=dfm,  rlm.rmses,  geom=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter06.html#choosing-the-best-model",
    "href": "slides/Chapter06.html#choosing-the-best-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Choosing the best model",
    "text": "Choosing the best model\n\nThe best model is not decided purely on statistical grounds.\nIf the main aim is to describe relationships, include all the relevant variables.\nIf the main aim is to predict, prefer the simplest feasible (parsimonious) model with smaller number of predictors.\n\nExamine the literature to discover similar examples, see how they are tackled, discuss the matter with the researcher etc.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference",
    "href": "slides/Chapter04.html#statistical-inference",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nThe term statistical inference means that we are using properties of a sample to make statements about the population from which the sample was drawn.\nFor example, say you wanted to know the mean length of the leaves on a tree (\\(\\mu\\)). You wouldn’t want to (nor need to) measure every single leaf! You would take a random sample of leaves and measure their lengths (\\(x_i\\)), calculate the sample mean (\\(\\bar x\\)), and use \\(\\bar x\\) as an estimate of \\(\\mu\\)."
  },
  {
    "objectID": "slides/Chapter04.html#normality-tests-and-eda",
    "href": "slides/Chapter04.html#normality-tests-and-eda",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Normality tests and EDA",
    "text": "Normality tests and EDA\n\nEDA approaches\n\nnormal quantile plot, Boxplots,mids vs. spreads plot etc.\n\nHypothesis tests\n\nKolmogorov-Smirnov test (based on the biggest difference between the empirical and theoretical cumulative distributions)\nShapiro-Wilk test (based on variance of the difference)\n\nExample: N(100,1) random data of size \\(n=50\\)\n\nThe null hypothesis of normality must be justified on empirical grounds\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rnorm(50, mean = 100)\nW = 0.94504, p-value = 0.02142\n\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  rnorm(50)\nD = 0.088169, p-value = 0.7995\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "slides/Chapter04.html#example",
    "href": "slides/Chapter04.html#example",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/Chapter04.html#sampling-distributions",
    "href": "slides/Chapter04.html#sampling-distributions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nThe term sampling distribution means the distribution of the computed statistic such as the sample mean when sampling is repeated many times.\nFor a normal population,\n\nStudent’s \\(t\\) distribution is the sampling distribution of the mean (after rescaling).\n\\(\\chi^2\\) distribution is the sampling distribution of the sample variance \\(S^2\\).\n\\((n-1)S^2/\\sigma^2\\) follows \\(\\chi^2\\) distribution.\n\n\\(F\\) distribution is ratio of two \\(\\chi^2\\) distributions.\n\nIt becomes the sampling distribution of the ratio of two sample variances \\(S_1^2/S_2^2\\) from two normal populations (after scaling).\n\n\\(t\\) distribution is symmetric but \\(\\chi^2\\) and \\(F\\) distributions are right skewed. - For large samples, they become normal - For \\(n&gt;30\\), the skew will diminish\nFor the three sampling distributions, the sample size \\(n\\) becomes the proxy parameter, called the degrees of freedom (df).\n\n\\(t_{n-1}\\), \\(\\chi_{n-1}^2\\) & \\(F_{(n_1-1),(n_2-1) }\\)"
  },
  {
    "objectID": "slides/Chapter04.html#students-t-distribution",
    "href": "slides/Chapter04.html#students-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Student’s \\(t\\) distribution",
    "text": "Student’s \\(t\\) distribution\n\nSample means from normal population are normally distributed ie. \\(\\bar{X} \\sim N(\\mu, \\frac {\\sigma}{\\sqrt{n}})\\)\nEven when the population is not normal, \\(\\bar{X} \\sim N(\\mu, \\frac {\\sigma}{\\sqrt{n}})\\) holds for large \\(n\\) (Central Limit Theorem)\nWhen \\(\\sigma\\) is unknown (which true for small sample sizes), the distribution of \\(\\left(\\frac{\\bar{x}-\\mu}{S/\\sqrt{n}} \\right)\\) is no longer normal unlike the distribution of \\(\\left(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} \\right) \\sim N(0, 1)\\)\nThe distribution of the statistic \\(t=\\left(\\frac{\\bar{x}-\\mu}{S/\\sqrt{n}} \\right)\\) is called the Student’s t-distribution with \\((n-1)\\) degrees of freedom.\nStudent’s t-distribution is symmetric but more spread than the standard normal."
  },
  {
    "objectID": "slides/Chapter04.html#density-curves",
    "href": "slides/Chapter04.html#density-curves",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Density curves",
    "text": "Density curves\n\\(t_2\\) density\n\n\n\n\n\n\n\n\n\n\\(\\chi_{5}^2\\) density\n\n\n\n\n\n\n\n\n\n\\(F_{2, 10}\\) density\n\n\n\n\n\n\n\n\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.critical.values/"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-interval-for-the-population-mean",
    "href": "slides/Chapter04.html#confidence-interval-for-the-population-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence interval for the population mean",
    "text": "Confidence interval for the population mean\n\nA confidence interval (CI) is nothing but an interval estimate of an unknown population quantity\n\nCI is of the form estimate \\(\\pm\\) margin of error\n\nStudent’s t quantiles are used to construct the confidence interval for the population mean as \\(\\bar{y}\\) \\(\\pm\\) \\(t \\times (S/\\sqrt n)\\)\n\n\\(S/\\sqrt n\\) is the estimated standard error of the mean"
  },
  {
    "objectID": "slides/Chapter04.html#one-sample-t-test",
    "href": "slides/Chapter04.html#one-sample-t-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "One-sample t-test",
    "text": "One-sample t-test\n\nNull hypothesis \\(H_0:\\mu=\\mu_0\\). Two-sided Alternative hypothesis \\(H_1:\\mu \\neq \\mu_0\\)\nCompute the Student’s t-statistic \\(t=\\left(\\frac{\\bar{x}-\\mu_0}{S/\\sqrt{n}} \\right)\\)\nObtain the P value from n-1 degrees of freedom\n\nP (Probability) value is the tail area above \\(t\\) and below \\(-t\\)\n\nIf the P value is below the set significance level \\(\\alpha\\), reject the null hypothesis\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/\nLearn how to perform t-tests in R\n\nfunction t.test"
  },
  {
    "objectID": "slides/Chapter04.html#concept-of-power",
    "href": "slides/Chapter04.html#concept-of-power",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Concept of power",
    "text": "Concept of power\nA hypothesis test has a certain power (probability) to reject the null hypothesis when it is false.\nThe power of the t test can be evaluated using R for given effect size \\(\\delta\\)\n\npower.t.test(n = 30, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 30\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.9677083\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\npower.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.9986074\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test",
    "href": "slides/Chapter04.html#two-sample-t-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nIn a two-sample t-test, the equality of two population means is tested. Null hypothesis \\(H_0:\\mu=\\mu_1=\\mu_2\\). Two-sided Alternative hypothesis \\(H_1:\\mu_1 \\neq \\mu_2\\)\nIf EDA suggests that the two populations have the same spread, we perform a pooled-sample t-test in which the variance common to the two population is estimated as \\(S_{p}^{2} = w_{1} S_{1}^{2} +w_{2} S_{2}^{2}\\) where the weights are \\(w_{1} =\\frac{n_{1}-1}{n_{1} +n_{2}-2}\\) and \\(w_{2} =\\frac{n_{2}-1}{n_{1} +n_{2}-2}\\)\nFor the pooled case, the \\(df\\) for the \\(t\\)-test is \\(n_{1}+n_{2}-2\\) but becomes smaller for the unpooled case to \\[df=\\frac{\\left(\\frac{S_{1}^{2}}{n_{1}} +\\frac{S_{2}^{2} }{n_{2}} \\right)^{2} }{\\frac{1}{n_{1} -1} \\left(\\frac{S_{1}^{2}}{n_{1}}\\right)^{2} +\\frac{1}{n_{2} -1} \\left(\\frac{S_{2}^{2}}{n_{2} } \\right)^{2}} \\]"
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nEqual variance assumption is found plausible in the following output:\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\nbartlett.test(TELETIME~SEX, data=tv)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  TELETIME by SEX\nBartlett's K-squared = 1.8482, df = 1, p-value = 0.174\n\ncar::leveneTest(TELETIME~factor(SEX), data=tv)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  3.1789 0.08149 .\n      44                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHighish p-values means there’s no strong evidence against the null hypothesis that the variances are equal."
  },
  {
    "objectID": "slides/Chapter04.html#paired-t-test",
    "href": "slides/Chapter04.html#paired-t-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Paired t-test",
    "text": "Paired t-test\n\nPaired t-test is nothing but a one-sample t-test on the individual differences. That is, we test whether the true mean of the paired differences is zero.\nPaired t-test requires a common pairing variable to relate the two population elements.\nThe two sample data must be correlated somewhat highly.\nIf the correlation is low or negative, the paired t-test is less powerful (so use the usual two-sample t-test).\nEg. studying twins; left and right legs (eyes) etc.\nTesting equality of mean IQ scores of twins\n\n\n\n\n    Paired t-test\n\ndata:  A and B\nt = 0.010661, df = 121, p-value = 0.9915\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.514008  1.530401\nsample estimates:\nmean difference \n    0.008196721"
  },
  {
    "objectID": "slides/Chapter04.html#shiny-apps-not-currently-working",
    "href": "slides/Chapter04.html#shiny-apps-not-currently-working",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Shiny apps (not currently working)",
    "text": "Shiny apps (not currently working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/\n\nCovers non-parametric tests, see Study Guide."
  },
  {
    "objectID": "slides/Chapter04.html#test-of-proportions",
    "href": "slides/Chapter04.html#test-of-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Test of proportions",
    "text": "Test of proportions\nTesting for a single proportion being different to 0.5, or some other value.\nA sample survey of size 1000 has had 450 females.\nCan we treat the survey as unbiased?\n\nprop.test(450, 1000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  450 out of 1000, null probability 0.5\nX-squared = 9.801, df = 1, p-value = 0.001744\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4189204 0.4814685\nsample estimates:\n   p \n0.45 \n\n\nAn exact version of the test\n\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value = 0.001731\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45 \n\n\nOr, for a test for a proportion other than 0.5\n\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45"
  },
  {
    "objectID": "slides/Chapter04.html#comparing-several-proportions",
    "href": "slides/Chapter04.html#comparing-several-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Comparing several proportions",
    "text": "Comparing several proportions\nFleiss (1981) Statistical methods for rates and proportions data on smokers in four group of patients\n\nsmokers  &lt;- c( 83, 90, 129, 70 )\npatients &lt;- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n\n\n    4-sample test for equality of proportions without continuity correction\n\ndata:  smokers out of patients\nX-squared = 12.6, df = 3, p-value = 0.005585\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4 \n0.9651163 0.9677419 0.9485294 0.8536585"
  },
  {
    "objectID": "slides/Chapter04.html#transformations",
    "href": "slides/Chapter04.html#transformations",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Transformations",
    "text": "Transformations\n\nI can be useful to transform data to make the distribution more ‘normal’\nThe linear transformation \\(Y^*= a+bY\\) result only in a change of scale or of origin\nA linear transformation does not change the shape of the distribution i.e. histogram, boxplot etc. remain the same shape\nWe prefer non-linear transformations which alter the shape of the distribution\nTransformations are needed to-\n\nto bring symmetry\nnormality\nstabilise the variance etc"
  },
  {
    "objectID": "slides/Chapter04.html#example-1",
    "href": "slides/Chapter04.html#example-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\n\nRight skewed distribution is made roughly symmetric using a log transformation for no. of vehicles variable (rangitikei.* dataset)"
  },
  {
    "objectID": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "href": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "A Ladder of Powers for Transforming Data",
    "text": "A Ladder of Powers for Transforming Data\n\nRight skewed data needs a shrinking transformation\nLeft skewed data needs a stretching transformation\nThe strength or power of the transformation depends on the degree of skew.\n\n\n\n\n\n\n\n\n\n\n\nPOWER\nFormula\nName\nResult\n\n\n\n\n3\n\\(x^3\\)\ncube\nstretches large values\n\n\n2\n\\(x^2\\)\nsquare\nstretches large values\n\n\n1\n\\(x\\)\nraw\nNo change\n\n\n1/2\n\\(\\sqrt{x}\\)\nsquare root\nsquashes large values\n\n\n0\n\\(\\log{x}\\)\nlogarithm\nsquashes large values\n\n\n-1/2\n\\(\\frac{-1}{\\sqrt{x}}\\)\nreciprocal root\nsquashes large values\n\n\n-1\n\\(\\frac{-1}{x}\\)\nreciprocal\nsquashes large values"
  },
  {
    "objectID": "slides/Chapter04.html#d-statistics",
    "href": "slides/Chapter04.html#d-statistics",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "D-statistics",
    "text": "D-statistics\n\nD-statistics are used for checking the adequacy of a transformation\nSymmetry implies mean = median\nA simple approach is to plot the distance from the median for points below and above the median.\n\n\n\n(mean-median) measures skew but we can standardise it in 3 ways\n\nD1 = |mean-median| / std.dev.\nD2 = |mean-median| / F-spread\nD3 = |mid_F-median| / F-spread\n\nD3 - sensitive to skewness in the middle 50% of data.\n\nD1 & D2 - sensitive to skewness in whole data\nSmaller the D value, the better the transformation for symmetry\n\n\n\n         D1          D2          D3 \n-0.04152225 -0.05830871  0.24199369"
  },
  {
    "objectID": "slides/Chapter04.html#box-cox-transformation",
    "href": "slides/Chapter04.html#box-cox-transformation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\n\nThis is a normalising transformation (i.e more than just symmetry) but we may not always find a suitable Box-Cox power \\(\\lambda\\).\nR gives a point estimate of power & the CI.\n\n\n\nFocus on the confidence interval for the estimated \\(\\lambda\\) and it may be wider"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "href": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference Based on Transformed data",
    "text": "Statistical Inference Based on Transformed data\n\nConfidence intervals found as estimate \\(\\pm\\) margin of error require symmetry.\nObtain the confidence interval using transformed data and then back transform the limits (to keep the original scale)\n\nThe confidence limits of log transformed data can be exponentiated back (ie. \\(e^{\\text{confidence limit}}\\))\nThe confidence limits of square-root transformed data can be squared back (ie. \\({\\text{confidence limit}^2}\\))\n\nFor hypothesis test, apply the same transformation on the value hypothesised under the null\nExplore https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/ app (not currently working)\nExample: Brain weight data is extremely skewed to the right. So rely on the back-transformed CI.\n\n\n\n[1]  46.88888 519.38209\n\n\n[1] 2.518901 3.761498\n\n\n[1] 12.41495 43.01280"
  },
  {
    "objectID": "slides/Chapter04.html#a-caution-on-the-transformation-technique",
    "href": "slides/Chapter04.html#a-caution-on-the-transformation-technique",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "A Caution on the transformation technique",
    "text": "A Caution on the transformation technique\n\nPerform EDA in addition to computing coefficients of skewness or D-Statistics etc\nSmall datasets may reveal skewness (So ignore)\nAvoid a transformation if marginal improvement happens.\nTransformed data may be difficult to interpret\nTry other options such as subgrouping etc.\nNormalising transformations are different from transformations for symmetry\nRead the section from your SG"
  },
  {
    "objectID": "slides/Chapter04.html#variance-stabilisation",
    "href": "slides/Chapter04.html#variance-stabilisation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Variance stabilisation",
    "text": "Variance stabilisation\n\nTwo or more batches of data may not have equal variance\nIf each batch is skewed in the same direction, then a common transformation can be used.\nIf batches are skewed in different directions, the transformation technique fails\nExplore LOCATION vs SPREAD plot of batches.\nIf there exists a relationship between spreads and averages, a single transformation can be applied for all the batches."
  },
  {
    "objectID": "slides/Chapter04.html#example-2",
    "href": "slides/Chapter04.html#example-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\n\nCommon shrinking transformation can be found in the following case"
  },
  {
    "objectID": "slides/Chapter04.html#shiny-apps",
    "href": "slides/Chapter04.html#shiny-apps",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Shiny apps",
    "text": "Shiny apps\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/\nhttps://shiny.massey.ac.nz/anhsmith/demos/perform.transformations/"
  },
  {
    "objectID": "slides/Chapter04.html#non-parametric-tests",
    "href": "slides/Chapter04.html#non-parametric-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nAn alternative to using power transformations\nRelies on replacing the actual observed data by their ranks\nSpearman’s Rank Correlation\n\nRank the \\(X\\) and \\(Y\\) variables, and then obtain usual Pearson correlation coefficient\nThe following plot shows both (Spearman correlation in the upper triangle)\n\n\nComparison of Pearsonian and Spearman’s rank correlations"
  },
  {
    "objectID": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "href": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nA non-parametric alternative to the one-sample t-test\n\n\\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median\nBased on based on ranking \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\)\n\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\n\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283"
  },
  {
    "objectID": "slides/Chapter04.html#mann-whitney-test",
    "href": "slides/Chapter04.html#mann-whitney-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Mann-Whitney test",
    "text": "Mann-Whitney test\nFor two group comparison, pool the two group responses and then rank the pooled data\nRanks for the first group are compared to the ranks for the second group\n\nThe null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\).\n\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308"
  },
  {
    "objectID": "slides/Chapter04.html#another-form-of-test",
    "href": "slides/Chapter04.html#another-form-of-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Another form of test",
    "text": "Another form of test\n\nkruskal.test(rangitikei$people~rangitikei$time)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides/Chapter04.html#permutation-tests",
    "href": "slides/Chapter04.html#permutation-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Permutation tests",
    "text": "Permutation tests\nA permutation (or randomisation) test is based on the idea of randomly permuting the observed data and then answering whether a hypothesis is negated or not.\nOne sample hypothesis test example follows:\n\nlibrary(exactRankTests)\nperm.test(tv$TELETIME, null.value=1500)\n\n\n    1-sample Permutation Test\n\ndata:  tv$TELETIME\nT = 79547, p-value = 2.842e-14\nalternative hypothesis: true mu is not equal to 0\n\n\nFor small samples, this test is not powerful."
  },
  {
    "objectID": "slides/Chapter04.html#two-group-comparison",
    "href": "slides/Chapter04.html#two-group-comparison",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nperm.test(TELETIME~SEX, distribution ='exact', data=tv)\n\n\n    2-sample Permutation Test\n\ndata:  TELETIME by SEX\nT = 38370, p-value = 0.471\nalternative hypothesis: true mu is not equal to 0\n\n\nAlso using a linear model fit (cover later)\n\nlibrary(lmPerm)\nsummary(lmp(TELETIME~SEX, data=tv))\n\n[1] \"Settings:  unique SS : numeric variables centered\"\n\n\n\nCall:\nlmp(formula = TELETIME ~ SEX, data = tv)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1178.261  -510.793    -7.283   402.989  1130.739 \n\nCoefficients:\n    Estimate Iter Pr(Prob)\nSEX      122   56    0.643\n\nResidual standard error: 570.9 on 44 degrees of freedom\nMultiple R-Squared: 0.0118, Adjusted R-squared: -0.01066 \nF-statistic: 0.5255 on 1 and 44 DF,  p-value: 0.4723 \n\n\nRead the study guide example for bootstrap tests (not examined)"
  },
  {
    "objectID": "slides/Chapter04.html#summary",
    "href": "slides/Chapter04.html#summary",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Summary",
    "text": "Summary\nInference is less complicated for normal populations\n\nAssessment of normality is particularly important for small sample sizes\nStudent t-tests are generally robust and can be used for non-normal populations (as long as there are no subgrouping or sample sizes are large)\n\nPower transformations aim to achieve symmetry\n\nBox-Cox transformations aim to normalise the data\nImproved Inference can be made\nSubgrouping effect can be dampened\n\nPermutation tests can be done for a second opinion\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter02.html#two-modes-of-data-analysis",
    "href": "slides/Chapter02.html#two-modes-of-data-analysis",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Two modes of data analysis",
    "text": "Two modes of data analysis\n\n\nHypothesis-generating\n\n“Exploratory analysis”\nAim is to explore data to discover new patterns\nResults must not be presented as formal tests of a priori hypotheses\nTesting a hypothesis using the same data that gave rise to the hypothesis is circular reasoning\n\n\nHypothesis-testing\n\n“Confirmatory analysis”\nAim is to evaluate evidence for specific a priori hypotheses\nThe hypotheses and ideas were conceived of before the data were observed\nCan be used for formal scientific inference\n\n\n\nPresenting hypothesis-generating analyses as hypothesis-testing analyses (i.e., pretending the hypotheses were conceived prior to the analysis) is scientifically dishonest, and a major contributor to the replication crisis in science."
  },
  {
    "objectID": "slides/Chapter02.html#plots-for-categorical-data",
    "href": "slides/Chapter02.html#plots-for-categorical-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Plots for categorical data",
    "text": "Plots for categorical data\nBar graphs\n\nShow the frequency of each category (level) in categorical variables\nThe height of each bar is proportional to the frequency\nCan be “stacked” or “clustered”"
  },
  {
    "objectID": "slides/Chapter02.html#tea-data",
    "href": "slides/Chapter02.html#tea-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Tea data",
    "text": "Tea data\nData from 300 individuals’ tea-drinking habits (18 questions), perceptions (12 questions), and personal details (4 questions).\n\ndata(tea, package = \"FactoMineR\")\nglimpse(tea)\n\nRows: 300\nColumns: 36\n$ breakfast        &lt;fct&gt; breakfast, breakfast, Not.breakfast, Not.breakfast, b…\n$ tea.time         &lt;fct&gt; Not.tea time, Not.tea time, tea time, Not.tea time, N…\n$ evening          &lt;fct&gt; Not.evening, Not.evening, evening, Not.evening, eveni…\n$ lunch            &lt;fct&gt; Not.lunch, Not.lunch, Not.lunch, Not.lunch, Not.lunch…\n$ dinner           &lt;fct&gt; Not.dinner, Not.dinner, dinner, dinner, Not.dinner, d…\n$ always           &lt;fct&gt; Not.always, Not.always, Not.always, Not.always, alway…\n$ home             &lt;fct&gt; home, home, home, home, home, home, home, home, home,…\n$ work             &lt;fct&gt; Not.work, Not.work, work, Not.work, Not.work, Not.wor…\n$ tearoom          &lt;fct&gt; Not.tearoom, Not.tearoom, Not.tearoom, Not.tearoom, N…\n$ friends          &lt;fct&gt; Not.friends, Not.friends, friends, Not.friends, Not.f…\n$ resto            &lt;fct&gt; Not.resto, Not.resto, resto, Not.resto, Not.resto, No…\n$ pub              &lt;fct&gt; Not.pub, Not.pub, Not.pub, Not.pub, Not.pub, Not.pub,…\n$ Tea              &lt;fct&gt; black, black, Earl Grey, Earl Grey, Earl Grey, Earl G…\n$ How              &lt;fct&gt; alone, milk, alone, alone, alone, alone, alone, milk,…\n$ sugar            &lt;fct&gt; sugar, No.sugar, No.sugar, sugar, No.sugar, No.sugar,…\n$ how              &lt;fct&gt; tea bag, tea bag, tea bag, tea bag, tea bag, tea bag,…\n$ where            &lt;fct&gt; chain store, chain store, chain store, chain store, c…\n$ price            &lt;fct&gt; p_unknown, p_variable, p_variable, p_variable, p_vari…\n$ age              &lt;int&gt; 39, 45, 47, 23, 48, 21, 37, 36, 40, 37, 32, 31, 56, 6…\n$ sex              &lt;fct&gt; M, F, F, M, M, M, M, F, M, M, M, M, M, M, M, M, M, F,…\n$ SPC              &lt;fct&gt; middle, middle, other worker, student, employee, stud…\n$ Sport            &lt;fct&gt; sportsman, sportsman, sportsman, Not.sportsman, sport…\n$ age_Q            &lt;fct&gt; 35-44, 45-59, 45-59, 15-24, 45-59, 15-24, 35-44, 35-4…\n$ frequency        &lt;fct&gt; 1/day, 1/day, +2/day, 1/day, +2/day, 1/day, 3 to 6/we…\n$ escape.exoticism &lt;fct&gt; Not.escape-exoticism, escape-exoticism, Not.escape-ex…\n$ spirituality     &lt;fct&gt; Not.spirituality, Not.spirituality, Not.spirituality,…\n$ healthy          &lt;fct&gt; healthy, healthy, healthy, healthy, Not.healthy, heal…\n$ diuretic         &lt;fct&gt; Not.diuretic, diuretic, diuretic, Not.diuretic, diure…\n$ friendliness     &lt;fct&gt; Not.friendliness, Not.friendliness, friendliness, Not…\n$ iron.absorption  &lt;fct&gt; Not.iron absorption, Not.iron absorption, Not.iron ab…\n$ feminine         &lt;fct&gt; Not.feminine, Not.feminine, Not.feminine, Not.feminin…\n$ sophisticated    &lt;fct&gt; Not.sophisticated, Not.sophisticated, Not.sophisticat…\n$ slimming         &lt;fct&gt; No.slimming, No.slimming, No.slimming, No.slimming, N…\n$ exciting         &lt;fct&gt; No.exciting, exciting, No.exciting, No.exciting, No.e…\n$ relaxing         &lt;fct&gt; No.relaxing, No.relaxing, relaxing, relaxing, relaxin…\n$ effect.on.health &lt;fct&gt; No.effect on health, No.effect on health, No.effect o…"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-one-variable",
    "href": "slides/Chapter02.html#bar-charts-one-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — one variable",
    "text": "Bar charts — one variable\n\n\nggplot(tea) +\n  geom_bar(aes(x = price)) + \n  ggtitle(\"Bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-two-variables",
    "href": "slides/Chapter02.html#bar-charts-two-variables",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — two variables",
    "text": "Bar charts — two variables\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts---flipped",
    "href": "slides/Chapter02.html#bar-charts---flipped",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts - flipped",
    "text": "Bar charts - flipped\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\") +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  aes(x = price) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nPie charts are popular but not usually the best way to show proportional data\nRequires comparison of angles or areas of different shapes\nBar charts are almost always better\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs",
    "href": "slides/Chapter02.html#one-dimensional-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nggplot(Animals) +\n  aes(x = brain) + \n  geom_dotplot() + \n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Dotplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs-1",
    "href": "slides/Chapter02.html#one-dimensional-graphs-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nAnimals |&gt; \n  mutate(\n    Animal = fct_reorder(\n      rownames(Animals), \n      brain )\n    ) |&gt; \n  ggplot() +\n  aes( y = Animal, \n       x = brain\n       ) + \n  geom_point() + \n  ylab(\"Animal\") + \n  ggtitle(\"Strip chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#histograms",
    "href": "slides/Chapter02.html#histograms",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nDivide the data range into “bins”, count the occurrences in each bin, and make a bar chart.\nY-axis can show raw counts, relative frequencies, or densities\n\nset.seed(1234); dfm &lt;- data.frame(X = rnorm(50, 100))\n\np1 &lt;- ggplot(dfm, aes(X)) + geom_histogram(bins = 20) + ylab(\"count\") + ggtitle(\"Frequency histogram\", \"Heights of the bars sum to n\")\np2 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(count/sum(count))) + geom_histogram(bins = 20) + ylab(\"relative frequency\") +\n  ggtitle(\"Relative frequency histogram\", \"Heights sum to 1\")\np3 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(density)) + geom_histogram(bins = 20) + \n  ggtitle(\"Density histogram\",\"Heights x widths sum to 1\")\n\nlibrary(patchwork); p1+p2+p3"
  },
  {
    "objectID": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "href": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Frequency polygon & kernel density plots",
    "text": "Frequency polygon & kernel density plots\n\n\nHistograma coarse approximation of the density\n\nggplot(vital) + aes(Life_female) + \n  geom_histogram(bins = 12) +\n  geom_freqpoly(bins = 12)\n\n\n\n\n\n\n\n\n\nKernel densitya smooth approximation of the density\n\nggplot(vital) + aes(Life_female) +\n  geom_histogram(bins = 12, aes(y = after_stat(density))) + \n  geom_density()"
  },
  {
    "objectID": "slides/Chapter02.html#kernel-density-estimation-kde",
    "href": "slides/Chapter02.html#kernel-density-estimation-kde",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Kernel density estimation (KDE)",
    "text": "Kernel density estimation (KDE)"
  },
  {
    "objectID": "slides/Chapter02.html#summary-statistics-for-eda",
    "href": "slides/Chapter02.html#summary-statistics-for-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary statistics for EDA",
    "text": "Summary statistics for EDA"
  },
  {
    "objectID": "slides/Chapter02.html#five-number-summary",
    "href": "slides/Chapter02.html#five-number-summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Five-number summary",
    "text": "Five-number summary\nMinimum, lower hinge, median, upper hinge and maximum\n\nset.seed(1234)\nmy.data &lt;- rnorm(50, 100)\nfivenum(my.data)\n\n[1]  97.65430  99.00566  99.46477  99.98486 102.41584\n\nsummary(my.data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.65   99.01   99.46   99.55   99.96  102.42"
  },
  {
    "objectID": "slides/Chapter02.html#boxplots",
    "href": "slides/Chapter02.html#boxplots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Boxplots",
    "text": "Boxplots\n\nGraphical display of 5-number summary\nCan show several groups of data on the same graph"
  },
  {
    "objectID": "slides/Chapter02.html#letter-value-table",
    "href": "slides/Chapter02.html#letter-value-table",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Letter-Value table",
    "text": "Letter-Value table\nAn extension of the boxplot, suitable for large data sets\n\n\nShows:\n\nM: Median\nF: Fourths\nE: Eighths\nD: Sixteenths\nC: Thirty-seconds\n…\n\n\n\n\n  Depth    Lower     Upper       Mid   Spread\nM  25.5 99.92736  99.92736  99.92736 0.000000\nF  13.0 99.43952 100.70136 100.07044 1.261832\nE   7.0 98.93218 101.20796 100.07007 2.275786\nD   4.0 98.73494 101.55871 100.14682 2.823770\nC   2.5 98.52396 101.75099 100.13747 3.227034\nB   1.5 98.17334 101.97793 100.07564 3.804590\nA   1.0 98.03338 102.16896 100.10117 4.135573\n\n\n\n\nHofmann, H; Wickham, H.; Kafadar, K. 2017. “Letter-Value Plots: Boxplots for Large Data.” Journal of Computational and Graphical Statistics 26 (3): 469–77. https://doi.org/10.1080/10618600.2017.1305277."
  },
  {
    "objectID": "slides/Chapter02.html#letter-value-plot",
    "href": "slides/Chapter02.html#letter-value-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Letter-Value Plot",
    "text": "Letter-Value Plot\nAn extension of the boxplot, suitable for large data sets"
  },
  {
    "objectID": "slides/Chapter02.html#cumulative-frequency-graphs",
    "href": "slides/Chapter02.html#cumulative-frequency-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Cumulative frequency graphs",
    "text": "Cumulative frequency graphs\n\nShow the left tail area\nUseful to obtain the quantiles (deciles, percentiles, quartiles etc)\n\n\n\nset.seed(123)\n\nd &lt;- data.frame(\n  x = rnorm(50, 100)\n  )\n\nggplot(d) + \n  aes(x) + \n  stat_ecdf()"
  },
  {
    "objectID": "slides/Chapter02.html#shiny-apps",
    "href": "slides/Chapter02.html#shiny-apps",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nLots of examples are available\n\nIn the study guide and workshops for this course (though not all of them are working currently)\nOn the web\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.univariate.graphs/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.univariate.plots/"
  },
  {
    "objectID": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "href": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile (Q-Q) plot",
    "text": "Quantile-Quantile (Q-Q) plot\nQ-Q plots compare the distributions of two data sets by plotting their quantiles against each other.\n\n\nvital &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/vital.txt\", \n  header=TRUE, sep=\",\")\n\nquants &lt;- seq(0, 1, 0.05)\n\nvital |&gt; \n  summarise(\n    Female = quantile(Life_female, quants),\n    Male = quantile(Life_male, quants)\n  ) |&gt; \n  ggplot() +\n  aes(x = Female, y = Male) +\n  geom_point() + \n  geom_abline(slope=1, intercept=0) +\n  coord_fixed() +\n  ggtitle(\n    \"Quantiles of life expectancy\",\n    subtitle = \"are lower for males vs females\"\n    )"
  },
  {
    "objectID": "slides/Chapter02.html#some-q-q-plot-patterns",
    "href": "slides/Chapter02.html#some-q-q-plot-patterns",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Some Q-Q Plot patterns",
    "text": "Some Q-Q Plot patterns\n\nCase a: Quantiles of Y (mean/median etc) are higher than those of X\nCase b: Spread or SD of Y &gt; spread or SD of X\nCase c: X and Y follow different distributions \n\nR function: qqplot()."
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships",
    "href": "slides/Chapter02.html#bivariate-relationships",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-1",
    "href": "slides/Chapter02.html#bivariate-relationships-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_smooth(span = 0.8, se = FALSE) + \n  ggtitle(\"Scatterplot with lowess smoother\")"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-2",
    "href": "slides/Chapter02.html#bivariate-relationships-2",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_density_2d() +\n  ggtitle(\"Scatterplot with 2D density\")"
  },
  {
    "objectID": "slides/Chapter02.html#marginal-plot",
    "href": "slides/Chapter02.html#marginal-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Marginal Plot",
    "text": "Marginal Plot\nShows both bivariate relationships and univariate (marginal) distributions\n\n\np1 &lt;- ggplot(rangitikei) +\n  aes(x = people, y = vehicle) + \n  geom_point() + theme_bw()\n\nlibrary(ggExtra)\nggMarginal(p1, type=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#shiny-apps-1",
    "href": "slides/Chapter02.html#shiny-apps-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nExplore (though many of these are currently not working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.bivariate.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.bivariate.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.facet.wrapped.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.facet.wrapped.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.facet.grid.plots/"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "href": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot / scatterplot matrix",
    "text": "Pairs plot / scatterplot matrix\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1])"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "href": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot with a grouping variable",
    "text": "Pairs plot with a grouping variable\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1], \n        aes(colour = pinetree$Area))"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-coefficients",
    "href": "slides/Chapter02.html#correlation-coefficients",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation coefficients",
    "text": "Correlation coefficients\nThe Pearson correlation coefficient measures the linear association between two variables."
  },
  {
    "objectID": "slides/Chapter02.html#correlation-matrix",
    "href": "slides/Chapter02.html#correlation-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\nTo show all pairwise correlation coefficients\nUseful to explore the inter-relationship between variables \n\n\n\nlibrary(psych)\ncorr.test(pinetree[,-1])\n\n\nCall:corr.test(x = pinetree[, -1])\nCorrelation matrix \n        Top Third Second First\nTop    1.00  0.92   0.96  0.97\nThird  0.92  1.00   0.95  0.91\nSecond 0.96  0.95   1.00  0.97\nFirst  0.97  0.91   0.97  1.00\nSample Size \n[1] 60\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       Top Third Second First\nTop      0     0      0     0\nThird    0     0      0     0\nSecond   0     0      0     0\nFirst    0     0      0     0\n\n To see confidence intervals of the correlations, print with the short=FALSE option"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-plots",
    "href": "slides/Chapter02.html#correlation-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Plots",
    "text": "Correlation Plots\n\n\nlibrary(corrplot)\ncorrplot(\n  cor(pinetree[,-1]),  \n  type = \"upper\", \n  method=\"number\"\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#network-plots",
    "href": "slides/Chapter02.html#network-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Network plots",
    "text": "Network plots\n\n\nlibrary(corrr)\npinetree[,-1] |&gt; \n  correlate() |&gt; \n  network_plot(min_cor=0.2)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots",
    "href": "slides/Chapter02.html#d-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-1",
    "href": "slides/Chapter02.html#d-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1 + aes(colour = Area)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "href": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D plots are far more useful if you can rotate them",
    "text": "3-D plots are far more useful if you can rotate them\nPackage plot3D\n\n\nlibrary(\"plot3D\")\n\nscatter3D(\n  x = pinetree$First, \n  y = pinetree$Second, \n  z = pinetree$Top, \n  phi = 0, bty = \"g\", \n  ticktype =\"detailed\"\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them-1",
    "href": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D plots are far more useful if you can rotate them",
    "text": "3-D plots are far more useful if you can rotate them\nPackage plotly\n\n\nlibrary(plotly)\n\nplot_ly(\n  pinetree, \n  x = ~First, \n  y = ~Second, \n  z = ~Top\n  ) |&gt; \n  add_markers()"
  },
  {
    "objectID": "slides/Chapter02.html#contour-plots",
    "href": "slides/Chapter02.html#contour-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Contour plots",
    "text": "Contour plots\n\n3D plots are difficult to interpret than 2D plots in general\nContour plots are another way of looking three variables in two dimensions\n\n\n\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots",
    "href": "slides/Chapter02.html#conditioning-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\ncoplot(Top ~ First | Second*Area, \n       data = pinetree)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots-1",
    "href": "slides/Chapter02.html#conditioning-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"mpru/ggcleveland\")\nlibrary(ggcleveland)\ngg_coplot(\n  pinetree, \n  x = First, \n  y = Top, \n  faceting = Second, \n  number_bins = 6, \n  overlap = 3/4\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#more-r-graphs",
    "href": "slides/Chapter02.html#more-r-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "More R graphs",
    "text": "More R graphs\nBuild plots in a single layout (R packages patchwork or gridExtra)\n\n\np1 &lt;- ggplot(testmarks) +\n  aes(y = English, x = Maths) + \n  geom_point()\n\np2 &lt;- p1 + \n  stat_density_2d(\n    geom = \"raster\",\n    aes(fill = after_stat(density)),\n    contour = FALSE) + \n  scale_fill_viridis_c() + \n  guides(fill=FALSE)\n\nlibrary(patchwork)\np1 / p2"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-data",
    "href": "slides/Chapter02.html#time-series-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series data",
    "text": "Time series data\n\nA Time Series is an ordered sequence of observations of a variable(s) (often) made at equally spaced time points.\nTime series Components of variation\n\nTrend - representing long term positive (upward) or negative (downward) movement\nSeasonal - a periodic behaviour happening within a block (say Christmas time) of a given time period (say in a calendar year) but this periodic behaviour will repeat fairly regularly over time (say year after year)\nError (Residual)"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-example",
    "href": "slides/Chapter02.html#time-series-example",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series example",
    "text": "Time series example\n\n\n\n\nnotes = read.table(\"../data/20dollar.txt\", header=TRUE, sep=\"\")\n\nlibrary(forecast)\n\n# value in millions; turn into time series (ts) object\nNZnotes20 &lt;- ts(notes$value / 1000, start=1968, frequency=1)\n\nautoplot(\n  NZnotes20, \n  xlab=\"Year\", \n  ylab=\"Value of $20 notes (millions)\"\n  ) + geom_point()"
  },
  {
    "objectID": "slides/Chapter02.html#autocorrelation-function-acf",
    "href": "slides/Chapter02.html#autocorrelation-function-acf",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Autocorrelation function (ACF)",
    "text": "Autocorrelation function (ACF)\nAutocorrelation captures the extent to which neighbouring data values are similar to each other.\nThe \\(k ^ \\text{th}\\) order ACF or the autocorrelation between \\(x_t\\) and \\(x_{t-k}\\) is\n\\[\\frac{\\text{Covariance}(x_t, x_{t-k})}{\\text{SD}(x_t)\\text{SD}(x_{t-k})} = \\frac{\\text{Covariance}(x_t, x_{t-k})}{\\text{Variance}(x_t)}\\]"
  },
  {
    "objectID": "slides/Chapter02.html#autocorrelation-function-acf-plot",
    "href": "slides/Chapter02.html#autocorrelation-function-acf-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Autocorrelation function (ACF) Plot",
    "text": "Autocorrelation function (ACF) Plot\nThe significance of autocorrelations may be judged from the 95% confidence interval band\n\n\nggAcf(NZnotes20)\n\n\n\n\n\n\n\n\n\n\nAutocorrelations decay to zero ($20 notes positively depend on the values of $20 notes held in the immediate past rather than too distant past)"
  },
  {
    "objectID": "slides/Chapter02.html#pacf-partial-autocorrelation-function",
    "href": "slides/Chapter02.html#pacf-partial-autocorrelation-function",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "PACF (Partial Autocorrelation Function)",
    "text": "PACF (Partial Autocorrelation Function)\nA type of correlation after removing the effect of earlier lags\n\n\nggPacf(NZnotes20)"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-trend-types",
    "href": "slides/Chapter02.html#time-series-trend-types",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series trend types",
    "text": "Time series trend types\nRequires a (parametric) model to fit the trend (covered later)\n\n\n\nNon-parametric fits can also be made"
  },
  {
    "objectID": "slides/Chapter02.html#seasonality",
    "href": "slides/Chapter02.html#seasonality",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Seasonality",
    "text": "Seasonality\nSimple scatter plot of the response variable against time may reveal seasonality directly\n\n\n# Data from:\n# https://www.stats.govt.nz/indicators/uv-intensity/\n\nuv = read.table(\"../data/uv.txt\",\n                header=TRUE, sep=\"\")\n\nuv &lt;- ts(uv$erythemal.uv, \n         start=c(1990,1), \n         frequency=12)\n\nautoplot(uv, \n         xlab=\"time\", \n         ylab=\"Erythemal UV\")"
  },
  {
    "objectID": "slides/Chapter02.html#sub-series-plots",
    "href": "slides/Chapter02.html#sub-series-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Sub-series plots",
    "text": "Sub-series plots\nSeasonality is easily seen graphically when grouping variables are used\n\np1 &lt;- ggseasonplot(uv)\np2 &lt;- ggsubseriesplot(uv)\nlibrary(patchwork); p1 + p2"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-decomposition",
    "href": "slides/Chapter02.html#time-series-decomposition",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series decomposition",
    "text": "Time series decomposition\n\n\n\nAdditive model\n\\(X_t\\) = Trend + Seasonal + Error\n(where \\(X_t\\) is an observation at time \\(t\\))\nMultiplicative model\n\\(X_t\\) = Trend \\(\\times\\) Seasonal + Error\n(trend and seasonal components are not independent)\nDetrending means removing the trend from the series, making it easier to see the seasonality.\nDeseasoning means removing the seasonality from the series, making it easier to see the trend.\n\n\n\n\n\nuv |&gt; decompose(type=\"additive\") |&gt; autoplot() + ggtitle(\"\")"
  },
  {
    "objectID": "slides/Chapter02.html#learning-eda",
    "href": "slides/Chapter02.html#learning-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Learning EDA",
    "text": "Learning EDA\n\nThe best way to learn EDA is to try many approaches and find which are informative and which are not.\n\nChatfield (1995) on tackling statistical problems:\n\nDo not attempt to analyse the data until you understand what is being measured and why. Find out whether there is prior information such as are there any likely effects.\nFind out how the data were collected.\nLook at the structure of the data.\nThe data then need to be carefully examined in an exploratory way before attempting a more sophisticated analysis.\nUse common sense, and be honest!"
  },
  {
    "objectID": "slides/Chapter02.html#summary",
    "href": "slides/Chapter02.html#summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary",
    "text": "Summary\n\nSize\n\nFor small datasets, we cannot be too confident in any patterns we see. More likely for patterns to occur ‘by chance’.\nSome displays are more affected by sample size than others\n\nShape\n\nIn can be interesting to display the overall shape of distribution.\nAre there gaps and/or many peaks (modes)?\nIs the distribution symmetrical? Is the distribution normal?\n\nOutliers\n\nBoxplots & scatterplots can reveal outliers\nMore influential than points in the middle\n\nGraphs should be simple and informative; certainly not misleading!\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for 161250 Data Analysis."
  },
  {
    "objectID": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "href": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "title": "Chapter 1:Data Collection",
    "section": "The nature of data: random variables",
    "text": "The nature of data: random variables\nData come from recording observations of the world. E.g.,\n\nrecording the number of heart beats per minute\ncounting birds in your yard\nmeasuring the lengths of fish you catch\n\nEach time you collect one of these observations, it is likely to be different.\n\nPulse may vary between 55 and 70 bpm, depending on when you record it\nNumber of birds varies at different times, and across different yards.\n\nWe call these random variables (often denoted with an upper case letter, \\(X\\)), because the particular value that a single observation or measurement will take is uncertain. The value varies across observations."
  },
  {
    "objectID": "slides/Chapter01.html#types-of-data",
    "href": "slides/Chapter01.html#types-of-data",
    "title": "Chapter 1:Data Collection",
    "section": "Types of data",
    "text": "Types of data"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "href": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of qualitative data",
    "text": "Subtypes of qualitative data\n\n\n\n\nNominal variables have no particular order (e.g., gender, colour, species, country)\n\n\n\nOrdinal variables can be ordered (e.g., altitude = {low, mid, high}, age group = {child, juvenile, adult} )"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "href": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of quantitative data",
    "text": "Subtypes of quantitative data\n\n\n\n\nContinuous variables have no gaps between possible values, as in measurements (e.g., weight, temperature, length)\n\n\n\nDiscrete variables have gaps between possible values, as in counts (e.g., number of siblings, number of flowers)"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-continuous-data",
    "href": "slides/Chapter01.html#subtypes-of-continuous-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of continuous data",
    "text": "Subtypes of continuous data\n\nInterval scale\n\nNo absolute zero\nDivision & subtraction may not be meaningful\nTemperature in degrees Celcius is interval because 20°C is not twice as hot as 10°C.\n\n\nRatio scale\n\nZero = zero\nAll arithmetic manipulation can be done\nLength is ratio because 20 mm is twice as long as 10 mm."
  },
  {
    "objectID": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "href": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "title": "Chapter 1:Data Collection",
    "section": "Data Collection: Survey, Experiment, Census",
    "text": "Data Collection: Survey, Experiment, Census\n\n\n\nWe collect data from the world to get information about patterns and processes.\nMost datasets contain a subset, a sample, of a much bigger population of interest.\n\nWe may conduct a survey to collect a sample of data from different places, times, people, or organisms. We would rarely survey all of them.\nWe might conduct an experiment where we take a sample of elements (people, organisms, objects) and apply some treatment in a lab (e.g., drug, temperature, exercise regime, or other treatment) to study its effects.\n\nIf we are not dealing with a sample, if every element of the population of interest is represented in the dataset, we call this a census rather than a sample."
  },
  {
    "objectID": "slides/Chapter01.html#measurement-issues",
    "href": "slides/Chapter01.html#measurement-issues",
    "title": "Chapter 1:Data Collection",
    "section": "Measurement issues",
    "text": "Measurement issues\n\nMeasuring Devices or Instruments\n\na physical device - measuring rule to gauge the heights of plants\na counting device - a Geiger- counter for measuring radioactive material\na questionnaire - requires a more subjective response.\n\nMeasurement Error\n\nmeasuring instrument may be faulty (bias)\nvalues recorded from the same object may vary from one measurement to another (variance)\n\nIndirect measures\n\nFor example, we use Body Mass Index (BMI) as a measure of condition, and we measure temperature with the expansion of mercury."
  },
  {
    "objectID": "slides/Chapter01.html#non-response",
    "href": "slides/Chapter01.html#non-response",
    "title": "Chapter 1:Data Collection",
    "section": "Non-response",
    "text": "Non-response\n\na non-sampling error\nSelection stage: an element may be selected but not found\n\ne.g. sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey.\n\nCollection stage: it may not be possible to take a measurement\n\nsome respondents may forget, or refuse, to answer the questionnaire\n\nDocumentation stage\n\nIncorrect record of measurement\n\nCall-backs reduce non-response"
  },
  {
    "objectID": "slides/Chapter01.html#census-related-concepts",
    "href": "slides/Chapter01.html#census-related-concepts",
    "title": "Chapter 1:Data Collection",
    "section": "Census related concepts",
    "text": "Census related concepts\nTARGET POPULATION the population under study\nFRAME operationalises data collection from a target population. e.g. listing of elements in population.\nACTUAL POPULATION is the resulting set of elements on which usable data have been collected."
  },
  {
    "objectID": "slides/Chapter01.html#sample-vs-population",
    "href": "slides/Chapter01.html#sample-vs-population",
    "title": "Chapter 1:Data Collection",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\n\nA sample is a subset of the population.\nDatasets usually only contain a sample from the population; rarely do we have the entire population of data!\nWhy sample?\n\nSampling conserves resources (money, time, etc.).\nA well collected sample is more useful than a badly designed census.\nCollecting data may be destructive.\nThe disadvantage: the statistics we calculate from sample data is subject to sampling variation, which introduces uncertainty* about their true values.\n\n\n\n\n\n“You don’t have to eat the whole ox to know that the meat is tough” – Samuel Johnson (1709-1784)"
  },
  {
    "objectID": "slides/Chapter01.html#population-frame-and-sample",
    "href": "slides/Chapter01.html#population-frame-and-sample",
    "title": "Chapter 1:Data Collection",
    "section": "Population, frame, and sample",
    "text": "Population, frame, and sample"
  },
  {
    "objectID": "slides/Chapter01.html#statistical-inference",
    "href": "slides/Chapter01.html#statistical-inference",
    "title": "Chapter 1:Data Collection",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical inference is the process of using information from sample data to make conclusions about the population.\n\nFor example, we want to know \\(\\mu\\), mean length of fish in a population. So, we collect a sample of fish, measure their lengths, calculate the mean \\(\\bar{x}\\), and use \\(\\bar{x}\\) as an estimate of \\(\\mu\\). This is statistical inference.\n\n\n\n\nThe sample mean \\(\\bar{x}\\) depends on which particular fish we happened to get in our sample.\nTherefore, the sample mean \\(\\bar{x}\\) itself is a random variable.\nIf we were to take 1000 different samples, we’d get 1000 different means."
  },
  {
    "objectID": "slides/Chapter01.html#bias-vs-sampling-variance",
    "href": "slides/Chapter01.html#bias-vs-sampling-variance",
    "title": "Chapter 1:Data Collection",
    "section": "Bias vs sampling variance",
    "text": "Bias vs sampling variance\n\n\nA method used to estimate \\(\\hat{\\theta}\\) a population parameter \\(\\theta\\) is called an estimator. An estimator includes the study design, methods of data collection, and mathematical operations.\nSampling variance is the sample-to-sample variation in an estimator.\nBias is when our estimator doesn’t get it right on average. That is, the average of estimates over \\(\\infty\\) samples is not centred on the population parameter; \\(\\text{Mean}(\\hat{\\theta}) \\neq \\theta\\).\n\n\n\n\nAn estimator can have high/low sampling variance and high/low bias."
  },
  {
    "objectID": "slides/Chapter01.html#principle-of-randomisation",
    "href": "slides/Chapter01.html#principle-of-randomisation",
    "title": "Chapter 1:Data Collection",
    "section": "Principle of randomisation",
    "text": "Principle of randomisation\n\n\n\nWe want our sample to be representative of (and have similar properties to) the population. The most straightforward way to do this is through randomisation.\nWe randomise the selection of objects for our sample to avoid bias. If we (consciously or subconsciously) tended to chose the largest fish for our sample, we’d get an upwardly biased estimate of the lengths.\nSimple random sampling or EPSEM (equal probability of selection) is the gold standard of random sampling."
  },
  {
    "objectID": "slides/Chapter01.html#simple-random-sampling-srs",
    "href": "slides/Chapter01.html#simple-random-sampling-srs",
    "title": "Chapter 1:Data Collection",
    "section": "Simple Random Sampling (SRS)",
    "text": "Simple Random Sampling (SRS)\n\n\n\nRandom selection of elements\n\n“Random” refers to the process not outcome\nEach (sampling) unit has same chance of being selected\nUnits can be selected with & without replacement\n\n\n\n\n\n\n\nSRS is easy to handle; suits even for a poor sampling frame\nSRS can be costly to implement\nSRS estimates are more variable than some alternatives\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#stratified-random-sampling-strs",
    "href": "slides/Chapter01.html#stratified-random-sampling-strs",
    "title": "Chapter 1:Data Collection",
    "section": "Stratified Random Sampling (STRS)",
    "text": "Stratified Random Sampling (STRS)\n\n\n\nSuitable for heterogeneous populations\nPopulation is divided into relatively homogeneous groups called strata and a random sample is taken from each stratum.\n\n\n\n\n\n\nSampling Approaches\n\nSample the larger strata more heavily (suits when all the strata are equally variable)\nSample the more varied strata are sampled\n\nAdvantages of STRS\n\nleads to efficient estimation That is, the variance (of an estimate) is usually less than that of SRS\nsample is spread throughout population\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#cluster-sampling",
    "href": "slides/Chapter01.html#cluster-sampling",
    "title": "Chapter 1:Data Collection",
    "section": "Cluster sampling",
    "text": "Cluster sampling\n\n\n\nA convenient method of sampling\npopulation is composed of clusters (groups)\nSelect certain clusters (randomly) and collect measurements from a random selection of the elements within the chosen clusters\nLarger variance than SRS!\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "href": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "title": "Chapter 1:Data Collection",
    "section": "Systematic Random sampling (SyRS)",
    "text": "Systematic Random sampling (SyRS)\n\n\n\nSelect every \\(k^{th}\\) element!\nRandom start within the first block of elements.\n\nConvenient and also the sample will be representative of population\nVariance of estimates - generally greater than those of SRS\nInefficient/inappropriate, if cycle or trend is present\n\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#other-sampling-methods",
    "href": "slides/Chapter01.html#other-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Other Sampling methods",
    "text": "Other Sampling methods\n\nProbability proportional to size (PPS)\n\ne.g., sampling high-value companies more likely than low-value companies\n\nMultistage\n\ne.g., first stage - cluster; second stage - SRS\n\nNon-probability sampling methods\n\nHaphazard / opportunistic / volunteer; take what you can get!\nSnowball; get your participants to find new participants\nPurposive; select items with certain characteristics; e.g., patients with particular symptoms\n\nNon-probability samples are often treated as random, requiring the assumption that the sample is representative. The validity of this assumption should be carefully considered."
  },
  {
    "objectID": "slides/Chapter01.html#some-sampling-methods",
    "href": "slides/Chapter01.html#some-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Some sampling methods",
    "text": "Some sampling methods"
  },
  {
    "objectID": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "href": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "title": "Chapter 1:Data Collection",
    "section": "Effective Sample size (thumb rule)",
    "text": "Effective Sample size (thumb rule)\n\n\n\n\n\n\n\n\nSample Design\nDesign Effect (\\(d\\))\nEffective Sample Size (\\(\\frac{n}{d}\\))\n\n\n\n\nSRS\n1.00\n\\(n\\)\n\n\nSTRS\n0.80 to 0.90\n\\(\\frac{n}{0.9}\\) to \\(\\frac{n}{0.8}\\)\n\n\nCluster\n1.02 to 1.26\n\\(\\frac{n}{1.26}\\) to \\(\\frac{n}{1.02}\\)\n\n\nSyRS\n1.05\n\\(\\frac{n}{1.05}\\)\n\n\nQuota\n2\n\\(\\frac{n}{2}\\)"
  },
  {
    "objectID": "slides/Chapter01.html#summary",
    "href": "slides/Chapter01.html#summary",
    "title": "Chapter 1:Data Collection",
    "section": "Summary",
    "text": "Summary\n\nIssues to address\n\nWHAT are collected?\nWHO does the data collection?\nHOW are the data collected?\n\nBias occurs due to\n\nSELECTION\nCOLLECTION\nNON-RESPONSE (the single largest cause of bias!)\n\nA sample may have the same biases as a census along with sampling errors\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter03.html#probability-and-randomness",
    "href": "slides/Chapter03.html#probability-and-randomness",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability and randomness",
    "text": "Probability and randomness\n\n\n\n\nProbability and randomness are placeholders for incomplete knowledge.\nAfter I shuffled a deck of cards, you might consider the identity of the top card to be “random”.\nBut is it really?\nIf you knew the starting positions of the cards and a good HD video of my shuffling, you could surely know the positions of the cards, and which is on top.\nLikewise for rolling a die. If we know everything about the starting position, how it was thrown, the texture of the surface, humidity, etc., could we predict what it would roll?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "href": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability as a relative frequency",
    "text": "Probability as a relative frequency\n\n\nThe classical definition of probability is just the relative frequency of an event.\n\nIf a fair die is rolled, there are \\(n = 6\\) possible outcomes. Let the event of interest be getting a number 4 or more. The probability of this event is 3 out of 6 or \\(p=1/2\\).\n\nThe sample space or the set of all possible outcomes need not be finite.\n\nExample: Tossing a coin until the first head appears will result in an infinite sample space. The probability can be viewed as a limiting or long run fraction of \\(m/n\\) (i.e. when \\(n \\to \\infty\\)).\n\nWhen the sample space is finite and outcomes are equally likely, we can assume that classical probability will be the same as empirical probability.\n\nExample: To find the probability of a fair coin landing heads it is not necessary to toss the coin repeatedly and observe the proportion of heads.\n\nProbabilities can only be between \\(0\\) (impossible) and \\(1\\) (certain).\nProbabilities can be subjective (such as expert opinion, or a guess)"
  },
  {
    "objectID": "slides/Chapter03.html#mutually-exclusive-events",
    "href": "slides/Chapter03.html#mutually-exclusive-events",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\nFor mutually exclusive events,\n\nThe probability of any two events co-occurring is zero\nThe probability of one event or another event occurring is the sum of the two respective probabilities.\nThe probability of any one event not occurring is the sum of those remaining.\n\n\nExample: A randomly selected single digit can be either odd (Event \\(O\\)) or even (Event \\(E\\)).\nThe events \\(O\\) and \\(E\\) are mutually exclusive because a number cannot be both odd and even.\nThe sample space is \\(\\{0,1,2,3,4,5,6,7,8,9\\}\\).\n\n\n\n\\(\\rm{Pr(E~\\&~O)=0}\\)\n\\(\\rm{Pr(E~ or~O)=1}\\)\n\\(\\rm{Pr(E)=1-Pr(O)}\\) and \\(\\rm{Pr(O)=1-Pr(E)}\\)"
  },
  {
    "objectID": "slides/Chapter03.html#statistical-independence",
    "href": "slides/Chapter03.html#statistical-independence",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Statistical Independence",
    "text": "Statistical Independence\nIf events \\(A\\) and \\(B\\) are statistically independent, then \\(P(A \\text{ and } B) = P(A) \\times P(B)\\).\n\n\n\n\n\n\nConditional probability\n\n\n\n\\(P(A|B)\\) is the probability of event \\(A\\) occurring given that event \\(B\\) is has occurred.\nFor example, the probability of a card you’ve drawn being a 5, given that it is a spade.\nThe sample space is reduced to that where \\(B\\) (e.g. the card is a spade) has occurred.\n\n\n\n\n\nWe say that two events (\\(A\\) and \\(B\\)) are independent if \\(P(A | B) = P(A)\\) and \\(P(B | A) = P(B)\\).\nObserving event \\(A\\) doesn’t make event \\(B\\) any more or less likely, and vice versa.\nFor any two independent events \\(A\\) and \\(B\\), \\(P(A \\text{ and } B ) = P(A|B) \\times P(B)\\) and \\(P(A \\textbf{ and } B ) = P(B|A) \\times P(A)\\)."
  },
  {
    "objectID": "slides/Chapter03.html#blood-group-example",
    "href": "slides/Chapter03.html#blood-group-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Blood Group Example",
    "text": "Blood Group Example\n\n\n\n Two systems for categorising blood are:\n\nthe Rh system (Rh+ and Rh–)\nthe Kell system (K+ and K–)\n\nFor any person, their blood type in any one system\nis independent of their blood type in any other.\nFor Europeans in New Zealand,\nabout 81% are Rh+ and about 8% are K+.\n\nFrom the table:\n\nIf a European New Zealander is chosen at random, what is the probability that they are (Rh+ and K+) or (Rh– and K–)?\n\n0.0648 + 0.1748 = 0.2396\n\nSuppose that a murder victim has a bloodstain on him with type (Rh– and K+), presumably from the assailant. What is the probability that a randomly selected person matches this type?\n\n0.0152"
  },
  {
    "objectID": "slides/Chapter03.html#bayes-rule",
    "href": "slides/Chapter03.html#bayes-rule",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Bayes rule",
    "text": "Bayes rule\n\\[P(A\\mid B)=\\frac {P(B\\mid A)P(A)}{P(B)}~~~~~~~~\\rm{s.t}~~ P(B)&gt;0\\]\n\n\\(P(A\\mid B)\\) and \\(P(B\\mid A)\\) are conditional probabilities.\n\\(P(A)\\) and \\(P(B)\\) are marginal or prior probabilities."
  },
  {
    "objectID": "slides/Chapter03.html#example",
    "href": "slides/Chapter03.html#example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nSay the following were true:\n\nPrevalence: \\(P(D) = 0.03\\) and \\(P(H) = 1-0.03=0.97\\)\nSensitivity: \\(P(T_+\\mid D) = 0.98\\)\nSpecificity: \\(P(T_{-}\\mid H) = 0.95\\)\n\nWe can use Bayes Rule to answer the following questions:\n\nWhat proportion of the overall population will test positive vs negative?\nWhat are the implications of a positive or negative test result?"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued",
    "href": "slides/Chapter03.html#example-continued",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat proportion of the overall population will test positive vs negative?\nThe overall proportion of positive tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{+}) &= P(T_{+} \\& D) + P(T_{+} \\& H) \\\\\n&= P(T_{+} \\mid D)P(D) + P(T_{+} \\mid H)P(H) \\\\\n&= 0.98 \\times 0.03 + 0.05 \\times 0.97 \\\\\n&= 0.0779\n\\end{aligned}\n\\] The overall proportion of negative tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{-}) &= 1 - P(T_{+}) \\\\ &= 0.9221\n\\end{aligned}\n\\]\nComplete table of probabilities:\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97\n\n\n\n0.0779\n0.9221\n1"
  },
  {
    "objectID": "slides/Chapter03.html#probability-tree-diagram",
    "href": "slides/Chapter03.html#probability-tree-diagram",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability tree diagram",
    "text": "Probability tree diagram\nIt can be useful to visualise the probabilities using a tree diagram.\n\n\n\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches."
  },
  {
    "objectID": "slides/Chapter03.html#discrete-probability-distributions-1",
    "href": "slides/Chapter03.html#discrete-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Discrete probability distributions",
    "text": "Discrete probability distributions\nConsider the number of eggs \\((X)\\) in an Adelie penguin’s nest. The values range from \\(1\\) to \\(5\\), each with a certain probability (or relative frequency) of occurrence.\n\n\nNote the probabilities add to \\(1\\) because \\({1,2,3,4,5}\\) is a complete sample space.\n\nThe population mean \\(\\mu_X\\) is simply the sum of each outcome multiplied by its probability.\n\\[\\mu_X = E(X)= \\sum xP(X=x)=\\sum xP(x)\\]\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\n(Mean=sum(X*P))\n\n[1] 3.15\n\n\nThe population variance is given by\n\\[Var(X)= \\sigma_X^2=\\sum (x-\\mu_X)^2 P(x)\\]\nThe population SD is simply the square-root of the variance.\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\nMean=sum(X*P)\n(Variance =sum((X-Mean)^2*P))\n\n[1] 1.4275\n\n(SD=sqrt(Variance))\n\n[1] 1.19478"
  },
  {
    "objectID": "slides/Chapter03.html#binomial-distribution",
    "href": "slides/Chapter03.html#binomial-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nConsider a variable that has two possible outcomes\n(say success and failure, with 50% probabilty each).\nThis can be described as a “Bernoulli” random variable.\nA “Binomial” is just a collection of Bernoulli trials.\nLet \\(X\\) be the number of heads when two coins are tossed.\nThe count of the number of successes \\(X\\) out of a fixed total of\n\\(n\\) independent trials follows the binomial distribution.\nThat is, \\(X \\sim Bin(n, p)\\), where \\(p\\) the probability of a success.\nThe binomial probability function \\(P(X=x)\\) or \\(P(x)\\)\nis given by \\[P(x)={n \\choose x}p^{x}(1-p)^{n-x}\\]\nFor \\(n=10\\), \\(p=0.3\\), the binomial probabilities,\n\\(P(x)\\) for \\(x=0,1,2, \\dots, 10\\), are plotted to the right.\nIf each of 10 basketball shots succeeded with probability 0.3, this describes the probability of your total score out of 10.\n\n\n\n\ndfm &lt;- data.frame(\n  x = as.factor(0:10), \n  Probability = dbinom(x = 0:10, size = 10, prob = 0.3))\nggplot(dfm) + aes(x = x, y = Probability) + geom_col() +\n  xlab(\"Number of successes (x)\") +\n  annotate(geom = \"table\", label = list(dfm), x=11, y=.05)"
  },
  {
    "objectID": "slides/Chapter03.html#example-1",
    "href": "slides/Chapter03.html#example-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nA microbiologist plates out certain bacteria on a plate, and picks out 10 colonies. She knows that the probability of successfully creating a recombinant is 0.15.\nWhat is the probability that if she mixes all 10 colonies in a growth medium with penicillin, something (anything) will grow?\nIn other words:\nIf \\(X \\sim Bin(n = 10, p = 0.15)\\), what is \\(P(x &gt; 0)\\)?\nNote \\(P(x &gt; 0)=1-P(x = 0)\\). So in R, compute this as follows:\n\n1 - dbinom(x=0, size=10, prob=.15)\n\n[1] 0.8031256\n\n\nor\n\n1-pbinom(q=0, size=10, prob=.15)\n\n[1] 0.8031256"
  },
  {
    "objectID": "slides/Chapter03.html#binomial",
    "href": "slides/Chapter03.html#binomial",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial",
    "text": "Binomial\nThe code pbinom(k,size=n,prob=p) gives the cumulative probabilities up to and including the quantile \\(k\\).\nThe Probability Mass Function (PMS) for a binomial random variable is:\n\\[P(X\\leq k)=\\sum _{i=0}^{k}{n \\choose x}p^{x}(1-p)^{n-x}\\]\nThe mean and variance of the binomial random variable is given by\n\\[\\mu_X=np~~~~ \\sigma^2_X=np(1-p)\\]\nIn the last example, the expected number of recombinant strain of bacteria is\n\\[\\mu_X=np=10*0.15=1.5\\]\nwith standard deviation\n\\[\\sigma_X=\\sqrt {np(1-p)}=1.129159\\]"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution",
    "href": "slides/Chapter03.html#poisson-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is used to obtain the probabilities of counts of relatively rare events that occur independently in space or time.\nSome Examples:\n\nThe number of snails in a quadrat \\((1~m^2)\\)\nFish counts in a visual transect (25m x 5m)\nBacterial colonies in 2 litres of milk"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution-1",
    "href": "slides/Chapter03.html#poisson-distribution-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe random variable \\(X\\), the number of occurrences (count), often follows the Poisson distribution whose probability function is given by\n\\[\\Pr(x)= \\frac{\\lambda^x e^{-\\lambda}}{x!}~~~ x=0,1,2,\\dots, \\infty\\]\nThe parameter \\(\\lambda\\) is the mean which is also equal to the variance.\n\\[\\mu_X=\\lambda~~~~ \\sigma^2_X=\\lambda\\]\nMain assumptions:\n\nThe events occur at a constant average rate of \\(\\lambda\\) per unit time or space.\nOccurrences are independent of one another as well as they do not happen at exactly the same unit time or space."
  },
  {
    "objectID": "slides/Chapter03.html#poisson-example",
    "href": "slides/Chapter03.html#poisson-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson example",
    "text": "Poisson example\n\n\n\n\n\n\n\n\n\n\n\n Consider the number of changes that accumulate along a\nstretch of a neutrally evolving gene over a given period of time.\nThis is a Poisson random variable with a\npopulation mean of \\(\\lambda=kt\\), where\n\\(k\\) is the number of mutations per generation, and\n\\(t\\) is the time in generations that has elapsed.\n     \nAssume that \\(k = 1\\times10^{-4}\\) and \\(t = 500\\).\nFor \\(\\lambda=kt=0.05\\), the Poisson probabilities are shown in the following plot.\nWhat is the probability that at least one mutation has occurred over this period?\n\\(P(x &gt; 0)=1-P(x = 0)\\) is found in R as follows:\n\n1 - dpois(x=0, lambda=0.05)\n\n[1] 0.04877058"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-1",
    "href": "slides/Chapter03.html#continuous-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nA discrete random variable takes values which are simply points on a real line. In other words, there is an inherent discontinuity in the values a discrete random variable can take.\nIf a random variable, \\(X\\), can take any value (i.e., not just integers) in some interval of the real line, it is called a continuous random variable.\nE.g., height, weight, length, percentage protein\nFor a discrete random variable \\(X\\), the associated probabilities \\(P(X=x)\\) are also just points or masses, and hence the probability function \\(P(x)\\) is also called as the probability mass function (PMF).\nFor continuous random variables, probabilities can be computed when the variable falls in an interval such as \\(5\\) to \\(15\\), but not when it takes a fixed value such as \\(10\\) (which is equal to zero).\nThe Probability Density Function (PDF) gives the relative likelihood of any particular value."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-2",
    "href": "slides/Chapter03.html#continuous-probability-distributions-2",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\n\nFor example, consider a random proportion \\((X)\\) between \\(0\\) and \\(1\\). Here \\(X\\) follows a (standard) continuous uniform distribution whose (probability) density function \\(f(x)\\) is defined as follows:\n\\[f(x)=\\begin{cases}{1}~~~\\mathrm {for} \\ 0\\leq x\\leq 1,\\\\[9pt]0~~~\\mathrm {for} \\ x&lt;0\\ \\mathrm {or} \\ x&gt;1\\end{cases}\\] This constant density function is the simple one in the graph to the right.\n\n\n\ntibble(x = seq(-.5, 1.5, length=1000),\n       `f(x)` = dunif(x, min=0, max=1)) |&gt; \n  ggplot() + \n  aes(x = x, y = `f(x)`) +\n  geom_area(colour = 1, alpha = .2)\n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-3",
    "href": "slides/Chapter03.html#continuous-probability-distributions-3",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n  \np\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1."
  },
  {
    "objectID": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "href": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "The Normal (Gaussian) Distribution",
    "text": "The Normal (Gaussian) Distribution\n\n\nThe Gaussian or Normal Distribution is parameterised in terms of the mean \\(\\mu\\) and the variance \\(\\sigma ^{2}\\) and its Probability Density Function (PDF) is given by\n\\[f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}\\] A Standard Normal Distribution has mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). It has a simpler PDF:\n\\[f(z)={\\frac {1}{ {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^{2}}\\] If \\(X \\sim N(\\mu, \\sigma)\\), you can convert the \\(X\\) values into \\(Z\\)-scores by subtracting the mean \\(\\mu\\) and dividing by the standard deviation \\(\\sigma\\).\n\\[Z={\\frac {X-\\mu }{\\sigma }}\\]\nWe often deal with the standard normal because the symmetric bell shape of the normal distribution remains the same for all \\(\\mu\\) and \\(\\sigma\\).\n\n\n \n\ndfn &lt;- tibble(x=seq(-4,4,length=1000), \n              `f(x)` = dnorm(x), \n              `F(x)` = pnorm(x))\np1 &lt;- ggplot(dfn) + aes(x=x,y=`f(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np2 &lt;- ggplot(dfn) + aes(x=x,y=`F(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Cumulative Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np1/p2 \n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#the-standard-normal-distribution",
    "href": "slides/Chapter03.html#the-standard-normal-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "The Standard Normal Distribution",
    "text": "The Standard Normal Distribution"
  },
  {
    "objectID": "slides/Chapter03.html#example-of-a-normal",
    "href": "slides/Chapter03.html#example-of-a-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example of a normal",
    "text": "Example of a normal\n\nThe weight of an individual of Amphibola crenata, a marine snail,\nis normally distributed with a mean of \\(40g\\) and variance of \\(20g^2\\).\n\n  \n\ndfs &lt;- tibble(x=seq(20, 60, length=1000), \n    `f(x)` = dnorm(x, mean=40, sd=sqrt(20)))\n\nps &lt;- ggplot(dfs) + aes(x = x, y = `f(x)`) + \n  geom_area(fill=\"gray\") +\n  geom_vline(xintercept=40) \n\nps\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs between \\(35g\\) and \\(50g\\)?\n\n\n\n\nIn R, the function pnorm() gives the CDF.\n\npnorm(50, mean=40, sd=sqrt(20)) - \n  pnorm(35, mean=40, sd=sqrt(20)) \n\n[1] 0.8555501\n\n\n\n\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &lt; 50 & x &gt; 35),\n            fill=\"coral1\", alpha=.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs below \\(35g\\) or over \\(50g\\)?\n\n\n\npnorm(35, mean=40, sd=sqrt(20)) + \n  pnorm(50, mean=40, sd=sqrt(20), lower.tail=FALSE) \n\n[1] 0.1444499\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &gt; 50),\n            fill=\"coral1\", alpha=.5) + \n  geom_area(data = dfs |&gt; filter(x &lt; 35),\n            fill=\"coral1\", alpha=.5)"
  },
  {
    "objectID": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "href": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Areas (probabilities) under the standard normal",
    "text": "Areas (probabilities) under the standard normal\nUnder standard normal, the areas under the PDF curve are shown below for various situations.\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#log-normal-distribution",
    "href": "slides/Chapter03.html#log-normal-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Log-normal distribution",
    "text": "Log-normal distribution\n\nA random variable \\(X\\) is log-normally distributed, when \\(log_e(X)\\) follows normal.\nAlternatively, if \\(X\\) follows normal, then \\(e^X\\) follows log-normal.\nR function dlnorm() gives the log-normal density.\nMean & variance:\n\\[\n\\begin{align}\n\\mu_X&=e^{\\left(\\mu +{\\frac {\\sigma ^{2}}{2}}\\right)} \\\\\n\\sigma_X^2&=(e^{\\sigma ^{2}}-1) e^{(2\\mu +\\sigma ^{2})}\n\\end{align}\n\\]\n\n\n\ndfln &lt;- tibble(x=seq(0, 4, length=1000), \n              `f(x)` = dlnorm(x), \n              `F(x)` = plnorm(x))\np1 &lt;- ggplot(dfln) + aes(x=x,y=`f(x)`) + geom_area(alpha=.8) + \n  labs(title = \"Standard log-normal density\", \n       x = \"standard log-normal deviate, z\")\np2 &lt;- ggplot(dfln) + aes(x=x,y=`F(x)`) + geom_line() + \n  labs(title = \"Cumulative standard log-normal density\", \n       x = \"standard log-normal deviate, z\")\np1/p2 \n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#weibull-distribution",
    "href": "slides/Chapter03.html#weibull-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Weibull distribution",
    "text": "Weibull distribution\n\nThe PDF of the Weibull distribution is:\n\\[f(t;\\eta,\\beta) =\n\\begin{cases}\n\\frac{\\beta}{\\eta}\\left(\\frac{x}{\\eta}\\right)^{\\beta-1}e^{-(x/\\eta)^{\\beta}}  ~~x\\geq0 ,\\\\\n0~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  x&lt;0,\n\\end{cases}\\]\n\\(\\beta~(&gt; 0)\\) is the called the shape parameter and \\(\\eta~(&gt; 0)\\) is the called scale parameter.\nThe Weibull distribution becomes the exponential distribution for \\(\\beta=1\\).\nThe scale parameter \\(\\eta\\) is called the characteristic life because \\(\\eta\\) becomes the quantile with slightly less than two-thirds of the population (63%) below it irrespective of the shape \\(\\beta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#gamma-distribution",
    "href": "slides/Chapter03.html#gamma-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nThe probability function of the gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\beta\\) is given below:\n\\[\\displaystyle {\\begin{aligned}f(x)={\\frac {\\beta ^{\\alpha }x^{\\alpha -1}e^{-\\beta x}}{\\Gamma (\\alpha )}}\\quad {\\text{ for }}x&gt;0\\quad \\alpha ,\\beta &gt;0,\\\\[6pt]\\end{aligned}}\\] where \\(\\displaystyle \\Gamma (\\alpha)=\\int _{0}^{\\infty }x^{\\alpha-1}e^{-x}\\,dx.\\)\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#beta-distribution",
    "href": "slides/Chapter03.html#beta-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Beta distribution",
    "text": "Beta distribution\nThe beta distribution is bounded on the interval \\([0, 1]\\) and parameterised by two positive shape parameters, say \\(\\alpha\\) and \\(\\beta\\).\nProbability function of the beta distribution:\n\\[\\begin{aligned}f(x;\\alpha ,\\beta ) ={\\frac {x^{\\alpha -1}(1-x)^{\\beta -1}}{\\displaystyle \\int _{0}^{1}u^{\\alpha -1}(1-u)^{\\beta -1}\\,du}}={\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}x^{\\alpha -1}(1-x)^{\\beta -1}\\end{aligned} \\] where \\(\\mathrm {B} (\\alpha ,\\beta )=\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}\\). Here’s a plot of beta density for various shape parameter combinations.\n\nWhen \\(\\alpha=\\beta=1\\), the beta distribution becomes the continuous uniform distribution.\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#small-sample-effect",
    "href": "slides/Chapter03.html#small-sample-effect",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Small sample effect",
    "text": "Small sample effect\nFor small samples, the shape might be difficult to judge.\n\n\nset.seed(1234)\ndfm &lt;- data.frame(\n  x=rnorm(50, \n          mean=80, \n          sd=12)\n  )\n\np1 &lt;- ggplot(dfm) + \n  geom_histogram(\n    aes(x=x, y=after_stat(density)), \n    colour=1\n    ) + \n  stat_function(\n    fun = dnorm, \n    args = list(mean = 80, sd = 12), \n    geom = \"line\"\n    ) +\n  xlim(min(dfm), max(dfm))\n\np2 &lt;- ggplot(dfm) + aes(x) + \n  geom_boxplot() +\n  xlim(min(dfm), max(dfm)) +\n  theme_void()\n\nlibrary(patchwork)\np1 / p2 + plot_layout(heights = c(5, 1))\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#normal-quantile-plots",
    "href": "slides/Chapter03.html#normal-quantile-plots",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Normal quantile plots",
    "text": "Normal quantile plots\nIn a normal quantile plot, the quantiles of the sample are plotted against the theoretical quantiles of the fitted normal distribution.\nThe points should roughly lie on a straight line\nWe can also compare the empirical and theoretical CDFs.\n\n\n\n\n\n\n\n\n\nTV viewing time data\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#skewed-data",
    "href": "slides/Chapter03.html#skewed-data",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Skewed data",
    "text": "Skewed data\nThe number of people who made use of a recreational facility is in the rangitikei dataset.\nThe deviation of points from the line indicate that this variable does not conform to a normal distribution.\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#fitting-lognormal",
    "href": "slides/Chapter03.html#fitting-lognormal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Fitting lognormal",
    "text": "Fitting lognormal\nR package fitdistrplus can be used to estimate the lognormal parameters for people data.\nThe likelihood function is based on the joint probability of the observed data as a function of the distributional parameters.\nThe ML method maximises the likelihood function to estimate the distributional (model) parameters.\n\nfitdistrplus default is the ML method\n\n\nlibrary(fitdistrplus)\nfitdist(rangitikei$people, \"lnorm\")\n\nFitting of the distribution ' lnorm ' by maximum likelihood \nParameters:\n        estimate Std. Error\nmeanlog 3.775514  0.1789851\nsdlog   1.028191  0.1265611\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#fitting-lognormal-1",
    "href": "slides/Chapter03.html#fitting-lognormal-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Fitting lognormal",
    "text": "Fitting lognormal\nR package fitdistrplus can be used to estimate the lognormal parameters for people data.\nThe likelihood function is based on the joint probability of the observed data as a function of the distributional parameters.\nThe ML method maximises the likelihood function to estimate the distributional (model) parameters. Examine the fit using graphical displays.\n\n\nlnormfit &lt;- fitdist(rangitikei$people, \"lnorm\")\nplot(lnormfit)\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#gamma-fit",
    "href": "slides/Chapter03.html#gamma-fit",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Gamma fit",
    "text": "Gamma fit\nGamma distribution also fits OK but the outlier remains.\nOutliers and subgroups will affect the Maximum Likelihood (ML) employed in the fitdistrplus package.\n\n\nfitdist(rangitikei$people, \"gamma\")\n\n\nFitting of the distribution ' gamma ' by maximum likelihood \nParameters:\n        estimate  Std. Error\nshape 1.14299006 0.249453814\nrate  0.01593588 0.004314727\n\n\n\n\n\ngammafit &lt;- fitdist(rangitikei$people, \"gamma\")\nplot(gammafit)\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#cullen-and-frey-plot",
    "href": "slides/Chapter03.html#cullen-and-frey-plot",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Cullen and Frey plot",
    "text": "Cullen and Frey plot\nThis is a plot of kurtosis, a measure of excessive peakedness, against the square of a measure of skew.\n\ndescdist(rangitikei$people)\n\n\nsummary statistics\n------\nmin:  4   max:  470 \nmedian:  46 \nmean:  71.72727 \nestimated sd:  86.28089 \nestimated skewness:  3.3264 \nestimated kurtosis:  17.11618 \n\n\nSymmetrical distributions such as normal (read corresponding to zero skew) are poor fits. Lognormal also does not fare well.\nWhen the observed data is a mixture from two or more distributions or contain a large number of unusual observations, no single distributional fit will be satisfactory.\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#tv-watching-time-data",
    "href": "slides/Chapter03.html#tv-watching-time-data",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "TV watching time data",
    "text": "TV watching time data\nNormal fit is supported.\n\ndescdist(tv$TELETIME)\n\n\nsummary statistics\n------\nmin:  490   max:  2799 \nmedian:  1760.5 \nmean:  1729.283 \nestimated sd:  567.913 \nestimated skewness:  -0.004088284 \nestimated kurtosis:  2.32614 \n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#which-distribution",
    "href": "slides/Chapter03.html#which-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Which distribution?",
    "text": "Which distribution?\n\nThis is a difficult task\nNot judged purely on statistical grounds\nLook for the cases in the literature; see whether any mechanistic justification is available in particular.\nLarge amount of data needed, often over 300.\nIf observed data is a mixture from 2 or more distributions, no point in fitting a single distribution.\n\nDo adequate EDA of the data\n\nBe liberal for small sample sizes but fit models based on normal and employ techniques such as the transformation method (covered later)\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter05.html#tables-and-frequencies",
    "href": "slides/Chapter05.html#tables-and-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tables and frequencies",
    "text": "Tables and frequencies\n\nMain question\n\nHow do the observed counts compare with known (assumed or expected) counts?\n\nModel\n`data = fit + error`  (or)\n\n`observed = expected + residual`\nObserved comes from sample data\nExpected comes from model or from assumption about the population"
  },
  {
    "objectID": "slides/Chapter05.html#example",
    "href": "slides/Chapter05.html#example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\nDoes the distribution of rejects of metal castings by causes in a particular week vary from the long term average counts?\n\n\n\n\nCauses of rejection\nRejects during the week\nlong term\n\n\n\n\nsand\n90\n82\n\n\nmisrun\n8\n4\n\n\nshift\n16\n10\n\n\ndrop-\n8\n6\n\n\ncorebreak\n23\n21\n\n\nbroken\n21\n20\n\n\nother\n5\n8\n\n\n\n\nLong term average counts are the expected counts.\nHow to compare observed with expected?"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-test",
    "href": "slides/Chapter05.html#goodness-of-fit-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit test",
    "text": "Goodness of fit test\n\nChi-squared test statistic (\\(\\chi ^ 2\\) = Ki Sq.)\n\n\\[\n\\chi ^{2} =\\sum _{1}^{c}\\frac{\\left({\\rm Observed-Expected}\\right)^{{\\rm 2}} }{{\\rm Expected}}  =\\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E}\n\\]\n\nIf the number of categories is \\(c\\), then the degrees of freedom is \\(c-1\\)\n\nA single df is lost due to \\(\\sum E= \\sum O\\)\n\n\\(\\chi ^ 2\\) distribution is a continuous distribution. When used for discrete data, Chi-squared approximation is very good for \\(E&gt;5\\) and often good if some expected values are as small as 1.\n\nMerge/combine categories in case of small expected counts."
  },
  {
    "objectID": "slides/Chapter05.html#example-1",
    "href": "slides/Chapter05.html#example-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nA survey of voters included 550 males and 450 females. Will you call this survey as a biased one?\n\n\n\nGender\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\nmale\n550\n500\n\\((550-500)^2/ 500=5\\)\n\n\nfemale\n450\n500\n\\((450-500)^2/ 500=5\\)\n\n\nsum\n1000\n1000\n10\n\n\n\n\ndf= (2-1)=1\nAt 5% level, the critical value is only 3.84. So the sample is a biased one."
  },
  {
    "objectID": "slides/Chapter05.html#mendels-experiment-read-sg",
    "href": "slides/Chapter05.html#mendels-experiment-read-sg",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Mendel’s experiment (Read SG)",
    "text": "Mendel’s experiment (Read SG)\n\n\nMendel discovered the principles of heredity by breeding garden peas. In one Mendel’s trials ratios of various types of peas (dihybrid-crosses) were 9:3:3:1\nThe observed results are very close to expected results. This results in a small chi squared value. Were experimental results fudged or was there a confirmation bias?"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions",
    "text": "Goodness of fit for distributions\n\nTreat class intervals as categories and obtain the actual counts\n\n\n\nThe assumed distribution gives the expected counts (E)\nPerform a goodness of fit and validate the assumed theoretical distribution\nAdjust the degrees of freedom (df) for the number of estimated parameters of the theoretical distribution\n\nFor example, assume that you have 10 class intervals and test for normal distribution, which has 2 parameters.\n\nSo the df for this test will be 10-1-2=7."
  },
  {
    "objectID": "slides/Chapter05.html#contingency-table",
    "href": "slides/Chapter05.html#contingency-table",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Contingency table",
    "text": "Contingency table\n\nGiven a two way table of frequency counts, we test whether the row and column variables are independent\nThe expected count for cell \\((i,j)\\) is given by \\(E_{ij}\\) = \\((T_i \\times T_j)/n\\) where\n\\(~~~~~T_i\\), the total for row \\(i\\);\n\\(~~~~~T_j\\), the total for column \\(j\\)\n\\(~~~~~n\\), the overall total count\nTest statistic : \\(\\chi ^{2} =\\sum _{{i=1}}^{r}\\sum _{{j=1}}^{{\\rm c}}\\frac{\\left({ O}_{{ ij}} { -E}_{{ij}} \\right)^{{ 2}} }{{ E}_{{ij}} }.\\)\ndegrees of freedom: \\((r-1)(c-1)\\)"
  },
  {
    "objectID": "slides/Chapter05.html#example-2",
    "href": "slides/Chapter05.html#example-2",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nContext: Porcine Stress Syndrome (PSS) result in pale, soft meat in pigs and under conditions of stress- death.\n\nPresence of PPS is a positive reaction to breathing halothane.\n\nSelective breeding for reducing incidence of PSS\n\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343\n\n\n\n\n\n\n\n\nchisq.test(dt)\n\n\n    Pearson's Chi-squared test\n\ndata:  dt\nX-squared = 16.433, df = 8, p-value = 0.03659"
  },
  {
    "objectID": "slides/Chapter05.html#computations",
    "href": "slides/Chapter05.html#computations",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Computations",
    "text": "Computations"
  },
  {
    "objectID": "slides/Chapter05.html#inference",
    "href": "slides/Chapter05.html#inference",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Inference",
    "text": "Inference\n\nThe tabulated Chisq with (4-1)x(2-1) = 3 d.f are\n\n7.81 at 5% level; 16.27 at 1% level.\n\n\nConclusion: There is a statistical evidence that the breed is not independent of the result of the Halothane test.\n\n\nNote that large counts in the second column (Halothane negative) lead to large expected values but low contributions to chi-squared statistic.\nThis is because of the division by the appropriate expected value. On the other hand, the small observations of first column lead to small expected values but large contributions to the \\(\\chi ^{2}\\)."
  },
  {
    "objectID": "slides/Chapter05.html#significant-cell-contribution",
    "href": "slides/Chapter05.html#significant-cell-contribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Significant cell contribution",
    "text": "Significant cell contribution\n\nCounts follow Poisson distribution for which mean = variance\nHence [^{2} = =( )^{2}. ]\nIndividual cell contribution is similar to standardized residual. Any standardized residual greater than 2 is regarded as significant.\nSo \\(2^2 = 4\\) is treated as a significant contribution to the \\(\\chi ^{2}\\) statistic."
  },
  {
    "objectID": "slides/Chapter05.html#warnings",
    "href": "slides/Chapter05.html#warnings",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Warnings",
    "text": "Warnings\n\nUse only frequency counts. Use of percentages in place of counts may lead to incorrect conclusions.\nCheck for small expected values. An expected value of less than 5 may lead to concern and a very small value of less than 1 is a warning.\nIf the chi-squared statistic is small enough to be not significant, there is no problem.\nIf chi-squared statistic is significant, check the contributions to each cell. If cells with large expected value (&gt;5) contribute a large amount to chi-squared statistic, again there is no problem.\nIf cells with expected values less than 5 lead large contributions to chi-squared statistic, the significance of the chi-squared statistic should be treated with caution."
  },
  {
    "objectID": "slides/Chapter05.html#simpsons-paradox",
    "href": "slides/Chapter05.html#simpsons-paradox",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nGroup 1\n\n\n     [,1] [,2]\n[1,]   80  120\n[2,]   30   80\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group1\nX-squared = 4.4809, df = 1, p-value = 0.03428\n\n\nGroup 2\n\n\n     [,1] [,2]\n[1,]   20   75\n[2,]   25   20\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group2\nX-squared = 15.122, df = 1, p-value = 0.0001008\n\n\nAfter amalgamation of both groups\n\n\n     [,1] [,2]\n[1,]  100  195\n[2,]   55  100\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  all\nX-squared = 0.053808, df = 1, p-value = 0.8166"
  },
  {
    "objectID": "slides/Chapter05.html#permutation-test",
    "href": "slides/Chapter05.html#permutation-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Permutation test",
    "text": "Permutation test\nThis test is done maintaining the marginal totals.\nData: Smoking Status vs. Staff  Groupings\n\n\n\nSmoking Status vis-a-vis Staff Groupings\n\n\n\nNone\nModerate\nHeavy\nTotals\n\n\n\n\nJunior employees\n18\n57\n13\n88\n\n\nJunior managers\n4\n10\n4\n18\n\n\nSecretaries\n10\n13\n2\n25\n\n\nSenior employees\n25\n22\n4\n51\n\n\nSenior managers\n4\n5\n2\n11\n\n\nTotals\n61\n107\n25\n193\n\n\n\n\n\n\n\nPermutation test\n\nset.seed(12321)\nchisq.test(tabledata, simulate.p.value = TRUE)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tabledata\nX-squared = 15.672, df = NA, p-value = 0.04948\n\n\nRegular Chi-square test\n\nchisq.test(tabledata)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#log-linear-model",
    "href": "slides/Chapter05.html#log-linear-model",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Log-linear model",
    "text": "Log-linear model\nThis model is built based on the assumption that the log of the count, which is predicted or modelled by categorical factors, is normally distributed.\nEasy to fit using the MASS package but the data must be in the tall format.\n\n\nRows: 15\nColumns: 3\n$ Smoke_status &lt;chr&gt; \"None\", \"Moderate\", \"Heavy\", \"None\", \"Moderate\", \"Heavy\",…\n$ Employ       &lt;chr&gt; \"Junior employees\", \"Junior employees\", \"Junior employees…\n$ Counts       &lt;dbl&gt; 18, 57, 13, 4, 10, 4, 10, 13, 2, 25, 22, 4, 4, 5, 2\n\n\n\nlibrary(MASS)\nMASS::loglm(Counts~ Smoke_status+Employ, smk)\n\nCall:\nMASS::loglm(formula = Counts ~ Smoke_status + Employ, data = smk)\n\nStatistics:\n                      X^2 df   P(&gt; X^2)\nLikelihood Ratio 15.52691  8 0.04967430\nPearson          15.67163  8 0.04732869\n\n\n\nIn addition to the usual Pearson Chi-square statistic, a different test statistic based on the likelihood principle is also given.\nUseful to model multi-way tables with more than two factors.\nGeneralised linear models are preferred to log-linear models where the count response can be assumed to follow Poisson. Logistic models can be built to binomial counts. These models are covered in the applied linear models course."
  },
  {
    "objectID": "slides/Chapter05.html#correspondence-analysis-not-examined-in-depth",
    "href": "slides/Chapter05.html#correspondence-analysis-not-examined-in-depth",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Correspondence Analysis | (not examined in depth)",
    "text": "Correspondence Analysis | (not examined in depth)\n\nCA is for assessing interdependence of categorical variables presented in the form of a contingency table\nData: Smoking Status vs. Staff Groupings\n\n\n\n                 None Moderate Heavy\nJunior employees   18       57    13\nJunior managers     4       10     4\nSecretaries        10       13     2\nSenior employees   25       22     4\nSenior managers     4        5     2\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#row-mass-row-profiles",
    "href": "slides/Chapter05.html#row-mass-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Row mass & row profiles",
    "text": "Row mass & row profiles\n\nrow profiles are found dividing the cell counts by the corresponding row total\nrow mass is found dividing the row totals by the grand total\n\n\n\n                  None Moderate  Heavy\nJunior employees 0.205    0.648 0.1477\nJunior managers  0.222    0.556 0.2222\nSecretaries      0.400    0.520 0.0800\nSenior employees 0.490    0.431 0.0784\nSenior managers  0.364    0.455 0.1818\nrowmass          0.316    0.554 0.1295\n\n\n\nSimilarly column profiles & column masses can be found"
  },
  {
    "objectID": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "href": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Graphical display of row profiles",
    "text": "Graphical display of row profiles\n\n\nNo clear patterns seen"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping\n\nlibrary(\"FactoMineR\")\nCA(tabledata) |&gt; summary()\n\n\n\nCall:\nCA(X = tabledata) \n\nThe chi square of independence between the two variables is equal to 15.7 (p-value =  0.0473 ).\n\nEigenvalues\n                       Dim.1   Dim.2\nVariance               0.074   0.008\n% of var.             90.570   9.430\nCumulative % of var.  90.570 100.000\n\nRows\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nJunior employees |    26.268 | -0.235 34.372  0.962 | -0.047 12.934  0.038 |\nJunior managers  |     8.784 | -0.236  7.042  0.590 |  0.197 47.082  0.410 |\nSecretaries      |     5.618 |  0.195  6.670  0.873 | -0.074  9.301  0.127 |\nSenior employees |    37.894 |  0.379 51.521  1.000 |  0.004  0.045  0.000 |\nSenior managers  |     2.636 |  0.071  0.394  0.110 |  0.203 30.638  0.890 |\n\nColumns\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nNone             |    49.186 |  0.394 66.705  0.997 |  0.020  1.688  0.003 |\nModerate         |    15.679 | -0.157 18.619  0.873 | -0.060 25.941  0.127 |\nHeavy            |    16.335 | -0.289 14.676  0.661 |  0.207 72.371  0.339 |"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping"
  },
  {
    "objectID": "slides/Chapter05.html#summary",
    "href": "slides/Chapter05.html#summary",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Summary",
    "text": "Summary\n\nGoodness of fit is for testing whether the observed counts are from the hypothesised population groups.\n\nFor \\(c\\) categories (groups), the test involves \\(c-1\\) df.\n\nContingency Table (\\(r\\) rows and \\(c\\) columns) data are tested for the independence.\n\nFor \\(r\\times c\\) cells the test involves \\((r-1)(c-1)\\) df\nshiny app http://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/\n\n\\(\\chi ^{2}\\) test works well if \\(E&gt; 5\\). Some could be as low as 1.\nDo correspondence analysis (symmetric plots) when independence is rejected.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression",
    "href": "slides/Chapter07.html#multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\nIn a simple regression, there is only one predictor. Multiple regression modelling involves many predictors.\nPerform EDA first\n\nA scatter plot matrix can show nonlinear relationships\nA correlation matrix will only show the strength of pairwise linear relationships\n\nLook for the predictors having the largest correlation with response\n\nLook for inter-correlations between the predictors and choose the one with high correlation with response variable but uncorrelated with the rest.\nTry https://shiny.massey.ac.nz/anhsmith/demos/explore.multiple.numerical.variables/"
  },
  {
    "objectID": "slides/Chapter07.html#pinetree-data-example",
    "href": "slides/Chapter07.html#pinetree-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Pinetree data example",
    "text": "Pinetree data example\n\n\n         Top Third Second First\nTop    1.000 0.917  0.955 0.972\nThird  0.917 1.000  0.947 0.908\nSecond 0.955 0.947  1.000 0.967\nFirst  0.972 0.908  0.967 1.000"
  },
  {
    "objectID": "slides/Chapter07.html#inter-relationships",
    "href": "slides/Chapter07.html#inter-relationships",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Inter-relationships",
    "text": "Inter-relationships\n\nStrong relationships revealed\n\nWe may not need all predictors\nArea effect also revealed below"
  },
  {
    "objectID": "slides/Chapter07.html#full-regression",
    "href": "slides/Chapter07.html#full-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Full regression",
    "text": "Full regression\nPlaces all of the predictors in the model\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-6.043\n0.799\n-7.56\n0.000\n\n\nThird\n0.160\n0.085\n1.88\n0.066\n\n\nSecond\n0.044\n0.137\n0.32\n0.750\n\n\nFirst\n0.605\n0.090\n6.69\n0.000\n\n\n\n\n\n\n\n\n\n\nModel summary measures\n\n\nr.squared\n0.95\n\n\nsigma\n1.23\n\n\nstatistic\n370.85\n\n\np.value\n0.00\n\n\nAIC\n201.34\n\n\nBIC\n211.81\n\n\n\n\n\n\n\n\nOld summary\n\n\n\n\nCall:\nlm(formula = Top ~ ., data = pinetree[, -1])\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.041 -0.928 -0.297  0.656  3.320 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -6.0430     0.7989   -7.56  4.0e-10 ***\nThird         0.1595     0.0851    1.88    0.066 .  \nSecond        0.0437     0.1365    0.32    0.750    \nFirst         0.6047     0.0904    6.69  1.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.23 on 56 degrees of freedom\nMultiple R-squared:  0.952, Adjusted R-squared:  0.95 \nF-statistic:  371 on 3 and 56 DF,  p-value: &lt;2e-16\n\n\n\nAlso try the app - https://shiny.massey.ac.nz/anhsmith/demos/fit.multiple.regression/"
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity",
    "href": "slides/Chapter07.html#multicollinearity",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity is where at least two predictor variables are highly correlated.\nMulticollinearity does not affect the residual SD very much, and doesn’t pose a major problem for prediction.\nThe major effects of multicolinearity are:\n\nIt changes the estimates of the coefficients.\nIt inflates the variance of the estimates of the coefficients. That is, it increases the uncertainty about what the slope parameters are.\nTherefore, it matters when testing hypotheses about the effects of specific predictors.\n\nThe impact of multicollinearity on the variance of the estimates can be quantified using the Variance Inflation Factor (VIF &lt; 5 is considered ok).\nThere are several ways to deal with multicollinarity, depending on context. We can discard one of highly correlated variable, perform ridge regression, or think more carefully about how the variables relate to each other.\n\n\n\n Third Second  First \n  9.73  26.18  15.51 \n\n\n\nTry https://shiny.massey.ac.nz/anhsmith/demos/explore.collinearity/"
  },
  {
    "objectID": "slides/Chapter07.html#additional-variation-explained",
    "href": "slides/Chapter07.html#additional-variation-explained",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Additional variation explained",
    "text": "Additional variation explained\n\nVariation in \\(Y\\) is separated into two parts SSR and SSE.\n\nThe shaded overlap of two circles represent the variation in \\(Y\\) explained by the \\(X\\) variables.\n\nThe total overlap of \\(X_1\\) and \\(X_2\\), and \\(Y\\) depends on\n\nrelationship of \\(Y\\) with \\(X_1\\) and \\(X_2\\)\ncorrelation between \\(X_1\\) and \\(X_2\\)"
  },
  {
    "objectID": "slides/Chapter07.html#sequential-addition-of-predictors",
    "href": "slides/Chapter07.html#sequential-addition-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Sequential addition of predictors",
    "text": "Sequential addition of predictors\n\nAddition of variables decreases SSE and increases SSR and \\(R^2\\).\n\\(s^2\\) = MSE = SSE/df decreases to a minimum and then increases since addition of variable decreases SSE but adds to df."
  },
  {
    "objectID": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "href": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Significance of Type I or Seq.SS",
    "text": "Significance of Type I or Seq.SS\n\nThe Type I SS is the SS of a predictor after adjusting for the effects of the preceding predictors in the model.\nF test for the significance of the additional variation explained\n\n\n\nFor pinetree data, Second (middle circumference) does not explain significant additional variation after First (bottom circumference)\n\n\n\nAnalysis of Variance Table\n\nModel 1: Top ~ First\nModel 2: Top ~ First + Second\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)  \n1     58 96.6                           \n2     57 90.6  1      6.01 3.78  0.057 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter07.html#type-ii-and-or-type-iii-ss",
    "href": "slides/Chapter07.html#type-ii-and-or-type-iii-ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type II and or Type III SS",
    "text": "Type II and or Type III SS\n\nR function anova() calculates sequential or Type-I SS\nType II SS is based on the principle of marginality.\n\nEach variable effect is adjusted for all other appropriate effects.\n\nequivalent to the Type I SS when the variable is the last predictor entered the model.\n\nOrder matters for Type I SS but not for Type II SS\n\n\n\n\nType III SS is the SS added to the regression SS after ALL other predictors including an intercept term.\n\nViolates the marginality principle and so avoided for hypothesis tests\n\n\n\n\nAnalysis of Variance Table\n\nResponse: Top\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nThird      1   1494    1494   981.7 &lt; 2e-16 ***\nSecond     1    131     131    86.1 6.4e-13 ***\nFirst      1     68      68    44.8 1.1e-08 ***\nResiduals 56     85       2                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnova Table (Type II tests)\n\nResponse: Top\n          Sum Sq Df F value  Pr(&gt;F)    \nThird        5.4  1    3.52   0.066 .  \nSecond       0.2  1    0.10   0.750    \nFirst       68.1  1   44.76 1.1e-08 ***\nResiduals   85.2 56                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnova Table (Type III tests)\n\nResponse: Top\n            Sum Sq Df F value  Pr(&gt;F)    \n(Intercept)   87.1  1   57.22 4.0e-10 ***\nThird          5.4  1    3.52   0.066 .  \nSecond         0.2  1    0.10   0.750    \nFirst         68.1  1   44.76 1.1e-08 ***\nResiduals     85.2 56                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSS explained is not always a good criterion for selection of variables"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection",
    "href": "slides/Chapter07.html#model-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection",
    "text": "Model selection\n\n\\(R^2\\) (the proportion of variance explained by the model) should not be used to select among candidate models. Adding another predictor variable always increases the \\(R^2\\), even if the new variable is unrelated in the population.\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\n\nTry https://shiny.massey.ac.nz/anhsmith/demos/demo.RSq.and.adjusted.RSq/\n\nResidual SD depends on its degrees of freedom\n\nSo comparison of models based on Residual SD is not fully fair\n\nOther summaries similar to residual SD include\nMean Squared Deviation (MSD)\n\n\\[\\frac{\\sum \\left({\\rm observation-fit}\\right)^{{\\rm 2}} }{{\\rm number~of~ observations}}\\]\n\nMean Absolute Deviation (MAD)\n\n\\[\\frac{\\sum \\left|{\\rm observation-fit}\\right| }{{\\rm number~of~observations}}\\]\n\nMean Absolute Percentage Error (MAPE)\n\n\\[\\frac{\\sum \\frac{\\left|{\\rm observation-fit}\\right|}{{\\rm observation}} }{{\\rm number~of~observations}} {\\times100}\\]"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-continued",
    "href": "slides/Chapter07.html#model-selection-continued",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection (continued)",
    "text": "Model selection (continued)\n\nAvoid over-fitting.\n\nTry https://shiny.massey.ac.nz/anhsmith/demos/demo.over-fitting.model/\n\nSo place a penalty for excessive model parameters\nAkaike Information Criterion (AIC; smaller is better)\n\n\\[AIC  =  n\\log \\left(\\frac{SSE}{n} \\right) + 2p\\]\n\nWe can also benchmark a small model with the full regression\nMallow’s \\(C_p\\) (look for \\(C_p\\) just less than \\(p\\) or equal)\n\n\\[C_{p} =\\; \\frac{{\\rm SS\\; Error\\; for\\; Smaller\\; Model}}{{\\rm Mean\\; Square\\; Error\\; for\\; full\\; regression}} -(n-2p)\\]"
  },
  {
    "objectID": "slides/Chapter07.html#selection-of-predictors",
    "href": "slides/Chapter07.html#selection-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Selection of predictors",
    "text": "Selection of predictors\n\nHeuristic (short-cut) procedures based on criteria such as \\(F\\), \\(R^2_{adj}\\), \\(AIC\\), \\(C_p\\) etc\n\nForward Selection: Add variables sequentially\n\nconvenient to obtain the simplest feasible model\n\nBackward Elimination: Drop variables sequentially\n\nIf difference between two variables is significant but not the variables themselves, forward regression would obtain the wrong model since both may not enter the model.\n\nKnown as suppressor variables case\n\n\n\n\nExample: (try)\n\nBest Subsets: Stop at each step and check whether predictors, in the model or outside, are the best combination for that step.\n\ntime consuming to perform when the predictor set is large"
  },
  {
    "objectID": "slides/Chapter07.html#software",
    "href": "slides/Chapter07.html#software",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Software",
    "text": "Software\n\nIn \\(R\\), lm() and step() function will perform the tasks\n\nleaps() and HH packages contain additional functions\nAlso MASS, car, caret, and SignifReg R packages\n\nR base package step-wise selection is based on \\(AIC\\) only.\nNote\n\nIf a model stands out, it will perform well in terms of all summary measures.\nIf a model does not stand out, summary measures will contradict.\n\n\nExamples:\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.multiple.regression/"
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection",
    "href": "slides/Chapter07.html#cross-validated-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection",
    "text": "Cross validated selection\n\nModel selection can be done focussing on prediction\n\nmethod = “leapForward” & method = “leapBackward” options\n\n\n\n\nSubset selection object\n3 Variables  (and intercept)\n       Forced in Forced out\nThird      FALSE      FALSE\nSecond     FALSE      FALSE\nFirst      FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         Third Second First\n1  ( 1 ) \" \"   \" \"    \"*\"  \n2  ( 1 ) \"*\"   \" \"    \"*\""
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models",
    "href": "slides/Chapter07.html#polynomial-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nA polynomial model includes the square, cube of predictor variables as additional variables.\nHigh correlation (multicollinearity) between the predictor variables may be a problem in polynomial models.\n\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                         44.121      7.039    6.27        0\npoly(First, degree = 3, raw = T)1   -3.972      0.695   -5.71        0\npoly(First, degree = 3, raw = T)2    0.142      0.022    6.39        0\npoly(First, degree = 3, raw = T)3   -0.001      0.000   -5.95        0\n\n\n\nMulticollinearity can occur with polynomial modelling but not always\n- For the pinetree example, all the slope coefficients are highly significant for the cubic regression\n- Not so for the quadratic regression\n\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                           3.85      2.450   1.569    0.122\npoly(First, degree = 2, raw = T)1     0.10      0.155   0.646    0.521\npoly(First, degree = 2, raw = T)2     0.01      0.002   4.319    0.000\n\n\n\nRaw polynomials do not preserve the coefficient estimates but orthogonal polynomials do.\n\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.146  119.26        0\npoly(First, degree = 2)1    41.01      1.130   36.29        0\npoly(First, degree = 2)2     4.88      1.130    4.32        0\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.115  151.03        0\npoly(First, degree = 3)1    41.01      0.892   45.96        0\npoly(First, degree = 3)2     4.88      0.892    5.47        0\npoly(First, degree = 3)3    -5.31      0.892   -5.95        0"
  },
  {
    "objectID": "slides/Chapter07.html#residual-diagnostics",
    "href": "slides/Chapter07.html#residual-diagnostics",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nFor multiple regression fits, including polynomial fits, examine the residuals as usual to-\n- Validate the model assumptions\n- Look for model improvement clues\nQuadratic regression for pinetree data is not satisfactory based on the residual plots shown below:"
  },
  {
    "objectID": "slides/Chapter07.html#local-polynomial-ﬁtting-or-spline-smoothing.",
    "href": "slides/Chapter07.html#local-polynomial-ﬁtting-or-spline-smoothing.",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Local polynomial ﬁtting or spline smoothing.",
    "text": "Local polynomial ﬁtting or spline smoothing.\n\nInstead of a single polynomial for all the data, local polynomials or splines can also be fitted.\nFor pinetree data, compare the cubic fit with the fifth degree spline fit\n\nvery similar fits"
  },
  {
    "objectID": "slides/Chapter07.html#robust-modelling",
    "href": "slides/Chapter07.html#robust-modelling",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Robust modelling",
    "text": "Robust modelling\n\nInstead of the regression fit using the lm() function, we can also fit robust models\n\nWe covered MASS::rlm() & robustbase::lmrob() functions in the last chapter\nRobust modelling syntax is similar for employing many predictors.\nPinetree example\n\n\n\n\n                         Value Std. Error t value\n(Intercept)              17.40      0.125  139.54\npoly(First, degree = 3)1 40.93      0.966   42.37\npoly(First, degree = 3)2  4.76      0.966    4.93\npoly(First, degree = 3)3 -5.17      0.966   -5.36"
  },
  {
    "objectID": "slides/Chapter07.html#comparison-of-regression-and-robust-fits",
    "href": "slides/Chapter07.html#comparison-of-regression-and-robust-fits",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Comparison of regression and robust fits",
    "text": "Comparison of regression and robust fits\n\n\nother summaries such as MAPE, MAD can also be obtained\n\n\n\n[1] 0.0629 0.0598 0.0602\n\n\n\nTry for the polynomial fit Top~poly(First,degree=3) which is a better model structure for the pinetree data."
  },
  {
    "objectID": "slides/Chapter07.html#categorical-predictors",
    "href": "slides/Chapter07.html#categorical-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nModels can include categorical predictors such as Area in the pinetree dataset\nMake sure that you use the factor() function when numerical codes are assigned to categorical variables.\nArea effect on Top circumference is clear from the following plot"
  },
  {
    "objectID": "slides/Chapter07.html#indicator-variables",
    "href": "slides/Chapter07.html#indicator-variables",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nFactors are employed in a multiple regression using indicator variables which are simply binary variables taking either zero or one\nFor for males and females, indicator variables are defined as follows:\n\nIndicator variable of males: \\(~~~~~~~~\\begin{array}{cccc} I_{\\text {male}} & = & 1 & \\text{for males}\\\\ & & 0& \\text{for females} \\end{array}\\)\nIndicator variable of females \\(~~~~~~~~\\begin{array}{cccc} I_{\\text{female}} & = & 1 & \\text{for females}\\\\ & & 0& \\text{for males} \\end{array}\\)\n\nThere are three different areas of the forest in the pinetree dataset. So we can define three indicator variables.\nOnly two indicator variables are needed because there is only 2 degrees of freedom for the 3 areas."
  },
  {
    "objectID": "slides/Chapter07.html#regression-output",
    "href": "slides/Chapter07.html#regression-output",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Regression output",
    "text": "Regression output\n\n\n\nRegression of Top Circumference on Area Indicator Variables\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.02\n1.11\n17.98\n0.00\n\n\nI2\n-1.96\n1.57\n-1.24\n0.22\n\n\nI3\n-5.92\n1.57\n-3.76\n0.00\n\n\n\n\n\n\n\n\nThe y-intercept is the mean of the response for the omitted category\n\n20.02 is the mean Top circumference for the first Area\n\nslopes are the difference in the mean response\n\n-1.96 is the drop in the mean top circumference in Area 2 when compared to Area 1 (which is not a significant drop)\n-5.92 is the drop in the mean top circumference in Area 3 when compared to Area 1 (which is a highly significant drop)\n\n\nAnalysis of Covariance model employs both numerical and categorical predictors (covered later on).\n\nWe specifically include the interaction between them"
  },
  {
    "objectID": "slides/Chapter07.html#permutation-tests",
    "href": "slides/Chapter07.html#permutation-tests",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Permutation tests",
    "text": "Permutation tests\nR package lmPerm function lmp() will obtain the P values by permuting the predictor data.\n\nlibrary(lmPerm)\nmdl  &lt;-  lmp(Top~I2+I3,  data=pinetree1)\n\n[1] \"Settings:  unique SS : numeric variables centered\"\n\nsummary(mdl)\n\n\nCall:\nlmp(formula = Top ~ I2 + I3, data = pinetree1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.92  -3.10  -1.01   3.68  11.93 \n\nCoefficients:\n   Estimate Iter Pr(Prob)   \nI2    -1.96  110   0.4818   \nI3    -5.93 5000   0.0048 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.98 on 57 degrees of freedom\nMultiple R-Squared: 0.205,  Adjusted R-squared: 0.177 \nF-statistic: 7.35 on 2 and 57 DF,  p-value: 0.00145 \n\n\nAlso try all other cases covered just replacing the lm() command with lmp() command."
  },
  {
    "objectID": "slides/Chapter07.html#time-series-smoothing",
    "href": "slides/Chapter07.html#time-series-smoothing",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Time series smoothing",
    "text": "Time series smoothing\n\nThis is a technique to remove the random variation but retain any trend and cyclic type of variations in a time series.\n\nMoving Average Smoothing\n\nExponential (Average) Smoothing\n\nMoving average smoothing\n\nCompute the mean of successive smaller periods of past data (moving window).\n\n\n\\[M_t = (x_t + x_{t-1} + ... + x_{t-N+1}) / N\\] where - \\(x_t\\)- the observation at time \\(t\\) & \\(N\\) - the moving average length/span\n\nLonger the span \\(N\\), greater the smoothing"
  },
  {
    "objectID": "slides/Chapter07.html#ma-smoothing-for-20-bills-data",
    "href": "slides/Chapter07.html#ma-smoothing-for-20-bills-data",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "MA smoothing for $20 bills data",
    "text": "MA smoothing for $20 bills data"
  },
  {
    "objectID": "slides/Chapter07.html#ma-centering",
    "href": "slides/Chapter07.html#ma-centering",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "MA Centering",
    "text": "MA Centering\n\nWe need to place moving averages in the middle time period.\n- moving average must fall at t = 1.5 2.5, 3.5 etc when the MA length is an even number.  \nSo, smooth again (in pairs) to place the moving averages at t = 2, 3, 4 etc."
  },
  {
    "objectID": "slides/Chapter07.html#exponential-average-smoothing",
    "href": "slides/Chapter07.html#exponential-average-smoothing",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Exponential (Average) Smoothing",
    "text": "Exponential (Average) Smoothing\n\nIn exponential average smoothing, past observations are given exponentially decreasing weights.\nThe average computed using exponentially decreasing weights is known as the Exponentially Weighted Moving average (EWMA).\n\nEWMA definition: \\[S_t = \\alpha x_t + (1-\\alpha) S_{t-1}\\] where \\(S_t\\) = EWMA at time \\(t\\), \\(x_t\\) = value of time series at time \\(t\\) & \\(\\alpha\\) = weighting factor \\((0 &lt; \\alpha &lt; 1)\\)\nEWMA for $20 bills data"
  },
  {
    "objectID": "slides/Chapter07.html#double-triple-exponential-smoothing",
    "href": "slides/Chapter07.html#double-triple-exponential-smoothing",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Double & triple exponential smoothing",
    "text": "Double & triple exponential smoothing\n\nDouble Smoothing (includes trend)\n\\(S_t = \\alpha X_t+(1-\\alpha)[S_{t-1} + T_{t-1}]\\) (level equation)\n\\(T_t = \\gamma [S_t - S_{t-1}] + (1 - \\gamma)T_{t-1}\\) (trend equation)\n\\(\\hat{Y}= S_{t-1}+T_{t-1}\\)\n\nTriple Smoothing (includes trend, seasonal)\nAlso called Holt and Winter method\n\\(S_t = \\alpha(X_t-P_{t-p}) + (1-\\alpha)[S_{t-1}+T_{t-1}]\\) (level equation)\n\\(T_t = \\gamma[S_t-S_{t-1}]+ (1-\\gamma)T_{t-1}\\) (trend equation)\n\\(P_t = \\delta (X_t-S_t)+(1-\\delta)P_{t-p}\\) (seasonal equation at period \\(p\\))\n\\(\\hat{Y}= S_{t-1}+T_{t-1}+P_{t-p}\\)"
  },
  {
    "objectID": "slides/Chapter07.html#double-triple-exponential-smoothing-1",
    "href": "slides/Chapter07.html#double-triple-exponential-smoothing-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Double & triple exponential smoothing",
    "text": "Double & triple exponential smoothing"
  },
  {
    "objectID": "slides/Chapter07.html#forecasting",
    "href": "slides/Chapter07.html#forecasting",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Forecasting",
    "text": "Forecasting\n\nProjecting the present time series for future time points (Prediction)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssessment of fit | Forecast accuracy using\n\nMean Squared Deviation (MSD)\nMean Absolute Deviation (MAD)\nMean Absolute Percentage Error (MAPE)\n\n\n\n\nRMSE  MAE MAPE \n81.1 59.2 14.2 \n\n\n\n\nRMSE  MAE MAPE \n64.9 48.7 10.1"
  },
  {
    "objectID": "slides/Chapter07.html#time-series-modelling",
    "href": "slides/Chapter07.html#time-series-modelling",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Time series modelling",
    "text": "Time series modelling\n\nWe can fit linear models to time series data including trend and seasonality components.\nIndicator variables to capture seasonality\n\nExamples:"
  },
  {
    "objectID": "slides/Chapter07.html#box-jenkins-modelling",
    "href": "slides/Chapter07.html#box-jenkins-modelling",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Box-Jenkins modelling",
    "text": "Box-Jenkins modelling\n\nA Box-Jenkins model is an improvement over the regression approach - combines regression and moving average approaches - suitable when data collected at equally spread time intervals\nBox-Jenkins approach consists of the following three stages:\n\nIdentification\nEstimation\nDiagnostics\n\nPast data or lagged data (or simply lags) play an important role"
  },
  {
    "objectID": "slides/Chapter07.html#identification-stage-of-modelling",
    "href": "slides/Chapter07.html#identification-stage-of-modelling",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Identification stage of modelling",
    "text": "Identification stage of modelling\n\nA time series is stationary if the mean and variance do not change over time.\n\nAlternatively the same probability law applies over time\nFor a white noise series has a constant mean and variance for all time \\(t\\).\n\nStationarity is investigated using the sample autocorrelation function (ACF) (covered in Chapter 2)\nUsing ACFs, a tentative assessment of the terms or order of the model is made."
  },
  {
    "objectID": "slides/Chapter07.html#common-patterns",
    "href": "slides/Chapter07.html#common-patterns",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Common patterns",
    "text": "Common patterns\n\nDrifting random walk series (non-stationary)\n\n\\[X_t = \\delta + X_{t-1} + W_t \\]\n\n\nModel the difference \\(X_{t} - X_{t-1}\\) or just use the first lag \\(X_{t-1}\\) as a predictor in the usual regression\nThe process of differencing can be done on the differences too. This process brings stationarity to a series"
  },
  {
    "objectID": "slides/Chapter07.html#auto-regressive-ar-model",
    "href": "slides/Chapter07.html#auto-regressive-ar-model",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Auto-regressive (AR) model",
    "text": "Auto-regressive (AR) model\n\\[X_t=\\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2}+ \\dots + \\alpha_p X_{t-p} + \\epsilon_t\\]\n\nAlso called the \\(AR(p)\\) process.\nAn AR model is approximately the same as the multiple regression fit.\n\nIf sample mean replaces the true mean \\(\\mu\\) of the process, the AR model becomes the multiple regression model with lags as predictors.\n\nmoving average process for errors\n\n\\[X_t=\\beta_0 z_{t}+ \\beta_1 z_{t-1}+ \\dots + \\beta_q z_{t-q}\\]\n\n\\(X_t\\) is modelled with errors \\(z_1\\), \\(z_2\\),…, whose means are assumed to be zero and constant variance."
  },
  {
    "objectID": "slides/Chapter07.html#autoregressive-moving-average-arma-model",
    "href": "slides/Chapter07.html#autoregressive-moving-average-arma-model",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Autoregressive moving average (ARMA) model",
    "text": "Autoregressive moving average (ARMA) model\n\n\\(ARMA(p, q)\\) model combines both the \\(AR(p)\\) and \\(MA(q)\\) models.\n\n\\[X_t=\\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2}+ \\dots + \\alpha_p X_{t-p} + \\beta_o z_{t}+ \\beta_1 z_{t-1}+ \\dots + \\beta_q z_{t-q}\\]\n\nEstimation of parameters\n\ndone using advanced (non-linear) iterative methods\n\nUsual Diagnostics guidelines apply\n\nexamine residuals for randomness\nmay indicate transformation\n\nFor nonstationary series, we perform differencing and then do the modelling\nARIMA (p,d,q) (P,D,Q) Models\n\nincludes ARMA (Stationary) Models\n\n\\(p\\), number of lag terms\n\\(q\\), number of successive error terms (moving average part)\n\\(d\\), number of differences in model (to deal with non-stationarity)\n\\(P\\),\\(D\\),\\(Q\\) - corresponding parameters for the seasonal part\n\n\n\n(will not be examined)"
  },
  {
    "objectID": "slides/Chapter07.html#automated-arima-modelling",
    "href": "slides/Chapter07.html#automated-arima-modelling",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Automated ARIMA modelling",
    "text": "Automated ARIMA modelling\n\n\nforecast R package has functions such as auto.arima for speedy ARIMA modelling\n\nmilk data example\n\n\n\n\nSeries: Y \nARIMA(1,1,4) \n\nCoefficients:\n         ar1    ma1    ma2     ma3     ma4\n      -0.305  0.246  0.150  -0.426  -0.649\ns.e.   0.116  0.082  0.055   0.049   0.061\n\nsigma^2 = 1380:  log likelihood = -840\nAIC=1692   AICc=1692   BIC=1710\n\n\n\nResidual diagnostics\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,4)\nQ* = 47, df = 5, p-value = 5e-09\n\nModel df: 5.   Total lags used: 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals are unsatisfactory - the 12-month lag shows that seasonality is not captured."
  },
  {
    "objectID": "slides/Chapter07.html#improved-model",
    "href": "slides/Chapter07.html#improved-model",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Improved model",
    "text": "Improved model\n\n\nauto-ARIMA with 12-month seasonality\n\n\n\nSeries: Y \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n         ma1    sma1\n      -0.220  -0.621\ns.e.   0.075   0.063\n\nsigma^2 = 53.4:  log likelihood = -530\nAIC=1066   AICc=1066   BIC=1075\n\n\n\nResidual diagnosis\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)(0,1,1)[12]\nQ* = 17, df = 22, p-value = 0.7\n\nModel df: 2.   Total lags used: 24\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals are better now!"
  },
  {
    "objectID": "slides/Chapter07.html#improved-model-1",
    "href": "slides/Chapter07.html#improved-model-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Improved model",
    "text": "Improved model"
  },
  {
    "objectID": "slides/Chapter07.html#summary",
    "href": "slides/Chapter07.html#summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary",
    "text": "Summary\n\nRegression methods aim to fit a model by least squares to explain the variation in the dependent variable \\(Y\\) by fitting explanatory \\(X\\) variables.\nMatrix plots and correlation coefficients provide important clues to the interrelationships.\nFor building a model, the additional variation explained is important. Summary criterion such as \\(AIC\\) is also useful.\nANCOVA employs both numerical variables (covariates) and qualitative factors for modelling.\nA time series data involve trend and seasonal variations.\nSimple smoothing methods are intended to dampen the effect of random errors\n\nDouble and triple exponential smoothing deal trends and seasonality\n\nSimple AR models can be built as multiple regression models employing lags as predictors.\n\nBuilding ARIMA models are more involved and will not be examined in depth.\n\nA model is not judged as the best purely on statistical grounds."
  },
  {
    "objectID": "slides/Chapter07.html#shiny-apps",
    "href": "slides/Chapter07.html#shiny-apps",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Shiny apps",
    "text": "Shiny apps\nhttp://shiny.massey.ac.nz/anhsmith/demos/explore.multiple.regression/\nhttp://shiny.massey.ac.nz/anhsmith/demos/fit.multiple.regression/\nhttp://shiny.massey.ac.nz/anhsmith/demos/explore.collinearity/\nhttp://shiny.massey.ac.nz/anhsmith/demos/explore.Cooks.distances.and.leverages/\nhttp://shiny.massey.ac.nz/anhsmith/demos/demo.RSq.and.adjusted.RSq/\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "161250 Slides",
    "section": "",
    "text": "Chapter 1:Data Collection\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2:Exploratory Data Analysis (EDA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3:Probability Concepts & Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4:Introduction to Statistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5:Tabulated Counts\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6:Models with a Single Predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7:Models with Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studyguide/2-eda.html",
    "href": "studyguide/2-eda.html",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "“Data has no meaning apart from its context.” — Walter A Shewhart"
  },
  {
    "objectID": "studyguide/2-eda.html#error-checking",
    "href": "studyguide/2-eda.html#error-checking",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Error checking",
    "text": "Error checking\nThe data are assumed to be in some sort of electronic form; e.g. text file, spreadsheet, database, etc. Before analysing the data, it is prudent to check for mistakes in the data. If one has access to the raw data (e.g. returned questionnaires), one can check observations by hand. If the raw data are not available, then the best one can do is to look for observations whose values are out of allowable ranges, or are very suspicious. A common problem occurs with the coding of missing information. Missing values are commonly recorded as a blank, *, -1, -999, or NA. Only one type of coding should be used. The use of NA is preferred since this is easier to spot. The use of blanks is particularly dangerous when one is exporting data from a spreadsheet to a text file. When the data from the text file is read into a computer package, a missing value may be skipped and the value from the next observation read in its place. When the data set is large, looking at a data set one observation at a time may not be feasible. In this case, plotting the data may show any observations with illegal values."
  },
  {
    "objectID": "studyguide/2-eda.html#understanding-data",
    "href": "studyguide/2-eda.html#understanding-data",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Understanding data",
    "text": "Understanding data\nAfter any data errors have been found and corrected, the data understanding stage commences. This stage involves tabulating, summarising, and plotting the data in many ways to gain insight into the data set. We typically want to:\n\nexamine the distributions, or shapes, of individual variables;\ncompare different groups of data, with special emphasis on location and scale;\nand discover any trends and relationships exhibited between pairs of variables.\n\nAgain, don’t fall into the trap of data snooping, where you develop and test hypotheses with the same data. For instance, suppose that boxplots of the income of individuals in different age groups show that youngest and oldest age groups are the most different. If one elects to perform the hypothesis test for the difference between these two groups based on observations made during EDA, the resulting \\(p\\)-value is not valid.\nIn the sections below, we briefly review many common tools used to visualise data during EDA."
  },
  {
    "objectID": "studyguide/2-eda.html#bar-chart",
    "href": "studyguide/2-eda.html#bar-chart",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Bar chart",
    "text": "Bar chart\nThe rules for constructing a bar chart are very simple and perhaps they can be reduced to a single rule and that is to make the height of each bar proportional to the quantity displayed. Consider the dataset Snodgrass available in the R package archdata. This dataset contains information on the size, location and contents of 91 house pits at the Snodgrass site which was occupied between about CE 1325-1420. The variable names and their description are given below:\n\nEast: East grid location of house in feet (excavation grid system)\nSouth: East grid location of house in feet (excavation grid system)\nLength: House length in feet\nWidth: House width in feet\n\nSegment: Three areas within the site 1, 2, 3\n\nInside: Location within or outside the “white wall” Inside, Outside\n\nArea: Area in square feet\n\nPoints: Number of projectile points\n\nAbraders: Number of abraders\n\nDiscs: Number of discs\n\nEarplugs: Number of earplugs\n\nEffigies: Number of effigies\n\nCeramics: Number of ceramics\n\nTotal: Total Number of artifacts listed above\n\nTypes: Number of kinds of artifacts listed above\n\nThe data from 91 house pits at the Snodgrass site were reported by Price and Giffin in 1979. The layout of the houses follows a grid pattern with the long axis oriented north-east surrounded by a fortification trench. There is also evidence of an interior wall that may have separated the houses inside that wall from those outside the wall. Price and Griffin use differences in house size and artifacts composition to suggest that those distinctions may have reflected rank differences between the occupants of the two areas.\nThe distribution of number of Ceramics found can be displayed in the form of a bar chart; see Figure 1.\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\n\n\nCode\ndata(Snodgrass, package = \"archdata\")\n\nSnodgrass |&gt; \n  ggplot() + \n  aes(x = Ceramics) + \n  geom_bar()\n\n\n\n\n\nFigure 1: Bar Chart of Ceramics counts\n\n\n\n\nThe bar graph illustrates clearly that no ceramics were found in many houses but a lot were found in a few houses. This may be due to status and wealth of the occupants.\nNote that we are treating number of Ceramics as ordinal in the above graph. For nominal data software tend to order the categories alphabetically.\nHowever, a graph is not always the best way to display a small set of numbers. Graphs do have some disadvantages: they take up a lot of space, and they do not usually allow the recovery of the exact observations. For these reasons, particularly for a small data sets, a table is sometimes more effective. There were only three segment categories; and their counts can be simply displayed as a table.\n\n\nCode\ntable(Snodgrass$Segment)\n\n\n\n 1  2  3 \n38 28 25 \n\n\nIt would be interesting to see whether the discovery of ceramics is Segment dependent. In other words, we would like to explore two factors in display. This can be done in many ways (Figure 2).\n\n\nCode\ndata(Snodgrass, package = \"archdata\")\n\np1 &lt;- ggplot(Snodgrass) +\n  aes(x = Ceramics, fill = Segment) + \n  geom_bar() + \n  ggtitle(\"Bar plot with colour grouping\")\n\np2 &lt;- ggplot(Snodgrass) + \n  aes(x = Ceramics) + \n  geom_bar() + \n  facet_grid(vars(Segment)) + \n  ggtitle(\"Segment-wise Bar plots\")\n\np3 &lt;- ggplot(Snodgrass) +\n  aes(x = Ceramics, fill = Segment) +\n  geom_bar(position = \"dodge\") + \n  ggtitle(\"Clustered Barplot\")\n\np4 &lt;- ggplot(Snodgrass) +\n  aes(x=Ceramics, fill = Segment) + \n  geom_bar(position = \"dodge\") + \n  ggtitle(\"Clustered Bar plot - flipped\") +\n  scale_fill_grey() + coord_flip()\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2)\n\n\n\n\n\nFigure 2: Bar Chart of Ceramics counts by Segment\n\n\n\n\nIt is easy to spot that the Ceramics findings were largely from the first segment. This EDA conclusion can be drawn from all of the above plots. However, these bar plots do not rate the same as a “presentation style graphic”.\nTufte (2001) argues in favour of certain principles of graphical integrity for presentation style graphics. These are:\n\nThe representation of numbers in a graph should be directly proportional to the numbers themselves;\nClear and detailed labels should be used to prevent ambiguity;\nProvide explanations and important events pertaining to data on the graphic itself;\nThe graph should display data variation, not design variation—visual changes in the graph should be due to the data, not changes in way the data are drawn;\nThe number of dimensions used in the graph should not exceed the number of dimensions in the data;\nGraphs must not quote data out of context; for example, a monthly drop in the murder rate of a city should be plotted within the context of the murder rate over the last few years, or murder rates in comparable cities over the same period.\n\nA final point concerns the use of colour. In many sorts of graphs, data types are differentiated by the use of colour. This practice is perfectly legitimate, but one should be aware that a surprising proportion of people have impaired colour perception, such as being unable to distinguish between red and green (see here for colourblind-friendly pallettes). A remedy to these problems is to use different plotting symbols, line types, or shading, in conjunction with different colours. One should always make sure that the colours selected are clearly visible in the medium in which they are presented. Colours on a computer monitor can appear quite different when projected onto a screen in a lecture theatre, or when printed onto paper.\nAccording to Tufte (2001), a good graphical display should:\n\nDisplay the data;\nEncourage the viewer to concentrate on the substance of the data, in the context of the problem, not on the artistic merits of the graph;\nAvoid distorting, intentionally or unintentionally, what the data have to say;\nHave a high data to ink ratio; that is, convey the greatest amount of information in the shortest time with the least ink in the smallest space;\nEnable one to easily compare different pieces of data;\nReveal overall trends and patterns in the data that are not obvious from viewing the raw numbers;\nReveal detailed information secondary to the main trend; for example, the location of outliers, variability of the response.\n\nThis Chapter is largely concerned with EDA for discovery of patterns and peculiarities in a dataset. So we will not be too worried if the graphs produced are not aesthetically pleasing or does not meet a particular standard for a presentation style graph.\nThe end goal of most analyses is of course some form of written report or presentation. In either case, the findings of the study, should be supported by tables and graphs created during the EDA; these should clarify and support the final conclusions, not deceive or confuse the audience. One should refrain from showing all tables and plots that were created, but rather a careful selection that reinforces the main findings. The remaining figures can, if necessary, be placed in an appendix of the written report. The figures and tables that are shown should be well labelled and annotated. Abbreviations and cryptic variable names should be replaced by sensible labels and descriptions. The emphasis should be on conveying information rather than producing an eye catching plot. Special care should be taken to ensure that the table or graph cannot be easily misinterpreted.\nIn addition, a poorly made graph can be deceptive. This is a problem both in the understanding the data phase, and the presenting the data phase. One common mistake (but by no means the only one) in creating a graph is to use more dimensions to represent the data than there are data dimensions. For example, it is not uncommon to see a bar chart where the bars are drawn as three dimensional rectangular prisms. The false perspective actually obscures the differences in the heights of the bars.\nAnother pitfall is to use a clever graphic rather than one that clearly presents the data. The classic example of this is a pictogram, where pictures of varying size represent some quantities. Pictograms are often drawn dishonestly: to show a two-fold difference, each side of the picture is increased by a factor of two, so the area is in fact increased by a factor of four. Dishonesty in data presentation is clearly unacceptable. But even an honest pictogram is usually not the best way to present data. It is difficult to accurately judge differences between areas; which is what a pictogram asks the viewer to do."
  },
  {
    "objectID": "studyguide/2-eda.html#pie-charts",
    "href": "studyguide/2-eda.html#pie-charts",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Pie charts",
    "text": "Pie charts\nPie charts are a popular, though usually rather poor, way of presenting data. Like bar charts, they frequently use a large figure to represent just a few numbers. Pie charts also have a poor accuracy of decoding. That means people’s impression of the difference in size between two pie slices is often quite different from the actual difference in size. Pie charts use angles to represent percentages; people tend to underestimate the size of acute angles and overestimate the size of obtuse angles (Stevens, 1957). The exploded pie chart makes it more difficult to accurately compare pie slices: the exploded piece always appears bigger. The three dimensional pie chart is even worse. Bar charts, dot charts, or tables should be preferred to pie charts.\nCount summaries of two categorical variables can be displayed in the form of a mosaic plot.\n\n\nCode\ntable(Snodgrass$Inside, Snodgrass$Earplugs)\n\n\n         \n           0  1  2  3  4\n  Inside  21 10  5  1  1\n  Outside 50  2  0  1  0\n\n\nFigure 3 shows the above tabulated counts graphically. Evidently, greater number of earplugs were found outside than inside.\n\n\nCode\ntab &lt;- table(Snodgrass$Inside, Snodgrass$Earplugs)\nmosaicplot(tab, main=\"Inside*Earplugs counts\", ylab=\"Earplugs\")\n\n\n\n\n\nFigure 3: A mosaic plot\n\n\n\n\nThe R package vcd can produce many other types of plots for categorical variables but these plots require an understanding of models fitted to categorical data. Figure 4 displays the counts in a different way (and ignore the P value displayed in the plot). The test of association topic is covered later on.\n\n\nCode\nlibrary(vcd)\n\n\nLoading required package: grid\n\n\nCode\nassoc(tab, shade=TRUE, legend=TRUE) \n\n\n\n\n\nFigure 4: Association plot\n\n\n\n\nSpine plots are a special cases of mosaic plots that mimics stacked bar plots; Try-.\n\n\nCode\nspine(tab) \n\n\n\n\n\nFigure 5: Spine plot\n\n\n\n\nA doubledecker plot is used to handle many factors. These plots become harder to interpret when the number of categorical factor levels becomes large; Try-\n\n\nCode\ndoubledecker(Effigies~Inside+Segment,  data=Snodgrass)\n\n\n\n\n\nFigure 6: Doubledecker plot"
  },
  {
    "objectID": "studyguide/2-eda.html#dotplot",
    "href": "studyguide/2-eda.html#dotplot",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Dotplot",
    "text": "Dotplot\nOne of the simplest kinds of graph for a numerical variable is the single line (or one dimensional) graph. Each observation is represented by a point (or a star * or a dot .) on the line. These graphs can be useful to see the rough distribution and gaps, if any, in the data. Figure 7 shows a dotplot of Length of the house pits for the Inside and Outside groupings. Evidently privileged tribes lived inside in general, and the length of their dwellings is longer.\n\n\nCode\nggplot(Snodgrass) +\n  aes(x=Inside, y=Length) + \n  geom_dotplot(binaxis='y', dotsize=.5) + \n  coord_flip()\n\n\n\n\n\nFigure 7: A typical dot plot for two groups\n\n\n\n\nOn reflection, the one dimensional dotplot gives rise to a second dimension wherever observations are identical or nearly the same. In a dotplot such points are usually stacked. As our eyes tend to pick up trends very easily, they tend to be distracted by the vertical changes. The main problem here is that some of the values which coincide by chance may be taken to represent real increases in density. To overcome this we try ‘jiggling’ or jittering the data, which is equivalent to plotting a random number on the vertical axis against the data values on the horizontal axis. It is suggested that the vertical jiggle be restricted to a small range so that the overall effect is that of a one-dimensional spread with greater, or lesser, density of points along the graph. Figure 8 shows a jiggled dotplot. You can decide for yourself which strategy is better.\nClearly, the use of jiggling is much more relevant for larger sets of data. Notice that with jiggling we are interested in the relative density of the graph in different places rather than in trends.\nAs the vertical reading is determined at random our eyes may be distracted by a seeming trend which is mainly due to this randomness. For this reason, it is helpful to try more than one plot with different jiggles. This is in the spirit of EDA, in which data is viewed from as many angles as is feasible. The other side of this coin is that conclusions from one display of data should be treated with some scepticism. The availability of computers allows data to be plotted, tabulated and transformed quickly, so we should look at it several ways to better appreciate the peculiarities inherent in our data.\nIn practice, one tends to avoid jiggling except in (two-variable) scatter plots where there are many repeated \\((x, y)\\) points. Graphs with a jiggle are mainly needed when the data are discrete or grouped in some way, or there are a large number of points to be plotted.\n\n\n\n\n\nFigure 8: A jittered dot plot"
  },
  {
    "objectID": "studyguide/2-eda.html#histograms",
    "href": "studyguide/2-eda.html#histograms",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nThe histogram is a standard method of showing the distribution of a quantitative variable. A histogram is made by dividing the range of a variable into “bins” and counting the number of data points that fall within each bin. It is like a bar chart after forcing a quantitative variable into bins. It can be used for discrete ungrouped data, but a bar chart is generally more suitable for discrete data because histograms can mislead the reader into thinking that values exist other than at the centre of the intervals.\nFigure 9 shows the histogram of Length. Clearly the distribution is far from symmetric and left skewed. The bimodal pattern also suggests subgrouping.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x = Length) + \n  geom_histogram(bins=12, \n                 color=\"black\", \n                 fill=\"white\")\n\n\n\n\n\nFigure 9: A typical histogram\n\n\n\n\nNotice that a different shape of histogram may result depending on the class intervals (bins) used (that is by choosing different class widths or different midpoints of the classes). This suggests that it may be wise to draw more than one histogram for a given data set particularly if statements are to be made about modes or gaps in the data. The guideline for the maximum number of intervals, \\(L\\), can follow the same formula as for stem-and-leaf displays. Some software use the Sturges (1926) formula namely \\[\\texttt{bin width} = \\frac {\\texttt{range}(x)} {\\log_2(n)+1}.\\] For fixing the width of the bins, the range of the data is divided by the sum of one and the base two logarithm of the sample size.\nThere are a few ways to scale the y-axis of a histogram, as shown in Figure 10:\n\nFrequency histograms show the counts in each bin; the heights of the bars sum to \\(n\\).\nRelative frequency histograms show the proportions or percent of data in each bin—the counts in each bin divided by the total; the heights sum to 1 or 100%.\nDensity histograms show the densities; the areas (heights \\(\\times\\) widths) of the bars sum to 1.\n\n\n\nCode\np1 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(bins = 20) +\n  ylab(\"\") + \n  ggtitle(\"(a) Frequency histogram\", \n          \"Heights of the bars sum to n\")\n\np2 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(\n    bins = 20, \n    mapping = aes(y = after_stat(count / sum(count)))\n    ) + \n  ylab(\"\") +\n  ggtitle(\"(b) Relative frequency histogram\",\n          \"Heights sum to 100%\") + \n  scale_y_continuous(labels = scales::percent)\n\np3 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(\n    bins = 20, \n    mapping = aes(y = after_stat(density))) +\n  ylab(\"\") + \n  ggtitle(\"(c) Density histogram\",\n          \"Heights x widths sum to 1\")\n\nlibrary(patchwork)\n\np1 + p2 + p3\n\n\n\n\n\nFigure 10: Three types of histograms\n\n\n\n\nIf, instead of drawing blocks over the class intervals as in a histogram, we join the mid points of the tops of the bars, we obtain a display known as a frequency polygon . If we use relative frequencies, it is a relative frequency polygon; see Figure 11.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x = Length, y = after_stat(density)) +\n  geom_freqpoly( bins = 12, color = \"black\")\n\n\n\n\n\nFigure 11: A typical frequency polygon\n\n\n\n\nFigure 12 shows a kernel density plot, which is a method that produces a smoother density line.\n\n\nCode\nSnodgrass |&gt; \n  ggplot() + \n  aes(Length) + \n  geom_density() + \n  geom_rug(alpha = .3, colour = \"darkorange2\")\n\n\n\n\n\nFigure 12: A typical smoothed density curve\n\n\n\n\nKernel Density Estimation (KDE) is generated by dropping a little mini-distribution, usually a normal curve, on each individual point and summing them up (Figure 13).\n\n\n\nFigure 13: A demonstration of kernel density estimation\n\n\nThese smooth plots are also useful to see the type of skewness involved, multimodality (several peaks) and unusual observations (or outliers). The disadvantage is that we may over-smooth and mask some of the subgrouping patterns; see Figure 14. Even though the two distributions appear to be similar, the length distribution for the Inside group has a larger mean/median etc (called the location parameters).\n\n\nCode\nSnodgrass |&gt; \n  ggplot() + \n  aes(x = Length, \n      colour = Inside) + \n  geom_density()\n\n\n\n\n\nFigure 14: Identification of Subgrouping"
  },
  {
    "objectID": "studyguide/2-eda.html#theoretical-distributions",
    "href": "studyguide/2-eda.html#theoretical-distributions",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Theoretical Distributions",
    "text": "Theoretical Distributions\nA histogram gives a general picture of the frequencies or relative frequencies of the data. If an observation is taken at random from the population from which the sample was drawn, the chance of it falling in any one of the class intervals should be approximately proportional to the area of the block over that interval (not exactly proportional because the histogram was only based on a sample from the population, not the population itself).\nIf we could draw a histogram for the whole population, then the probabilities would be exact. The relative frequencies form a probability distribution and if the class intervals are small enough the tops of the bars of the histogram would often approximate a smooth curve such as the density plot. In many cases this smooth curve is almost symmetrical with a rounded peak at the centre. One particular shape, fitting a certain mathematical formula, is a very good approximation to many sets of data. For instance, heights of people, weights of animals or yields of wheat all follow this distribution fairly well (although theoretically the distribution has an infinite range of possible values). One of the widely used theoretical distribution is called the Normal or Gaussian distribution but we will not bore you with its formula. However, most software can compute the tail areas (probabilities) under theoretical distributions. We shall study this distribution in greater detail in a Chapter 3. For the moment let us note that our EDA techniques should provide us with information about the parameters of the distribution, and we also want graphical techniques that display the continuous nature of the data.\nDiscrete data also have probability distributions and the bar graph gives a general picture of its shape. One commonly occurring discrete theoretical distribution is known as the Poisson distribution and approximates reasonably well the numbers of road accidents over a fixed time period. The main feature of a Poisson distribution is that there is a peak at the nearest integer below or equal to the average, and then a steady falling off. The mean is a parameter of the model, so we want our exploratory techniques to tell us about the mean, and to help us decide whether or not the Poisson model is appropriate for a given set of data. Theoretically, possible values are any whole positive numbers, but large numbers of accidents are very unlikely. Again, any graphical method we use must properly display the discrete nature of Poisson data. The other commonly adopted discrete distribution is the binomial distribution. This distribution is valid when the individual outcomes are classified into two categories such as Success and Failure."
  },
  {
    "objectID": "studyguide/2-eda.html#symmetry-plots",
    "href": "studyguide/2-eda.html#symmetry-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Symmetry plots",
    "text": "Symmetry plots\nA symmetry plot is based on the distance of left and right side points. If the underlying distribution is symmetric, the points on the left and right side would be roughly equidistant from the median. For the length data, we obtain the symmetry plot after writing a little R function as follows:\n\n\nCode\nsymmetryplot &lt;- function(x)\n{\n  require(ggplot2)\n  \n  x &lt;- na.omit(x)\n  m &lt;- length(x) %/% 2\n  sx &lt;- sort(x)\n  dfm &lt;- tibble( \n    X = median(x) - sx[1:m], \n    Y = rev(sx)[1:m] - median(x)\n    )\n  \n  dfm |&gt; \n    ggplot() + \n    geom_point(mapping = aes(x=X, y=Y)) + \n    geom_line(mapping = aes(x=X, y=X)) + \n    theme(aspect.ratio=1) + \n    xlab(\"Distance below median\") + \n    ylab(\"Distance above median\")\n}\n\n\n\n\nCode\nsymmetryplot(Snodgrass$Length)\n\n\n\n\n\nThe points do not follow the \\(45^\\circ\\) line where \\(Y=X\\); divergence from this line indicates a lack of symmetry.\nIf each half of the distribution is further divided in halves, we obtain the fourths or( hinges), as the medians for the lower and upper halves. The end points, the minimum and the maximum are called the lower and upper extreme values, respectively.\nThe R function obtains the set (minimum, lower hinge, median, upper hinge, maximum) in its function fivenum.\n\n\nCode\n# Five number summary\nfivenum(Snodgrass$Length)\n\n\n[1]  4.00 13.00 14.50 17.75 21.00"
  },
  {
    "objectID": "studyguide/2-eda.html#box-plots",
    "href": "studyguide/2-eda.html#box-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Box plots",
    "text": "Box plots\nA boxplot (or sometimes a box-and-whisker plot) is a visual display of the five-number summary, which can be used to assess the skew in the data. The components of a boxplot are explained in Figure 15.\n\n\n\n\n\nFigure 15: Annotated boxplot\n\n\n\n\nThe box shows the positions of the first and third quartiles, or “hinges”, and the thick middle line shows the median. As 50% of the observations fall between the first and third quartiles, the position of the box is important and is the first thing that catches our eyes. If the median is in the centre of the box, this suggests that the distribution may be symmetric; if the median is near one end, the distribution is skewed. In particular, if the median is in the right side of the box, it implies that the middle 50% of the data are skewed to the left. For right skewed data, the median will be placed at the left end of the box (as against in the centre of the box).\nAfter having considered the ‘centre’ of a distribution, which we have measured by the median, and looked at where the bulk of the data lies, measured by the quartiles; then we are particularly interested in the strange, unusual values. They could be obvious mistakes (incorrectly coded, or decimal places in the wrong place, transposition of digits such as 91 instead of 19) or incorrect measurements or recording. These last two possibilities often occur before the data gets to the statistician so that it may be difficult to check whether these mistakes have occurred. Apart from obvious errors, we can only point to ‘far out’ values as being strange and we term them ‘outliers’. To help decide this matter, we define a few more terms, of which the most common one is the interquartile range or IQR, which is the distance between the first and third quartiles ( \\(Q3-Q1\\) ). Any observation which is more than \\(1.5 \\times IQR\\) (sometimes called the “step”) less than the first or more than the third quartile can be considered a possible outlier (Figure 15.\nBoxplots are ideal for comparing a few groups of data because they give a visual summary of certain main features of the distributions but they are not as cluttered as other displays. Common sense must prevail, for there is a limit to the amount of information that the eye can take in. Up to 10 or 12 boxplots can be plotted, preferably on the same scale; more than 12 could be confusing.\nShown in Figure 16 are the boxplots of Length for the Inside-Outside groupings. Note that the boxplots have been constructed on the same scale; that is, they have common (Length) axis.\n\n\nCode\nggplot(Snodgrass) + \n  aes(y = Length, \n      x = Inside, \n      col = Inside) + \n  geom_boxplot() + \n  coord_flip()\n\n\n\n\n\nFigure 16: Boxplots\n\n\n\n\nThe boxplots in Figure 16 show that the general shapes of the distributions of Length are broadly similar. In each case, the median is near the centre of the box. The box of the Outside group is entirely to the right of the box of the Inside group, indicating that the distribution of Length is higher for the Outside group. We can also see that the Inside group may have greater spread than the Outside group."
  },
  {
    "objectID": "studyguide/2-eda.html#letter-value-plots",
    "href": "studyguide/2-eda.html#letter-value-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Letter-value plots",
    "text": "Letter-value plots\nFor large datasets, a summary based on five numbers may be overly simple, showing too many ‘outliers’ and obscuring some structure in the more extreme range of the data. Hofmann, Wickham, and Kafadar (2017) describe an extension to the boxplot called the “letter-value plot”.\nLike boxplots, letter-value plots include the median and the two middle fourths of the data. However, the whiskers are replaced by smaller boxes, representing ever-decreasing proportions of the data, such as eighths, sixteenths, etc., represented by letter values. The extent of this extra boxing depends on the quantity and shape of the data.\nLetter values are defined by their depths, or distances from the nearer edge of the ordered data.\nmedian: found at depth \\(d(M) = (n + 1)/2\\) where \\(n\\) is the number of observations\nfourths: found at depth \\(d(F) = \\{\\verb\"int\"[d(M)] + 1\\}/2\\) where \\(\\verb\"int[]\"\\) refers to the integer part function.\neighths: found at depth \\(d(E) = \\{\\verb\"int\"[d(F)] + 1\\}/2\\)\nSimilarly if we had enough data, we could extend further\nsixteenths: found at depth \\(d(D) = \\{\\verb\"int\"[d(E)] + 1\\}/2\\)\nthirty-seconds: found at depth \\(d(C) = \\{\\verb\"int\"[d(D)] + 1\\}/2\\)\nand thus define points \\(B, A, Z, Y, X,\\) and so on.\nNote the M, F and E could be termed measures of location. It is also helpful to have some measures of spread which indicate the variability in the distribution. One such measure, the F-spread, is the distance between the two F (fourths) values; that is, the inter-quartile range (IQR). Finally, the range is the distance between the two extreme points.\nAnother useful measure of location is obtained by averaging the two hinges which is termed the mid-fourth (or mid-F). If the distribution were perfectly symmetrical, the mid-F would be the same as the median. If mid-F is smaller (larger) than the median, the distribution is skewed to the left (right). Note that the mid values can be found at any depths.\nSee here for more details.\nLetter values are displayed as a graph called letter value plot; see Figure 17. This type of plot is a better display than a boxplot for big data which tend to show many unusual points. Boxplots are suitable to moderate size datasets only. For large datasets, the skew may not be consistent within the data. The LV plot will show such anomalies in the skew patterns.\n\n\nCode\nlibrary(lvplot)\nLVboxplot(Snodgrass$Length, xlab = \"Length\")\n\n\n\n\n\nFigure 17: Letter-value plot for length data\n\n\n\n\nA letter value plot showing the eighths is shown in Figure 18. Note that this LV plot does not have a whisker, but the points falling outside the eighths are tagged.\n\n\nCode\nLVboxplot(Snodgrass$Length ~ Snodgrass$Inside, xlab = \"Length\")\n\n\n\n\n\nFigure 18: Letter-value plots of Length by Inside\n\n\n\n\nIf we compare Figure 16 with Figure 18, we can see that the whiskers have been replaced with more ‘outliers’ and two extra boxes. The middle boxes are the same; they show the first and third quartiles. The smaller, darker boxes in Figure 18 show the first and seventh eighths.\nFigure 19 shows a ggplot version of an LV plot for a big random data set.\n\n\nCode\nset.seed(123)\n\nlibrary(lvplot)\n\ndata.frame(\n  y = c( rlnorm(n=1e4, mean = 10) , \n         rnorm(n=1e4, mean = 5e5, sd = 1e5) \n         ),\n  x = rep(c(\"Log-normal(10,1)\",\"Normal(500k,100k)\"), each=1e4) |&gt; factor()\n) |&gt; \n  ggplot() + \n  aes(x=x, y=y, fill=after_stat(LV)) +\n  geom_lv() + \n  scale_fill_lv() +\n  coord_flip() +\n  theme_bw() +\n  xlab(\"\")\n\n\n\n\n\nFigure 19: A ggplot letter-value plot"
  },
  {
    "objectID": "studyguide/2-eda.html#quantile-quantile-q-q-plot",
    "href": "studyguide/2-eda.html#quantile-quantile-q-q-plot",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile (Q-Q) Plot",
    "text": "Quantile-Quantile (Q-Q) Plot\nAt times, two groups may have different numbers of observations but they may be similar in some ways and it may be appropriate to compare them. This comparison can be done in several ways but Q-Q plot is once such graph useful for such a comparison. The quantiles of the first batch is plotted against the quantiles of the second batch. A 45 degree \\((Y=X)\\) line is also drawn. If the points on a quantile-quantile plot fall along, or close to, the 45 degree line, then the two variables follow the same distribution; if they fall on a line parallel to the 45 degree line, the two variables follow similar distributions but one has a larger location parameter (say mean, or median) than the other; if the slope differs from the 45 degree line the variables have a similar distribution but have different scale parameters (say, standard deviations or F-spreads); if the plot is linear then the distributions of \\(Y\\) and \\(X\\) are different.\n\n\nCode\nset.seed(12344)\nX &lt;- rnorm(100)\nY &lt;- rnorm(100)\nnq &lt;- 201\n\np &lt;- seq(1 , nq, length.out = 50) / nq - 0.5 / nq\n\np1 &lt;- ggplot() + \n  aes(x = quantile(X, p), y = quantile(Y, p)) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"X & Y roughly follow\\nthe same distributions\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX2 &lt;- rnorm(100)\nY2 &lt;- rnorm(100, 1)\n\np2 &lt;- ggplot() + \n  aes(x = quantile(X2, p), \n      y = quantile(Y2, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distribution of Y has\\na larger location (mean/median)\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX3 &lt;- rnorm(100, 0, 1)\nY3 &lt;- rnorm(100, 0, 2)\n\np3 &lt;- ggplot() + \n  aes(x = quantile(X3, p), \n      y = quantile(Y3, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distribution of Y has\\na larger scale (SD, F-spread etc)\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX4 &lt;- rbeta(100, 1, 4)\nY4 &lt;- rnorm(100, 0, 2)\n\np4 &lt;- ggplot() + \n  aes(x = quantile(X4, p), y = quantile(Y4, p)) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distributions of  X and Y \\nare different\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2)\n\n\n\n\n\nFigure 22: Quantile-Quantile plot patterns\n\n\n\n\nIn Figure 22, different kinds of quantile-quantile plots are shown; note the following three cases in particular.\n\n\\(Y\\) and \\(X\\) follow similar distributions but the quantiles of \\(Y\\) are a constant amount greater than the quantiles of \\(X\\).\n\\(Y\\) and \\(X\\) follow similar distributions but the standard deviation of \\(Y\\) is greater than that of \\(X\\).\n\\(Y\\) and \\(X\\) follow different distributions.\n\nFigure 23 gives Q-Q plot comparing the Length of house pits for Inside and Outside groups. This Q-Q plot suggests that the two distributions are similar but the average Length is greater for the Inside dwellings.\n\n\nCode\nnq &lt;- length(Snodgrass$Length)\np &lt;- seq(1 , nq, length.out = 20) / nq - 0.5 / nq\n\nX &lt;- Snodgrass |&gt; \n  filter(Inside == \"Inside\") |&gt; \n  pull(Length)\n\nY &lt;- Snodgrass |&gt; \n  filter(Inside == \"Outside\") |&gt; \n  pull(Length)\n\nggplot() + \n  aes(x = quantile(X, p), \n      y = quantile(Y, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  xlab(\"Length quantiles for Inside Group\") + \n  ylab(\"Length quantiles for Outside Group\")\n\n\n\n\n\nFigure 23: Quantile-Quantile plot of Length for Inside-Outside groups\n\n\n\n\nNote that the distribution of two variables \\(X\\) and \\(Y\\) can be compared in a number of ways using boxplots, overlaid ECDF plots etc."
  },
  {
    "objectID": "studyguide/2-eda.html#exploring-relationships",
    "href": "studyguide/2-eda.html#exploring-relationships",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Exploring relationships",
    "text": "Exploring relationships\nConsider data which can be written as pairs (\\(X\\), \\(Y\\)). The two groups of data (\\(X\\) and \\(Y\\)) need not be in the same units but they are assumed to be related in some way. Scatterplots are useful in exploring the type of relationship between \\(X\\) and \\(Y\\).\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData\", \n  destfile = \"testmarks.RData\")\n\nload(\"testmarks.RData\")\n\n\n\n\nCode\nggplot(testmarks) +\n  aes(y=English, x=Maths) + \n  geom_point() + \n  coord_equal()\n\n\n\n\n\nFigure 26: Scatter plot of English vs Maths scores\n\n\n\n\nThe data in testmarks.txt consists of the records of 40 students, at a school, on standard tests of comprehension of English and Mathematics. The test marks data can be graphed plotting English test scores against Mathematics test scores. The resulting graph, known as the scatterplot, is shown as Figure 26. This plot enables us to see if there is a relationship between \\(X\\) and \\(Y\\).\nHere, the scatterplot shows a clear upward trend – as the Mathematics mark increases so does the English mark, in general.\nThere are a number of useful features that can be added to a scatterplot. The individual distributions of \\(X\\) and \\(Y\\) can be shown by adding appropriate graphical displays (e.g. boxplots) along the axes. If a number of the students have the same marks then it may be advisable to add a jitter as we discussed earlier.\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- ggplot(testmarks) + \n  aes(y=English, x=Maths) +\n  geom_point() + \n  coord_equal()\n\nggMarginal(p1, type=\"boxplot\")\n\n\n\n\n\nFigure 27: English vs Maths scores\n\n\n\n\nIn Figure 27, it can be seen (with some effort) that each set of scores seems to be uniformly distributed along its axis, although the Maths marks exhibit gaps which may indicate different groups within the class.\nFinally, does the variability in English marks change as the Maths mark changes, or is it fairly constant, i.e. does the vertical height of the scatter change from left to right? In this case the variability appears to be fairly constant (we will see why this is important later).\n\n\nCode\nggplot(rangitikei) +\n  aes(y = people, x = vehicle, col = loc, shape = loc) + \n  geom_point()\n\n\n\n\n\nFigure 28: Scatterplot with a grouping variable\n\n\n\n\nIf the data is known to be grouped, then this can be shown on the scatterplot by using different symbols for each group. The data file Rangitikei.txt gives the number of people making recreational use of the Rangitikei River and the number of vehicles seen at a particular location and time. The data were collected at two locations (location being a categorical variable). The effect of location, if any, on the number of people and vehicles can be seen on a scatterplot if we identify each plotted point with a different symbol (or colour) depending on the location (see Figure 28). There appears to be little effect of location on the number of people and vehicles seen.\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- rangitikei |&gt; \n  ggplot() + \n  aes(y=people, x=vehicle) + \n  geom_point() +\n  ggtitle(\"(A): All of the data\")\n\np2 &lt;- rangitikei |&gt; \n  # remove the rows with highest value of 'people'\n  filter(people &lt; max(people)) |&gt; \n  ggplot() + \n  aes(y=people, x=vehicle) + \n  geom_point() + \n  ggtitle(\"(B): Without observation #26\")\n\ngridExtra::grid.arrange(\n  ggMarginal(p1, type=\"boxplot\"), \n  ggMarginal(p2, type=\"boxplot\"), \n  ncol=1)\n\n\n\n\n\nFigure 29: People vs vehicle plots\n\n\n\n\nFigure 30 shows the number of people against the number of vehicles seen as well as the distribution of each variable along the axes using boxplots (these are often referred to as marginal distributions). The first plot in Figure 30 (A) shows the scatter of all points, whereas the second one (B) shows the scatter with the very large value (Observation #26) removed. In the first plot the outlier shows up quite clearly in both boxplots. In the second plot the boxplot associated with the variable ‘people’ still shows one point as an outlier, though this doesn’t appear to be as extreme as the one in the first plot, and in the scatterplot the points fall close to a line which has a positive slope as is the case of Figure 30 (A). However, the distribution of each variable is skewed to the right.\nNote that the case which produced the outlier in both the marginal distributions did not produce a scatterplot value that was in any way unusual. While this might not surprise us (the relationship between people and vehicles might be expected to stay the same even for a location with large numbers of each), it follows that an outlier with respect to the underlying relationship between two variables will be different from an outlier in the marginal distribution sense. And such a value may not be an outlier in either marginal distribution. Look again at Figure 27 - there is one student with a high Maths mark but a low English mark. While neither mark is unusual in itself, the point stands out from the rest of the scatterplot because it is unusually high or low (i.e. up or down) on the plot. In general an outlier with respect to the relationship between \\(X\\) and \\(Y\\) is one whose \\(Y\\) value is unusually large or small compared to other cases with similar \\(X\\) values.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/horsehearts.RData\",\n  destfile = \"horsehearts.RData\")\n\nload(\"horsehearts.RData\")\n\n\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- ggplot(horsehearts) + \n  aes(y=WEIGHT, x=EXTDIA) + \n  geom_point()\n\np2 &lt;- p1 + geom_smooth(se=FALSE)\n\ngridExtra::grid.arrange(\n  ggMarginal(p1, type=\"density\"), \n  ggMarginal(p2, type=\"density\"), \n  ncol=1)\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nFigure 30: Weight vs Exterior Widths\n\n\n\n\nAnother example of a scatterplot is shown in Figure 30 which displays the weight of horses’ hearts against their exterior widths in the systole phase (that is the contracted phase)(data set horsehearts.txt). The scatterplot indicates that a straight line would not fit the data well. On the other hand, there is some evidence the weights fall into one of two subgroups (with a value around 2.5kg being the cut off point). Scatterplots are particularly useful in pinpointing possible groupings of points so that the structure of the data is revealed which might otherwise go undetected in descriptive statistics calculated for the full data set.\nOne way of quickly assessing whether a straight line fit is reasonable is to smooth the points in the plot using a LOWESS (LOcally WEighted Scatterplot Smoother) smoothed line. The lowess (also called loess) procedure is based on local sets of points (instead of the whole data set) at a given \\(x\\) range, and fits a smoothed line to the data. The second scatterplot in Figure 30 shows the lowess smoother which confirms that there is curvature in the underlying relationship- possibly a cubic curve would fit this.\n\nScatterplots and Correlation Coefficients\nWe will use the data set horsehearts.txt of the weights of horses’ hearts in relation to other heart measurements to illustrate the measurement of correlation. As we have seen before, scatterplots provide a good starting point for exploring the type of relationship between two variables. In Figure 31, the weight, \\(Y\\), is plotted against the exterior width \\(X\\) (diastole phase in column 7 of horsehearts.txt). It is helpful to divide the scatterplot into four quarters, called quadrants, by drawing a line horizontally at the average \\(Y\\) value or weight (2.235kg) and vertically at the average \\(X\\) value (or diastole exterior width 14.13mm). The first quadrant is the top right hand side quarter. We move anti-clockwise for numbering the quadrants, and the bottom left hand side quadrant is called the third quadrant.\n\n\n\n\n\nFigure 31: Four quadrants of a scatter plot\n\n\n\n\nIn Figure 31, most of the points fall in the third and first quadrants which indicates that there is an increasing relationship between the two variables (as the length increases, the weight tends to increase which is not too surprising!).\nAlthough this scatterplot indicates a nonlinear relationship would fit better than a linear relationship, we will continue to use it for illustration. It is useful to have an objective measure of how strong a linear relationship is (as we have seen, stretching or compressing one or both axes can give different impressions). This measure is called the Pearsonian correlation coefficient between \\(X\\) and \\(Y\\) and is denoted by \\(r_{xy}\\) or \\(r\\) for short:\n\\[\n\\begin{aligned}\nr_{xy} &= \\frac{{\\text {sum}(x - \\bar x)(y - \\bar y)}}{{\\text{squareroot}[\\text {sum}(x - \\bar x)^2  {\\text{sum}}(y - \\bar y)^2 ]}} \\\\\n\\\\\n&= \\frac{\\sum (x-\\bar{x})(y-\\bar{y}) }{\\sqrt{\\sum (x-\\bar{x})^{2}  \\sum (y-\\bar{y})^{2}  } } \\\\\n\\\\\n&=\\frac{S_{xy} }{\\sqrt{S_{xx} S_{yy} } }\n\\end{aligned}\n\\]\nHere \\(S_{xx}=S_x^2\\) is the variance of \\(X\\), \\(S_{yy}=S_y^2\\) is the variance of \\(Y\\), and \\(S_{xy}\\) is the covariance between \\(X\\) and \\(Y\\).\nThe (Pearsonian) correlation (coefficient) between WEIGHT and EXTDIA equals 0.759. Perfect correlation (+l) would occur if all points fell on a straight line in the first and third quadrants. Assuming a bivariate normal distribution, the null hypothesis that the true correlation coefficient \\(\\rho _{xy} =0\\) can be tested using the test statistic \\[t = \\frac{{r_{xy} }}{{\\sqrt {\\frac{{1 - r_{xy}^2 }}{{n - 2}}} }}\\] Statistical software programs directly give the \\(p\\)-value for this test. The estimated correlation coefficient of 0.759 for \\((n-2)=44\\) df is significant (\\(p\\)-value is close to zero). Testing of hypothesis related topics are covered in the next Chapter.\nThe scatterplot indicates that there may be two groups of horses; whose hearts weigh about 2.3 kg or more and whose heart weights are less than 2.3 kg. If possible, one should try to establish whether there are two groups for some reason (perhaps age, sex or breed). If the points tended to concentrate in quadrants 2 and 4, this would indicate that the correlation between \\(Y\\) and \\(X\\) is negative (as \\(X\\) increases, \\(Y\\) tends to decrease). If the points were evenly scattered among the four quadrants, the correlation coefficient would be close to zero.\n\n\n\n\n\nFigure 32: Limitations of the correlation coefficient\n\n\n\n\nFigure 32 shows a number of possible plots of \\(Y\\) against \\(X\\) and their associated correlation coefficients. Notice that the correlation coefficient may be close to zero even though there is a strong pattern present. This is because the correlation coefficient only measures the strength of the linear relationship between two variables. In the presence of subgroups the correlation coefficient can become spurious or even be close to zero when there is indeed the correlation is high within the subgroup. Simpson (1951) demonstrated that relationships noted within a population may not hold and could be the opposite within all subgroups. This amalgamation paradox can happen to most summary statistical measures including the correlation coefficient.\nThe other situation that is not shown in Figure 32 is that the correlation coefficient can be positive and a line would be a reasonable fit although the scatter diagram indicates that both \\(Y\\) and \\(X\\) have skewed distributions suggesting that transformations should be applied.\n\n\n\nFigure 33: Sign of the correlation coefficient\n\n\nTo see why the sign of \\(r\\) indicates the direction of the relationship, consider Figure 33.\n\nRegardless of which quadrant the point is in, the contribution to \\(S_{xx}\\) and \\(S_{yy}\\) is positive, as these are sums of squares.\nIn the first quadrant the contribution of each point to the cross product \\(S_{xy}\\) is \\((+) \\times (+)\\), that is a positive amount. In the third quadrant, each contribution is \\((-)\\times (-)\\), that is a positive amount. This is the reason that the correlation coefficient is positive if most points are in the first and third quadrants.\nPoints in the second and fourth quadrant contribute negative amounts, \\((-)\\times (+)\\) or \\((+)\\times (-)\\), to the cross product \\(S_{xy}\\). If there are a considerable number of points in quadrants 2 and 4, the correlation coefficient will tend to be negative.\n\n\n\nScatterplot and correlation Matrices\nWhen there are more than two variables of interest, it is possible to draw scatterplots of each pair of variables but this becomes a bewildering exercise as the number of variables increases. One way to keep track of these plots is to set them out in a scatterplot matrix as in Figure 34 which is based on the data set pinetree.txt containing the circumference measurements of pine trees at four positions (Top, Third, Second and First) and in three areas of a forest. This plot also shows the simple correlation coefficients on the upper diagonal and smoothed density plots on the main diagonal of the matrix. This plot clearly shows that the variables are strongly related to each other. In particular, the Top circumference can be predicted reasonably well based on the bottom circumference. Even though the correlations are high, a quadratic model may fit better. Figure 34 seems to show a non-random pattern in the way points appear on the plot. This may be due to the Area effect because the circumference data were collected from three different areas. Figure 34 confirms that this is indeed the case.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/pinetree.RData\", \n  destfile = \"pinetree.RData\")\nload(\"pinetree.RData\")\n\n\n\n\nCode\nlibrary(GGally)\n\nggpairs(pinetree[,2:5],\n        mapping=aes(shape=pinetree$Area))\n\n\n\n\n\nFigure 34: Four quadrants of a scatter plot\n\n\n\n\nIt is also possible to use the colour option as well as the subgroup-wise correlation coefficients. Try\nggpairs(pinetree[,2:5],\n        mapping=aes(color=pinetree$Area))\n\n\nExploring bivariate distributions\nThe two-dimensional or the bivariate probability distribution of \\(X\\) and \\(Y\\) can be displayed as a 2-d smoothed density plot or as a probability contour plot. Figure 35 shows such plots. This figure rather suggests that there may be two separate joint probability distributions of Maths and English scores. This clue is not obtained in the scatter plot of English score against Maths score but only from the other three plots.\n\n\nCode\ny = testmarks$English\nx = testmarks$Maths\ndef.par = par(mfrow = c(2, 2))\n\nplot(x, y, xlim = c(0,100), ylim = c(0,100),\n     xlab = \"Maths Marks\",\n     ylab = \"English Marks\",\n     main = \"Scatter plot of English Scores vs Maths Scores\")\n\nf1 = kde2d(x, y, n = 50, lims = c(0, 100, 0, 100))\n\nimage(f1,\n      xlab = \"Maths Marks\",\n      ylab = \"English Marks\",\n      col=gray((0:32)/32), main = \"Image Plot\")\n\ncontour(f1, \n        xlab = \"Maths Marks\",\n        ylab = \"English Marks\",\n        main = \"Density contourPlot\")\n\nop = par(mar=c(0,0,2,0)+.1)\n\npersp(f1, phi = 45, theta = 30,\n      xlab = \"Maths Marks\", ylab = \"English Marks\", zlab = \"density\",\n      col = \"gray\", shade = .5, border = NA,\n      main = \"Perspective plot (density estimation)\")\n\npar(op)\n\npar(def.par)\n\n\n\n\n\nFigure 35: Exploring bivariate distributions\n\n\n\n\n\n\n\n\n\n\n\nIn summary, scatterplots may reveal many properties and peculiarities about the relationship of \\(Y\\) to \\(X\\), namely\n\nTrends: the overall trend may be positive or negative. It may be appropriate to fit a straight line or a curve.\nOutliers: points which appear to be unusual, or outliers, may be evident. In a scatterplot, the outlier need not be in the extreme scale of both \\(X\\) and \\(Y\\), and anything appearing peculiar or as a rogue point must be investigated.\nGaps may be seen in the data which may indicate that different groups are present. On the other hand, if it is known that points belong to different groups, the points can be tagged with different symbols. The basic idea is to see whether different groups follow the same pattern or different patterns.\nVariability of \\(Y\\) change with \\(X\\), or may be fairly constant. Clustering of points with smaller variability when compared to rest of the points may also be revealed in some scatterplots.\n\nBoth joint and marginal distributions of \\(X\\) and \\(Y\\) are important. We also consider the distribution of each variable separately, and explore whether they have symmetric distributions or not."
  },
  {
    "objectID": "studyguide/2-eda.html#higher-dimensional-data",
    "href": "studyguide/2-eda.html#higher-dimensional-data",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Higher dimensional data",
    "text": "Higher dimensional data\nA short-cut to explore three numerical variables is to obtain a contour plot. In Figure 35, the contours were the joint density of Maths and English marks. Instead, we can just use three numerical variables and show the third variable (Z) as a contour on a scatter plot of Y vs. X. What this means is that several combinations of X and Y can lead to the same Z. So we have to use some smoothing method or a model to obtain the contours and form a grid. The R package plotly is rather simple and powerful to obtain dynamic or interactive plots. The plots produced with this package are colour optimised for viewing and hence they may not be the best for printing but good for EDA purposes. We can also hover over the graph and view the graphed data. After loading the pine tree data, try-\n\n\nCode\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)\n\n\n\n\n\n\nFor the Snodgrass data, the contour plot of East (X) vs, South (Y) vs. Area (Z) is shown in Figure 36. This plot shows that bigger house pits are inside the X and Y axis show the geolocations.\n\n\n\nFigure 36: A contour plot for the Snodgrass data\n\n\nA bubble plot is another option where a scatter plot shows the third variable as the size of the bubble on a scatter plot. This type of plot can be restrictive for some datasets depending on the variability involved in the third variable. Figure 37 shows the location of the house pits and total Area. This plot not only gives the spatial view of the tribal area but indirectly reveals the clustering of house sizes.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x=East, y=South, colour=Area, size=Area) + \n  geom_point()\n\n\n\n\n\nFigure 37: A bubble plot for the Snodgrass data\n\n\n\n\nUsing the appropriate software, it is possible to explore three variables using a 3-D scatter plot. We explore the pinetree data in Figure 38 which clearly shows the strong relationships and the Area effect. The other forms of exploring 3 variables include 3-D surface plots.\n\n\nCode\nlibrary(lattice)\ncloud(Top ~ First+Second, group=Area, data=pinetree)\n\n\n\n\n\nFigure 38: A 3-D plot\n\n\n\n\nR packages such as rgl and plotly allow rotations of axes for graphical exploration of the relationships among variables, detection of peculiar points etc. It is easy to miss the form of relationships if proper rotational exploration is not done. Try:\n\n\nCode\nlibrary(rgl)\nplot3d(Snodgrass$East, Snodgrass$South, Snodgrass$Area)\n\n\nplotly is great for interactive 3d plots (though they only work with HTML, not PDF).\n\n\nCode\nplot_ly(Snodgrass, x = ~East, y = ~South, z = ~Area) |&gt; \n  add_markers()\n\n\n\n\n\n\nIf we wish to examine the relationships between (three or more) variables we could do so with a conditioning plot, otherwise known as a coplot. A coplot is a scatterplot matrix where each panel in the matrix is a scatterplot of the same two variables, whose observations are restricted by the values of a third (and possibly a fourth) variable.\n\n\nCode\ncoplot(Top~ First | Second*Area, data = pinetree)\n\n\n\n\n\nFigure 39: A co-plot\n\n\n\n\nThe coplot in Figure 39 shows that pine trees are not maturing well in Area 3, given that fewer points are on the right hand side of the graph when compared to Areas 1 and 2.\nObviously we cannot to visualise several dimensions easily but there are few shortcuts for multivariate data visualisation such as the Chernoff or cartoon faces plot. This type of plot is based on our ability to recognise features in human faces. This type of plot is suitable for small datasets and identify unusual observations, for example observation #46 in horsehearts data shown in Figure 40.\n\n\neffect of variables:\n modified item       Var       \n \"height of face   \" \"INNERSYS\"\n \"width of face    \" \"INNERDIA\"\n \"structure of face\" \"OUTERSYS\"\n \"height of mouth  \" \"OUTERDIA\"\n \"width of mouth   \" \"EXTSYS\"  \n \"smiling          \" \"EXTDIA\"  \n \"height of eyes   \" \"WEIGHT\"  \n \"width of eyes    \" \"INNERSYS\"\n \"height of hair   \" \"INNERDIA\"\n \"width of hair   \"  \"OUTERSYS\"\n \"style of hair   \"  \"OUTERDIA\"\n \"height of nose  \"  \"EXTSYS\"  \n \"width of nose   \"  \"EXTDIA\"  \n \"width of ear    \"  \"WEIGHT\"  \n \"height of ear   \"  \"INNERSYS\"\n\n\n\n\n\nFigure 40: A Cartoon Faces plot\n\n\n\n\nExploration of higher dimensional data requires further theory. Reduction of dimensions and display of crucial information using newly generated variables based on the theory of multivariate statistics are the natural way to explore higher dimensional data. Many such approaches are taught mainly in 300 level papers. A couple of such methods are explained below:\nOne of the tricks to handle multidimensional data is to reduce the number of dimensions using principal components. These are simply linear combinations of the original variables and may not have physical meaning. In some cases, we may be able to identify a latent variable that represents such a linear combination of variables. For example, physical endurance is latent but can be measured indirectly using many variables such as time taken to run 100m, maximum weight lifted etc. The first principal component (PC) is found in such a way that it accounts for the most variation in the data. The second principal component is found independent of the first in such a way that it accounts for the most variation that is not explained by the first principal component. The following output shows that the first two components are sufficient to explain most (&gt;98%) of the variation in the four numerical variables in the pinetree dataset.\n\n\nCode\nfit &lt;- prcomp(pinetree[, -1], scale. = TRUE)\nsummary(fit)\n\n\nImportance of components:\n                          PC1     PC2     PC3    PC4\nStandard deviation     1.9579 0.32825 0.19678 0.1415\nProportion of Variance 0.9584 0.02694 0.00968 0.0050\nCumulative Proportion  0.9584 0.98532 0.99500 1.0000\n\n\nCode\n# Try autoplot(fit)\n\n\nA biplot aims to show both the observations and variables in the same plot formed by the first two principal components. Figure 41 shows the biplot for the pinetree variables and data.\n\n\nCode\nlibrary(ggfortify)\nautoplot(fit, loadings = TRUE, loadings.label = TRUE)\n\n\n\n\n\nFigure 41: A biplot\n\n\n\n\nFor interpreting the biplot, consider the following patterns:\n\nIf points that are close to each other, it means observations with similar values. So you can spot the odd ones.\nThe correlation between variables is indicated by the cosine of the angle between vectors displayed for the variables. Highly correlated variables will point in the same direction. If variables are uncorrelated, they will be at right angles to each other. For the pine tree data, we notice high correlation. This has implications in model building. We may not need all of the three predictor variables of Top. More will follow in later Chapters.\nThe cosine of the angle between a vector and an axis indicates the contribution of the corresponding variable to the axis dimension. Look particularly the X-axis that represents the first principal component. The variables contribute somewhat the same.\n\nNote that the biplot is not useful if the first two principal components account for only a small part of the overall variation. We can also examine relationship between the categorical factors and the principal components but this topic is skipped in this course."
  },
  {
    "objectID": "studyguide/2-eda.html#autocorrelation",
    "href": "studyguide/2-eda.html#autocorrelation",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nTime series data are not often independent and there often exists a relationship between the current observation \\(X_t\\) at time \\(t\\) with the past observations \\(X_j\\) \\((j=t-1, t-2, \\ldots, 1)\\). The term first order lag refers to \\(X_{j-1}\\) for \\(X_j\\). In general the \\(k^\\text {th}\\) lag refers to a past observation of \\(k\\) periods back. We may explore the time related linear dependency in time series data using lagplots. Figure 44 is a lag plot for the NZ $20 series.\n\n\nCode\ngglagplot(NZnotes20)\n\n\n\n\n\nFigure 44: Lag plot of NZD 20 notes series\n\n\n\n\nThis plot suggests that the time related dependency is rather strong for lower order \\(k\\) but the lag effect diminishes when \\(k\\) becomes large . This is a good thing because we can use recent lagged data as predictors in our models.\nBy the term autocorrelation we mean the simple correlation between the observations but lagged back one or more time periods. For a given lag \\(k\\), the sample autocorrelation function (SACF or simply ACF) is defined as\n\\[r_{k} = \\frac{ \\sum \\limits_{t=k+1}^N (y_{t}-\\bar{y})(y_{t-k}-\\bar{y})} {\\sum \\limits_{t=1}^T (y_{t}-\\bar{y})^2}\\]\nwhere \\(N\\) is the length of the time series. SACFs can be used to make a tentative assessment of the terms or order of the model. Figure 45 is the plot of SACF values (called ACF plot or correlogram) for the NZ $20 series.\n\n\nCode\nggAcf(NZnotes20)\n\n\n\n\n\nFigure 45: ACF plot of NZD 20 notes series\n\n\n\n\nThis graph shows that the autocorrelations decay to zero (implying that the value of $20 notes in public hands positively depend on the values of $20 notes held in the immediate past rather than the distant past). The significance of autocorrelations may be judged from the 95% confidence interval band. More on this later.\nWe also plot the partial autocorrelations instead of autocorrelations. Partial Autocorrelation Function (PACF) at a given time is without the linear effect of other lags. Figure 46 shows the PACF plot for the ‘$20 Notes’ series. Only the first PACF is significant and not the rest. This means that the first lag is sufficient and the rest may not be needed for modelling.\n\n\nCode\nggPacf(NZnotes20)\n\n\n\n\n\nFigure 46: PACF plot of NZD 20 notes series\n\n\n\n\nBoth ACF and PACF plots are useful to assess the time series components which is discussed next."
  },
  {
    "objectID": "studyguide/2-eda.html#time-series-components-of-variation",
    "href": "studyguide/2-eda.html#time-series-components-of-variation",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Time Series Components of Variation",
    "text": "Time Series Components of Variation\nThe nature of structural variations in a time series are broadly classified under three major headings:\n\nTrend\nThe trend component is to represent long term positive (upward) or negative (downward) movement. See Figure 47 below which lists some of the parametric models for fitting the trend component:\n\n\n\nFigure 47: Trend types\n\n\nWe will cover more on fitting models to capture the trend later on.\n\n\nSeasonal\nThe seasonal component is to account for the periodic behaviour happening within a block (say Christmas time) of a given time period (say in a calendar year) but this periodic behaviour will repeat fairly regularly over time (say year after year). Seasonality is often present in business related time series data. EDA tools are useful for judging the presence of seasonality. A simple scatterplot of the response variable against time may reveal seasonality directly. Figure 48 is a time series plot of monthly means of Erythemal UV, the sunburn causing UV, measured at Lauder (45.0S, 169.7E), New Zealand (uv.txt).\n\n\nCode\nlibrary(readr)\n\nuv &lt;- read_table(\"http://www.massey.ac.nz/~anhsmith/data/uv.txt\") |&gt; \n  pull(erythemal.uv) |&gt; \n  ts(start = c(1990, 01), frequency = 12)\n\nforecast::autoplot(uv, xlab=\"Year\", ylab=\"Erythemal UV\")\n\n\n\n\n\nFigure 48: Time series plot revealing seasonality\n\n\n\n\nSeasonality is more easily seen graphically when two or more grouping variables such as month is used to subset the data as has been done in Figure 49. All tools of EDA covered in this Chapter such as boxplots etc will also be useful to explore seasonal effects. The autocorrelation plot is also useful for detecting seasonality.\n\n\nCode\n# monthplot(uv) or\nggsubseriesplot(uv)\n\n\n\n\n\nFigure 49: Month plot revealing seasonality\n\n\n\n\nA different version of the plot, known as season plot, can also be used; see Figure 50 drawn for the credit balances data. This plot shows the month related seasonal effects over the years. We can also assess whether the seasonal effect is the same for various years or not using this plot.\n\n\nCode\nggseasonplot(credit.balance, year.labels=TRUE)\n\n\n\n\n\nFigure 50: Seasonal plot for credit balances series\n\n\n\n\nThe ACF plot is also useful to assess seasonality. Figure 51 shows a scallop pattern which is attributed to seasonality.\n\n\nCode\nggAcf(uv)\n\n\n\n\n\nFigure 51: ACF plot of uv series\n\n\n\n\nSpectral plot is another graphical technique for examining cyclic structure of variation in time series data. This plot is based on a sinusoidal model. The cycle frequency is expressed per unit of time where a unit of time is the distance between 2 time series points. A frequency of 0.5 means a cycle of 2 time series data points. Methods are available to determine the amplitude and the dominant frequency etc under the sinusoidal model.\n\n\nError\nThis component of the time series is to account for the random or irregular movement in the series. Ideally the plot of errors should not show any time effect; for example plotting a set of random normal data will not show any of the systematic time related components, see Figure 52. Such a series is called a white noise series.\n\n\nCode\nset.seed(123)\nwhtnoise &lt;- ts(rnorm(120), start=1, frequency = 12)\nforecast::autoplot(whtnoise, \n                   xlab=\"time\", \n                   ylab=\"N(0,1) random data\")\n\n\n\n\n\nFigure 52: Time series plot of random normal data\n\n\n\n\nThe ACF plot of our white noise series is shown in Figure 53. As you would expected, the ACFs are small in size and not significant. The ACFs also do not die out in any systematic pattern too.\n\n\nCode\nggAcf(whtnoise)\n\n\n\n\n\nFigure 53: ACF plot of random normal data\n\n\n\n\nIt is the practice to assume a typical structural interrelationship between the trend, seasonal or cyclical components of a time series. This may be done using an additive model for the observation at time \\(t\\) say \\(X_t\\) as\n\\(X_t\\) = Trend + Seasonal + Error\nor, using the multiplicative model,\n\\(X_t\\) = Trend \\(\\times\\) Seasonal + Error.\nThe multiplicative model takes the interaction between the components into account whereas the additive model assumes independence of the components. We have not explicitly considered the cyclical component of variation in the models and assume that the either the trend or the seasonal part of the model will include it.\nConsider the UV series, which is a short time series. Hence this series may not reveal secular (long term) trend or cycles. Figure 54 shows the decomposition done for the UV series. While this plot is useful in revealing the seasonality, trend component is probably over-fitted.\nThe associated standard error bars also appear on the RHS side of the plot and the trend estimation is done poorly after all.\n\n\nCode\nuv |&gt; \n  decompose(type=\"additive\") |&gt;  \n  forecast::autoplot() + \n  ggtitle(\"\")\n\n\n\n\n\nFigure 54: Classical decomposition for UV data\n\n\n\n\nThe classical decomposition done for the credit card balances data is shown in Figure 55. For this series, the trend component is strong and estimated well. The seasonal component is weaker and not estimated well at all.\n\n\nCode\ncredit.balance |&gt; \n  decompose(type=\"additive\") |&gt; \n  forecast::autoplot() + \n  ggtitle(\"\")\n\n\n\n\n\nFigure 55: Classical decomposition for credit data\n\n\n\n\nThere are many approaches to decomposing the components of a time series. Some are designed to improve forecasting. These methods are covered in higher level statistics courses.\nBy the term detrending, we mean the process of removing the trend from a time series. This can be model based. For example, we may fit a S-type logistic model and the residuals of this model is called the detrended series, which mainly contains the seasonal and error components of variation. The other approach to detrending is to apply differencing which is discussed later on. There are also nonparametric approaches to fitting trends. For example, the moving average smoothing (discussed in a later Chapter) can be treated as fit (filter) and the residuals after smoothing can be regarded as detrended data.\nBy the term deseasoning we mean the process of removing the seasonal component of the variation in a time series. For example, we may use a regression approach to deseason the series after fitting the model with seasonal indicator variables. We may also use seasonal differencing or use trigonometric functions for deseasoning.\nIt is not always easy to completely separate trends from seasonal effect but time series decomposition helps us to understand the underlying complexities.\nR packages are available for exploring spatial data, particularly for integrating with maps. Some software programs also employ audio to augment exploration of higher dimensional data. Plots for EDA are introduced at an introductory level in this Chapter but there are many dedicated R packages available for conducting EDA in a specialised area such as finance or ecology."
  },
  {
    "objectID": "studyguide/4-inference.html",
    "href": "studyguide/4-inference.html",
    "title": "Chapter 4: Statistical Inference",
    "section": "",
    "text": "“All models are wrong, but some are useful.”\n– George Box\nThis chapter provides an introduction to statistical inference. Many of the concepts in this chapter should be familiar to you because they are covered in all first-year statistics courses."
  },
  {
    "objectID": "studyguide/4-inference.html#populations-and-parameters-samples-and-statistics",
    "href": "studyguide/4-inference.html#populations-and-parameters-samples-and-statistics",
    "title": "Chapter 4: Statistical Inference",
    "section": "Populations and parameters, samples and statistics",
    "text": "Populations and parameters, samples and statistics\nStatistical inference is a fundamental concept in statistics. The vast majority of statistical analyses that you will do as an undergraduate involve statistical inference. Anything involving p-values, confidence intervals, or standard errors are a form of statistical inference.\n\n\n\n\n\n\nStatistical inference is:\n\n\n\nthe use of information from a sample to make statements about a population.\n\n\nAs discussed in previous chapters, most datasets contain information about a sample from a population, rather than the whole population of interest. For example (Figure 1), say we owned a fish farm, and we wished to know the average length of the fish in our farm. Let’s say we had 2,000 fish in our farm. It would be too time-consuming to catch and measure every single fish. Instead, we take a random sample of, say, 10 fish, measure their lengths, and calculate the mean.\nRemember, our goal here is to know something about the whole population of 2,000 fish. We don’t really care about the 10 fish in our sample. It is no use to say “Well, I’ve no idea about the average length of my whole population fish, but you see those 10 fish there? They average 36.7 cm in length.”. We only care about the 10 fish in our sample in so far as they tell us something about the broader population. We use the average length of the fish in our sample as and estimate of the average length of fish in the population. This is statistical inference: using information from a sample to make conclusions about a population.\n\n\n\nFigure 1: Statistical inference from a sample to a population of fish\n\n\nTo clarify some terminology using the example in Figure 1:\n\nThe population is all 2,000 fish in our farm.\nThe sample is the 10 fish we happened to measure.\nThe parameter of interest (often denoted \\(\\mu\\), if it is a mean, or \\(\\theta\\) more generally) is the average length of the fish in the population of 2,000. Population parameters are usually considered to be fixed and unknown values.\nThe statistic (often denoted \\(\\bar{y}\\) or \\(\\hat\\mu\\), if it is a mean, or \\(\\hat\\theta\\) more generally) is the average of the 10 lengths of the fish in our sample. Unlike population parameters, which are fixed and unknown, sample statistics are random variables.\nStatistical inference in this case is the use of the sample mean \\(\\bar{y}\\) as an estimate of the population mean \\(\\mu\\).\n\nThe fact that we’ve only measured lengths from a sample rather than the whole population necessitates statistical inference. If we’d measured every fish in the farm, we wouldn’t need statistical inference, because we’d know precisely the population parameter (assuming negligible measurement error)."
  },
  {
    "objectID": "studyguide/4-inference.html#sampling-error",
    "href": "studyguide/4-inference.html#sampling-error",
    "title": "Chapter 4: Statistical Inference",
    "section": "Sampling Error",
    "text": "Sampling Error\nThis brings us to the next important concept of statistical inference: sampling error. A consequence of having collected data from a sample rather than the whole population is that there is uncertainty in our knowledge of the population parameter. Our sample mean is an estimate of the population mean; if we wanted to know population mean with zero uncertainty, we’d have to measure all the fish. This is the trade-off of sampling. It’s a lot cheaper to sample, but we sacrifice certainty.\nThe practical application of statistical inference involves (1) making estimates and (2) quantifying the uncertainty of those estimates. Uncertainty is often quantified using standard errors, confidence intervals, and P-values. All these quantities relate to sampling error. They’re all expressions of the uncertainty of an estimate of a population parameter.\nIn the fish farm example (Figure 1), sampling error is the hypothetical variation in the means of the lengths of samples of fish, with a sample size of \\(n\\) = 10. That is, if we were to (hypothetically) repeat the scientific process (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), over and over again, how much would those sample means vary? That variation of sample statistics is sampling variation, or sampling error. And understanding sampling variation is the key to understanding most of undergraduate statistics.\nSo, when we do our study (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), we are drawing one value of the sample mean, \\(\\bar y\\), from a random variable, \\(\\bar Y\\), which is the distribution of sample means that we could hypothetically draw.\nGiven this random sampling variation, here are some explanations for some commonly used measures of uncertainty:\n\nA standard error is simply the standard deviation of a statistic under repeated sampling–that is, how much it would vary (hypothetically) from sample to sample.\nA confidence interval is a pair of numbers that contain the true value of the population parameter with 95% confidence. It is a simple function of the sample statistic and its standard error.\nA p-value is the probability of obtaining a sample statistic as or more extreme than the one observed, given a particular hypothesised value (usually zero) of the population parameter.\n\nDon’t worry if those definitions aren’t completely clear to you right now, but I encourage you to refer back to this section again and again as you learn about them in more detail during the rest of this course.\nKeep sampling error front of mind whenever you see a standard error, a confidence interval, or a p-value.\nNow, we’ll introduce some specific inference methods."
  },
  {
    "objectID": "studyguide/4-inference.html#example-rangitikei",
    "href": "studyguide/4-inference.html#example-rangitikei",
    "title": "Chapter 4: Statistical Inference",
    "section": "Example: rangitikei",
    "text": "Example: rangitikei\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_minimal())\n\n\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\",\n  destfile = \"rangitikei.RData\")\n\nload(\"rangitikei.RData\")\n\n\n\n\nCode\nshapiro.test(rangitikei$people)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rangitikei$people\nW = 0.65346, p-value = 1.382e-07\n\n\nThe low \\(p\\)-value here indicates a significant departure from normality. We conclude that there is very strong evidence against the null hypothesis that we the population is normally distributed. (Remember, always express your conclusions by reference to the population, not the sample, or even “the data”.)\nWe can examine a Q-Q plot (Figure 2), which plots the observed values (\\(y\\)) against the theoretical values if the population were normally distributed. Departure from the diagonal line indicates departure from normality.\n\n\nCode\np1 &lt;- ggplot(rangitikei) + \n  aes(sample = people) + \n  stat_qq() +\n  stat_qq_line() \n\np2 &lt;- ggplot(rangitikei) + \n  aes(y=people, x=\"\") +\n  geom_boxplot() +\n  xlab(\"\") +\n  coord_flip()\n\ngridExtra::grid.arrange(p1, p2, ncol=1) \n\n\n\n\n\nFigure 2: Distribution of people\n\n\n\n\nThe same conclusion is drawn with the Kolmogorov-Smirnov test. Note that this test does not allow ties and can be used to test the fitting of non-normal distributions.\n\n\nCode\nks.test(rangitikei$people, \"pnorm\")\n\n\nWarning in ks.test.default(rangitikei$people, \"pnorm\"): ties should not be\npresent for the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  rangitikei$people\nD = 0.99997, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nIt is often informative to analyse data by fitting a statistical model. The idea is to look for real patterns, “signals” amongst the “noise” of individual variation, patterns that would reoccur in other, hypothetical samples we might have drawn from the population. We often try to approximate patterns by fitting a “statistical model”. A A statistical model usually comprises a mathematical formula describing the relationships among variables, along with a probabilistic description of the variation of the data around the formula. If the statistical model is a good approximation, it serves as a neat way of describing the system that generated the data, and we can use such a model to predict future values of the variables."
  },
  {
    "objectID": "studyguide/4-inference.html#example-testmarks",
    "href": "studyguide/4-inference.html#example-testmarks",
    "title": "Chapter 4: Statistical Inference",
    "section": "Example: testmarks",
    "text": "Example: testmarks\nThe data set tv consists of the time that 46 school children spent watching television. Before fitting a model to the data, it is a good idea to see whether the data approximately follows a Normal distribution using a normal Q-Q Plot; see Figure 3. The points plotted fall pretty much along the line, suggesting at least approximate Normality.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/tv.RData\", \n  destfile = \"tv.RData\")\n\nload(\"tv.RData\")\n\n\n\n\nCode\nP.val &lt;- tv$TELETIME |&gt; \n  shapiro.test() |&gt; \n  pluck('p.value') |&gt; \n  round(digits = 3)\n\np1 &lt;- ggplot(tv) + \n  aes(sample = TELETIME) + \n  stat_qq() +\n  stat_qq_line() +\n  labs(caption = paste(\"Shapiro Test P value\", P.val))\n\np2 &lt;- ggplot(tv) + \n  aes(y=TELETIME, x=\"\") +\n  geom_boxplot() +\n  xlab(\"\") +\n  coord_flip()\n\ngridExtra::grid.arrange(p1, p2, ncol=1) \n\n\n\n\n\nFigure 3: Distribution of TV viewing times\n\n\n\n\nFigure 3 shows the boxplot of the data. The boxplot again suggests a very mild skew to the left but the middle 50% data show right skewness. However the whiskers are about the same length and there are no outliers. There is a difference of 31 between the mean and median, suggestive of a slight skew to the lower values. However this difference is small given the overall variability (standard deviation is 567.9, and the range is 2309) so we can probably ignore the observed skew. However we will look for any further evidence of skewness, since this could invalidate any inference we make based on the Normal distribution (at least it would if we had a smaller sample). The TV viewing time data also passes normality tests such as Shapiro-Wilk test. All told, we conclude that the normal model describes the distribution of these data fairly well.\nAs indicated earlier, a large number of naturally occurring measurements, such as height, appear to follow a Normal distribution so that a considerable amount of theory has been built on this distribution.\nTo recapitulate, Normal (or Gaussian) curves are determined by just two numbers, one indicating the location and the other the spread. Although there are an infinite number of Normal curves, their shapes are similar and, of course, the area under each curve is 1. Indeed, the location and spread parameters are the only differences between curves.\nIt is usual to take the measure of location as the mean, denoted by \\(\\mu\\), and the measure of spread as the standard deviation, denoted by \\(\\sigma\\). If a variable, Y, follows a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we write Y \\(\\sim\\) N(\\(\\mu\\), \\(\\sigma\\)) where \\(\\sim\\) means is distributed as (The squiggly symbol \\(\\sim\\) is known as tilde).\nNote, however, that the Normal distribution, like all statistical distributions, is a theoretical concept and no naturally occurring measurement will exactly follow a probability distribution model. For one thing, any measurement is finite whereas the Normal curve is continuous in the interval \\(\\left(-\\infty ,\\infty \\right)\\). Also, the curve is asymptotic to the \\(X\\)-axis so that any range of values of \\(Y\\) however large or small will have a certain probability according to the Normal distribution, but in practice there will be limitations such as that a person’s blood pressure must be greater than zero.\nSuppose we didn’t know that \\(\\mu\\) = 80 and \\(\\sigma\\) = 12. The obvious estimator of \\(\\mu\\), based solely on the sample, is the sample mean \\(\\bar{y}\\), and the obvious estimator of \\(\\sigma\\) is the sample standard deviation \\(S\\). Note that by estimator we don’t mean the observed value based on the particular sample. Rather the word estimator means the mathematical formula or procedure that we use to produce our estimates, namely \\(\\bar{y}={\\frac{1}{n}} \\sum y\\) and \\(S=\\sqrt{{\\frac{1}{n-1}} \\sum _{i=1}^{n}(y_{i} -\\bar{y})^{2} }\\).\nThe point is that there can be several alternative procedures for estimating the same parameters \\(\\mu\\) and \\(\\sigma\\), and in particular samples the actual computed estimates may be the same or different. For example, since the mean and median are the same for Normal data, we could estimate \\(\\mu\\) by the sample median, namely 81.313 for our example. This different procedure has given rise to a different number, and if we didn’t know the answer we would not know which estimate to use. The median estimate has a lot of attraction, since the median is robust, that is, is not affected by outliers.\nClearly all these estimates are close to the true parameter values, but not the same. A statistical question relates to how close estimates are to their true values in general. We usually can’t answer this question about the actual estimates (observed numbers) since we usually don’t know the correct answer. One approach, which is often used to test procedures in research, is to try a number of simulations and see which approach produces the closest results on average. We can also give error bounds that say, for example, that 95% of the time the estimator is within such-and-such a distance of the true parameter. This leads to the idea of using probability or so-called ‘confidence’. By making probability statements about the estimators we can say something useful about how trustworthy the particular estimates are also. We use standard errors to measure the trustworthiness of the estimators. The standard error is the standard deviation of the estimator, so the smaller the standard error the better.\nWithout going into details, it turns out that \\(\\bar{y}\\) has the smallest possible standard error for any unbiased estimator of \\(\\mu\\) for normal data. (An unbiased estimator is one that is not systematically too big or too small.) While the same is not true of \\(S\\) in relation to \\(\\sigma\\), the latter does have other useful mathematical properties. So these are some reasons for using these formulae so routinely."
  },
  {
    "objectID": "studyguide/4-inference.html#hypothesis-testing-for-mean",
    "href": "studyguide/4-inference.html#hypothesis-testing-for-mean",
    "title": "Chapter 4: Statistical Inference",
    "section": "Hypothesis testing for mean",
    "text": "Hypothesis testing for mean\nFor testing the mean of a population, the fact that we have specified a parameter value in \\(H_0\\) enables us to specify a probability distribution, for example that the data are a random sample from N(\\(\\mu_0\\), \\(\\sigma\\)). This immediately implies that\n\\(t = \\frac{\\bar{y}-\\mu _{0} }{S/\\sqrt{n} } \\sim t{}_{n-1}\\)\nNow if the hypothesis \\(H_0\\) is true, we should have \\(\\mu\\cong\\mu_0\\) . The notation equal-with-squiggle, \\(\\cong\\), stands for “approximately equal to” ), so that \\(t\\) should generally be close to 0. If the hypothesis is false we should get either much less than \\(\\mu_0\\) or much greater than \\(\\mu_0\\), in which case \\(t\\) should be large, out in the tails of the \\(t_{n-1}\\) distribution. Since we have a probability distribution, we can quantify just how unlikely our particular \\(t\\) is by comparing our sample value with the distribution.\nLet’s make things more specific by considering the television example. The degrees of freedom are \\(n-1 = 45\\). We have already indicated that \\(t\\cong0\\) implies \\(\\mu\\cong\\mu_0\\), in other words that the data matches the hypothesis very closely. But suppose instead we observed \\(t= 0.68\\). Could we regard this as an unusually large value of \\(t\\), that is, as evidence against \\(H_0\\)? The answer is no! The reason is that 0.68 is the upper quartile of the \\(t_{45}\\) distribution; in other words half (50%) of the time we would see values of \\(t\\) either greater than 0.68 or less than -0.68, even if the hypothesis \\(H_0\\) is true. Now what if \\(t\\) were below -1.6794? Would that be regarded as an unusual amount of discrepancy between \\(t\\) and \\(\\mu\\), that is as evidence against \\(H_0\\)? The answer is maybe. The fact is that 10% of the time one sees \\(t\\) \\(&lt;-1.6794\\) or \\(t&gt;1.6794\\), even if \\(H_0\\) is true. So if we use this rule, we have a 10% chance of wrongly rejecting \\(H_0\\). Few New Zealanders would feel happy with a legal system that allowed a 10% chance of wrongfully convicting an innocent person. Finally, what if we observe \\(t= -2.6896\\)? Only 1% of the area under the \\(t_{45}\\) curve lies outside the interval -2.6896 to +2.6896, so we would conclude that such a value of \\(t\\) was quite unlikely. This then would be strong evidence against \\(H_0\\).\nSuppose now we test the extremely unlikely hypothesis \\(H_0:\\mu = 0\\). Since \\(S/\\sqrt n=83.7\\), we obtain \\(t = (1729.3-0)/83.7 = 20.65\\). This is far larger than could be expected by chance, so the \\(t\\) statistic provides clear evidence against \\(H_0\\).\nA more realistic test may be whether \\(H_0:\\mu = 1500\\). Perhaps a previous study found an average time of viewing of 1500 minutes per week, and we want to check this. The test statistic is: \\(t=(1729.3-1500)/83.7=2.74\\). This is just outside our 1% bounds established earlier, so we conclude the data and \\(H_0\\) do not seem to agree. We reject \\(H_0:\\mu=1500\\).\nFinally suppose we test the hypothesis \\(H_0:\\mu=1600\\). Again this hypothesis could have been prompted by a previous study. Then \\(t=(1729.3-1600)/83.7=1.54\\). This is within the interval -1.6794 to 1.6794, suggesting that 1.54 is not an unusually highly value since it is exceeded more than 10% of the time when \\(H_0\\) is true. We would not reject the hypothesis \\(H_0:\\mu = 1600\\).\nWhen statistical test of hypothesis is done using R software, we rely on the \\(p\\)-value displayed.\n\n\nCode\nt.test(tv$TELETIME, mu=1500)\n\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 2.7382, df = 45, p-value = 0.008818\nalternative hypothesis: true mean is not equal to 1500\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283 \n\n\nSo what is a \\(P\\)-value? Formally, A P-value is the probability of observing data as extreme or more extreme as the data you actually observed, if \\(H_0\\) is true. This sounds like a very abstract and difficult concept to grasp, but it’s in fact exactly the rule we have been using. We saw that 50% of the time, \\(t&lt;-0.68\\) or \\(t&gt;0.68\\). So the \\(p\\)-value for a \\(t=0.68\\) is 0.5. We saw that 10% of the time, \\(t&lt;-1.6794\\) or \\(t&gt;1.6794\\). So the \\(p\\)-value of \\(t= -1.6794\\) is 0.1. And we saw that 1% of the time \\(t&lt;-2.6896\\) or \\(t&gt;2.6896\\), so the \\(p\\)-value of \\(t=2.6896\\) is 0.01. What is the \\(p\\)-value of \\(t= 2.74\\)? The answer is 0.009 or just under 1%. What is the \\(p\\)-value of \\(t= 1.54\\)? Answer is 0.130.\nUsually, we reject \\(H_0\\) in favour of \\(H_{1}\\) if the \\(p\\)-value of the data is \\(&lt; 0.05\\). In this case the test is said to be significant, and the 0.05 is called the significance level of the test. Otherwise (when we don’t reject \\(H_0\\)) we call the test non-significant. Some refer to a \\(p\\)-value \\(&lt;0.01\\) as very significant or highly significant. In journal articles and published tables of results, the three case non-significant, significant, and highly significant are often abbreviated as NS, * and ** respectively. The issues of using a standard cut-off of 5% for significance level, largely insisted in journals, has had its unintended consequences. The false discovery in science can be avoided if P values are not used as the sole criterion to draw conclusions. Read the advice on P values presented next.\nAdvice on the use of P values\nUnfortunately the P-values are often misunderstood in practice. The advice issued by the American Statistical Association (https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf) is noteworthy:\n\nThe statement’s six principles, many of which address misconceptions and misuse of the p-value, are the following:\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\nIn order to fully appreciate the reasons behind the above advice, we need learn more theory but you should be able to understand the following distinction between statistical significance and practical significance. A hypothesis test may suggest that the estimated size of the effect is not big enough compared to the effect that can occur due to errors under the assumed model. A small effect, particularly if it is known to be caused by a variable, can be of practical importance and can contribute to scientific knowledge. The opposite scenario is also possible. We may find a small difference to be statistically significant because of the large sample size but such a difference may not be of much practical significance.\nA hypothesis test has a certain power (probability) to reject the null hypothesis when it is false. This power (probability) is a function of the sample size and the unknown parameters of the probability model adopted for testing.\nAssume that we are testing the null hypothesis \\(H_0:\\mu = 0\\) using the null model \\(N(0,1)\\). The power of the one sample t test can be evaluated using the R function power.t.test() for a given \\(\\delta\\), the difference in the true mean and what was hypothesised under \\(H_0\\). For example, the power of the \\(t\\)-test for \\(n=30\\) is lower than the power when \\(n=50\\) (say) when other settings are the same. Try-\n\n\nCode\npower.t.test(n = 30, delta = 1, sd = 1, sig.level = 0.05)\npower.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)\n\n\nThe power to detect a small change in the mean is often low. Try-\n\n\nCode\npower.t.test(n = 30, delta = .25, sd = 1, sig.level = 0.05)\npower.t.test(n = 30, delta = 1, sd = 1, sig.level = 0.05)\n\n\nThere is a trade-off between the significance level (Type I error or false positive) and Type II error or false negative (=1-power) probabilities. Try-\n\n\nCode\npower.t.test(n = 30, delta = 0.5, sd = 1, sig.level = 0.05)\npower.t.test(n = 30, delta = 0.5, sd = 1, sig.level = 0.01)\n\n\nWhen we test many hypothesis in tandem, we are more concerned on the overall or family-wise error rates. The issues of false discovery in science is discussed in a later section.\nP hacking is a phrase used when a particular test or a meta procedure is deliberately chosen either to ensure a low p-value or just to achieve a value below 0.05."
  },
  {
    "objectID": "studyguide/4-inference.html#hypothesis-tests-for-two-groups",
    "href": "studyguide/4-inference.html#hypothesis-tests-for-two-groups",
    "title": "Chapter 4: Statistical Inference",
    "section": "Hypothesis tests for two groups",
    "text": "Hypothesis tests for two groups\nThe null hypothesis is that population means of the two groups are equal (written as \\(H_0:\\mu_1 = \\mu_2\\) or \\(H_0:\\mu_1-\\mu_2=0\\)). The alternative hypothesis is that the population means of the two groups are different (i.e., \\(H_0:\\mu_1\\neq \\mu_2\\) or \\(H_0:\\mu_1-\\mu_2\\neq0\\)). Note that when the two population means are different either \\(\\mu_1&gt;\\mu_2\\) or \\(\\mu_1&lt;\\mu_2\\).\nThe difference in sample means \\(\\bar{y}_{1} -\\bar{y}_{2}\\) can be standardized to give a \\(t\\) statistic:\n\\(t\\) = ((\\(\\bar{y}_{1} -\\bar{y}_{2}\\))- expected)/e.s.e.\nThe test statistic is \\((\\bar{y}_{1} -\\bar{y}_{2})\\), which has the expected value of zero under the null hypothesis.\nThe estimated standard error (e.s.e) of (\\(\\bar{y}_{1} -\\bar{y}_{2}\\)), is obtained in two ways depending on whether it is plausible or not to make the assumption that the variances within the populations are the same, (i.e. \\(\\sigma _{1}^{2} =\\sigma _{2}^{2}\\)). Whether this assumption appears to be tenable or not can be explored using boxplots etc. For the television viewing time example, the variances of the TV viewing times do not appear to be the same for boys and girls. If the variances of the two populations are the same, then we will use a method of combining the individual variances of the groups to form a pooled variance estimate. To do this, we cannot simply average the two variances as the sample sizes may be quite different. A weighted sum is called for to give:\npooled estimate of variance, \\(S_{p}^{2} =w_{1} S_{1}^{2} +w_{2} S_{2}^{2}\\)\nwhere the weights are, \\(w_{1} =\\frac{n_{1}-1}{n_{1} +n_{2}-2}\\) and \\(w_{2} =\\frac{n_{2}-1}{n_{1} +n_{2}-2}\\). Hence the pooled estimate of the standard deviation is given by\n\\(S_p = \\sqrt{ w_1S_{1}^2 + w_2S_{2}^2 }\\) or\n\\[S_{p} =\\sqrt{\\frac{\\left(n_{1}-1\\right)S_{1}^{2} +\\left(n_{2}-1\\right)S_{2}^{2} }{n_{1} +n_{2} -2} }\\] Consequently,\nEstimated standard error (\\(\\bar{y}_{i}\\))=\\(\\frac{S_{p}}{\\sqrt{n_{i}}},~~~i = 1, 2\\)\nso that\nEstimated standard error (\\(\\bar{y}_{1}-\\bar{y}_{2}\\))=\\(S_{p} \\sqrt{1/n_{1}+1/n_{2}}\\)\nIf the variances of the two populations are not the same, then we cannot pool the variances. Hence the estimated standard error for the difference in the two sample means is given by\nEstimated standard error (\\(\\bar{y}_{1}-\\bar{y}_{2}\\))=\\(\\sqrt{\\frac{S_{1}^{2}}{n_{1}} +\\frac{S_{2}^{2}}{n_{2}}}\\)\nThe degrees of freedom for our \\(t\\)-test (called the two-sample \\(t\\) test) depends on whether estimated standard error is based on the pooled variance or not. For the variance pooled case, the \\(df\\) for the \\(t\\)-test is \\(n_{1}+n_{2}-2\\) but becomes smaller for the unpooled case to \\[df=\\frac{\\left(\\frac{S_{1}^{2}}{n_{1}} +\\frac{S_{2}^{2} }{n_{2}} \\right)^{2} }{\\frac{1}{n_{1} -1} \\left(\\frac{S_{1}^{2}}{n_{1}}\\right)^{2} +\\frac{1}{n_{2} -1} \\left(\\frac{S_{2}^{2}}{n_{2} } \\right)^{2}}\\] That is, the \\(df\\) is adjusted according to the ratio of the two variances. The \\(t\\)-test is also approximate when the variances are not pooled and hence it would be advisable to perform a transformation as later outlined in this Chapter.\nHand calculations for the two sample \\(t\\)-test are cumbersome. The test done on computer usually gives us an output which contains the \\(t\\)-statistic value and the associated \\(p\\)-value. Supplementary details such as the standard errors of the sample means, and their difference, associated \\(df\\) etc will also be contained. The R output given below shows the two-sample test results for the TV viewing data set. Here we test whether the true mean TV viewing times are the same for boys and girls.\n\n\nCode\nt.test(TELETIME~SEX, data=tv)\n\n\n\n    Welch Two Sample t-test\n\ndata:  TELETIME by SEX\nt = -0.7249, df = 40.653, p-value = 0.4727\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -462.1384  218.0514\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304 \n\n\nBased on the EDA evidence seen in Figure 6, we may take a conservative stand and prefer the unpooled two-sample \\(t\\)-test (which is also known as Welch Two Sample t-test). The \\(t\\)-value of -0.72 is not unusual as the probability of getting such an extreme value under the null hypothesis is \\(p=0.47\\). In other words, we cannot reject the null hypothesis; we accept it until we have more evidence to the contrary. Hence the conclusion of the \\(t\\)-test is that the mean TV viewing times can be regarded as the same for the population of boys and girls. Alternatively there is no statistically significant gender effect on TV watching for boys and girls of Standards 2 to 4."
  },
  {
    "objectID": "studyguide/4-inference.html#confidence-intervals-for-the-difference-in-means",
    "href": "studyguide/4-inference.html#confidence-intervals-for-the-difference-in-means",
    "title": "Chapter 4: Statistical Inference",
    "section": "Confidence Intervals for the Difference in Means",
    "text": "Confidence Intervals for the Difference in Means\nThe 95% Confidence Interval for the difference \\(\\left(\\mu _{1} -\\mu _{2} \\right)\\) in population means is given by:\ndifference in sample means \\(\\pm t \\times\\) e.s.e .\nOr more specifically,\nInterval estimate for \\((\\mu _1-\\mu _2)\\) = \\((\\bar{y}_{1}-\\bar{y}_{2})\\pm t \\times\\) e.s.e.\\((\\bar{y}_{1} -\\bar{y}_{2})\\).\nBased on the \\(t\\) quantile value of 2.021 for 40 \\(df\\), the CI in the unpooled case is\n\\(-122 \\pm2.021\\times\\sqrt{\\frac{648^{2}}{23}+\\frac{482^{2}}{23}}\\) or \\((-462.3, 218.2)\\)\nThe \\(t\\)-test output for the null hypothesis \\(H_{0}:\\mu _1=\\mu _2\\) (or \\((\\mu _1-\\mu _2)= 0)\\) gives the confidence interval too. Notice that the CI actually includes zero as a possible value. This means that it is possible \\((\\mu_1-\\mu_2)=0\\); so we cannot reject the null hypothesis."
  },
  {
    "objectID": "studyguide/4-inference.html#paired-t-test",
    "href": "studyguide/4-inference.html#paired-t-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Paired \\(t\\) test",
    "text": "Paired \\(t\\) test\nNote that the two-sample data may be simply paired observations. For instance, a measurement may be made on the left and right eyes of the same person. If observations are paired in some way, a one-sample \\(t\\)-test on the difference \\(\\left(X_{i} ,Y_{i} \\right)\\) will suggest whether the true mean of the differences can be regarded as zero or not. Such a test will be more powerful than a two sample \\(t\\)-test because of the correlation between the paired observations. If the correlation is weak, it is desirable to ignore the pairing variable and perform a two-sample \\(t\\)-test.\nConsider the maths and English test scores of students available in the data set testmarks. These test scores are paired being the scores of the same student. The correlation or linear relationship between the maths and English scores is high; see Figure 7.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData\",\n  destfile = \"testmarks.RData\")\n\nload(\"testmarks.RData\")\n\n\n\n\nCode\nlibrary(GGally)\nggpairs(testmarks)\n\n\n\n\n\nFigure 7: Relationship between Maths and English scores\n\n\n\n\nThe paired \\(t\\)-test or the one-sample \\(t\\)-test on the difference in scores gives a \\(t\\)-statistic of 0.17 (\\(p\\)-value of 0.868). This means that the true average difference in test scores can be regarded as zero or alternatively the true mean scores of maths and English can be regarded as equal.\n\n\nCode\nt.test(testmarks$Maths, testmarks$English, paired=T)\n\n\n\n    Paired t-test\n\ndata:  testmarks$Maths and testmarks$English\nt = 0.16745, df = 39, p-value = 0.8679\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.154646  4.904646\nsample estimates:\nmean difference \n          0.375"
  },
  {
    "objectID": "studyguide/4-inference.html#transformation-and-shape",
    "href": "studyguide/4-inference.html#transformation-and-shape",
    "title": "Chapter 4: Statistical Inference",
    "section": "Transformation and shape",
    "text": "Transformation and shape\nWhen considering data, we may decide to use the measurements as they are, or we may rescale them. For example, we may change them to percentages of the total. As a simple example, consider a town with four stores in which the weekly turnovers are one, two, four and eight thousand dollars. These could be rescaled to percentages of the total (which is 15).\n\n\n\n\n\n\n\n\n\n\nRaw data\n1\n2\n4\n8\n\n\nData in %\n1/15 \\(\\times\\) 100= 6.7%\n2/15 \\(\\times\\) 100 = 13.3%\n4/15 \\(\\times\\) 100= 26.7%\n8/15 \\(\\times\\) 100= 53.3%\n\n\n\nIf you were to compare a dotplot of the original data with a dotplot of the rescaled data you would find that the shape of the data had not changed by this rescaling. That is, the second percent is twice the first and the second weekly turnover is almost twice the first; the third percent is four times the first and twice the second and so on. This is an example of a linear transformation.\nA linear transformation is one that can be described by the formula, \\(y=a+bx\\) for certain constants \\(a\\) and \\(b\\), and where \\(x\\) is the old data and \\(y\\) is the transformed data. The key thing about linear transformations is that they do not change the shape of a dotplot, only the scale. Another linear example is converting temperature data from Fahrenheit (\\(x\\)) to centigrade \\(y = 5(x-32)/9\\). Boxplots of the temperatures would look the same even though the scale was altered.\nAnother way of rescaling the store example would be to calculate the weekly turnover of a store divided by the number of employees in that store, to give weekly turnover per employee. Even though this looks like a linear transformation it is not, since the relative positions of the four stores on a scale would change depending on the number of employees. If we have two or more variables (e.g. turnover, employees) it is often useful to look at ratios like this to seek simple explanations of the data.\n\n\nCode\nrht &lt;- data.frame(RHT=rbeta(1e3, 1,5))\n\np1 &lt;- ggplot(rht) + \n  aes(RHT) +\n  geom_histogram() +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"(a) Needs a Shrinking Transformation\")\n\np2 &lt;- ggplot(rht) +\n  aes(y=RHT, x=\"Right Skewed Data\") +\n  geom_boxplot() +\n  xlab(\"\") + ylab(\"\") +\n  coord_flip()\n\nlht &lt;- data.frame(LHT=rbeta(1e3, 5,1))\n\np3 &lt;- ggplot(lht) +\n  aes(LHT) +\n  geom_histogram() +\n  xlab(\"\") + ylab(\"\") + \n  ggtitle(\"(a) Needs a Stretching Transformation\")\n\np4 &lt;- ggplot(lht) +\n  aes(y=LHT, x=\"Left Skewed Data\") +\n  geom_boxplot() + \n  xlab(\"\") + ylab(\"\") +\n  coord_flip()\n\ngridExtra::grid.arrange(p1, p3, p2, p4, ncol=2) \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8: Transformations\n\n\n\n\nIn this chapter, we focus on transformations involving just one variable (though perhaps more than one batch of data on that variable). Our goal is to change the shape of a distribution to make it more symmetric. Consider the two distributions in Figure 8 in which (a) is skewed to the left and (b) to the right. These could be made more symmetric by stretching the large values in (a) but shrinking them in (b)."
  },
  {
    "objectID": "studyguide/4-inference.html#the-ladder-of-powers",
    "href": "studyguide/4-inference.html#the-ladder-of-powers",
    "title": "Chapter 4: Statistical Inference",
    "section": "The Ladder of Powers",
    "text": "The Ladder of Powers\nFrom now on we assume that the data are very skewed, and we wish to transform it, (or, in Tukey’s terms, re-express it) to be as symmetrical as possible. A simple approach considers the data raised to different powers, that is, if the original data are \\(x\\) and the new data are \\(y\\),\n\\[y =\\left\\{\\begin{array}{l} {\n\\text {sign} (\\lambda )x^\\lambda~~~~~ \\lambda \\ne 0} \\\\\n{\\log(x)~~~~~~~~~~~\\lambda = 0} \\end{array} \\right.\\]\nNote that the Greek letter \\(\\lambda\\) is pronounced as lambda. Here, (\\(\\text {sign}(\\lambda)\\)) is +1 if \\(\\lambda&gt;0\\), and \\(\\text {sign}(\\lambda)=-1\\) if \\(\\lambda&lt;0\\), for reasons discussed below. Some special cases of this power transformation are set out below:\n\n\n\n\n\n\n\n\n\nPOWER\nFormula\nName\nResult\n\n\n\n\n3\n\\(x^3\\)\ncube\nstretches large values\n\n\n2\n\\(x^2\\)\nsquare\nstretches large values\n\n\n1\n\\(x\\)\nraw\nNo change\n\n\n1/2\n\\(\\sqrt{x}\\)\nsquare root\nsquashes large values\n\n\n0\n\\(\\log{x}\\)\nlogarithm\nsquashes large values\n\n\n-1/2\n\\(\\frac{-1}{\\sqrt{x}}\\)\nreciprocal root\nsquashes large values\n\n\n-1\n\\(\\frac{-1}{x}\\)\nreciprocal\nsquashes large values\n\n\n\nRaising the data to the power of 1 does not change it at all; as we proceed down or up from 1, the strength of the transformation increases. The special case \\(\\lambda\\)=0 has to be handled differently since \\(x^0=1\\) for all non-zero \\(x\\). Instead we conventionally regard it as being equivalent to taking the natural logarithm because the transformation \\(\\frac{x^{\\lambda } }{\\lambda } -\\frac{1}{\\lambda }\\) is close to the logarithmic transformation if \\(\\lambda\\) is small. The ‘common’ logarithm to base 10 could be used but it just yields a constant multiple of the natural logarithm (ln = log to the base \\(e\\)). Now regarding the \\(\\text {sign}(\\lambda)\\): Notice that with two numbers, say 2 and 5, the reciprocal transformation would yield 0.5 and 0.2 so that, whereas the original numbers are increasing in size the transformed values are decreasing. To keep the order the same we take the negative of the reciprocal values, -0.5 and -0.2. These are again increasing. The same principle holds for all transformations where \\(\\lambda\\) is negative. \\(\\text {sign}(\\lambda)\\) is employed to keep the order the same as the raw data. (Alternatively we could divide by \\(\\lambda\\) which is consistent with the case of power zero that is the logarithm transformation).\n\n\nCode\np1 &lt;- ggplot(rangitikei) +\n  aes(y=vehicle^2, x=\"\") +\n  geom_boxplot() + \n  xlab(\"\") + \n  coord_flip() +\n  ggtitle(\"Square Transformation\")\n\np2 &lt;- ggplot(rangitikei) +\n  aes(y=vehicle, x=\"\") +\n  geom_boxplot() + \n  xlab(\"\") + \n  coord_flip() + \n  ggtitle(\"Raw Data\")\n\np3 &lt;- ggplot(rangitikei) +\n  aes(y=vehicle^.5, x=\"\") + \n  geom_boxplot() +\n  xlab(\"\") +\n  coord_flip() + \n  ggtitle(\"Square-root Transformation\")\n\np4 &lt;- ggplot(rangitikei) + \n  aes(y=log(vehicle), x=\"\") + \n  geom_boxplot() + \n  xlab(\"\") + \n  coord_flip() +\n  ggtitle(\"log Transformation\")\n\ngridExtra::grid.arrange(p1,p3,p2, p4, ncol=2) \n\n\n\n\n\nFigure 9: Effect of Transformations\n\n\n\n\nAs an example, the boxplots in Figure 9 represent the number of vehicles at the two Rangitikei river locations (from rangitikei). The first boxplot shows the square of the number of vehicle, which is highly right-skewed. The second boxplot shows the raw data (vehicle) which are still skewed towards the larger values. So to squash these to make the distribution more symmetric, the ladder of powers suggests we could try a square root transformation, or a stronger one such as the logarithm (or reciprocal root). These are shown in the third and fourth boxplots."
  },
  {
    "objectID": "studyguide/4-inference.html#checking-the-adequacy-of-a-transformation",
    "href": "studyguide/4-inference.html#checking-the-adequacy-of-a-transformation",
    "title": "Chapter 4: Statistical Inference",
    "section": "Checking the adequacy of a transformation",
    "text": "Checking the adequacy of a transformation\nSince the purpose of transforming the data is to make it more symmetric, we need to decide if we have achieved this goal. One approach which has been suggested is to consider the difference mean-median where the vertical lines stand for “the absolute value of” which means we ignore a negative sign if it occurs. If the batch is symmetric then the mean equals the median so that the difference mean - median= 0. We could design a measure, D, of symmetry by standardising it. That is, we divide this difference by a measure of spread - we could choose either the standard deviation or the F spread, whichever is at hand. These choices give the so-called D-Statistics:\nD1 = (mean - median)/ std.dev. or\nD2 = (mean - median)/F-spread.\nInstead of calculating the mean, we could use the mid-F to give\nD3 = (mid-F - median)/F-spread.\nWe first choose one of these criteria, and then apply it to all the transformations in turn, choosing as ‘best’ that transformation which gives the smallest absolute value of D (provided that other considerations do not disqualify it). Note that, in practice it is useful not to take absolute values when calculating the D-statistics, since a change of sign helps us to locate the zero of D, i.e. the direction of skewness of the data.\nThe following table gives the values of D1, D2, and D3 (with sign retained) for the four transformations.\n\n\n\n\n\n\n\n\n\n\nD-statistic\nvehicle\nvehicle\\(^2\\)\n\\(\\sqrt{vehicle}\\)\nloge(vehicle)\n\n\n\n\nD\\(_{1}\\)\n0.2724\n0.2831\n0.1308\n-0.0945\n\n\nD\\(_{2}\\)\n0.2973\n1.0615\n0.0978\n-0.0611\n\n\nD\\(_{3}\\)\n0.0714\n0.2273\n-0.0183\n-0.1092\n\n\n\n\n\nCode\nsource(\"http://www.massey.ac.nz/~anhsmith/161250/eda/lval.R\")\nlval(log(rangitikei$vehicle))\n\n\n\n\n\nFigure 10: Letter values of Vechicle data\n\n\n\n\nThe D-statistics D1 and D2 presented above suggest that the natural log would be the best transformation. But D3 values suggest in favour of the square root transformation. Note that D3 is sensitive only to skewness in the middle 50% of the data, whereas D1 and D2 are sensitive to skewness in the whole data set. The boxplots of Figure 9 would tend to confirm these observations on the D-statistics. Note also that \\(\\text {vehicle} ^2\\), \\(\\text {vehicle}\\) and \\(\\sqrt{\\text {vehicle}}\\) have produced positive values for the D-statistics indicating that these variables are skewed to the right (i.e. positive skewness) while \\(\\text {loge(vehicle)}\\) data are mildly negatively skewed.\nHaving chosen the transformation it is a good idea to check it graphically using a mids vs. spreads plot. The plot in Figure 10 shows a consistent upward trend in the untransformed (raw) data (right-skewed). Again a general upward trend is seen for squared data indicating that the skewness is not fully corrected. The square root data still shows the positive trend, which means that the transformation has failed to correct the skew in the non-middle 50% of the data. For logged data, the mids vs. spread plot shows a random scatter of points. This confirms the conclusion from the D statistic that the log transformation works well to correct the skew in the non-middle part of the data. Note that we may also compute the D-Statistics using the IQR in place of F-spread and the results will often be similar."
  },
  {
    "objectID": "studyguide/4-inference.html#some-words-of-caution-about-transformations",
    "href": "studyguide/4-inference.html#some-words-of-caution-about-transformations",
    "title": "Chapter 4: Statistical Inference",
    "section": "Some Words of Caution About Transformations",
    "text": "Some Words of Caution About Transformations\n\nThe D values can be compared for the same data set but we should be wary of comparing the D value from one data set with that of another (possibly transformed) data set.\nAny one statistic can never capture all the peculiarities of a data set so that the D values should be used in conjunction with other measures and/or graphs.\nIf the data set is small, transformations should be approached with some scepticism, for if more data were available the shape of the distribution may change.\nIt should be kept in mind that there are different levels at which data can be considered such as (a) just a collection of numbers (any reasonable transformation would suffice); (b) referring to physical quantities (certain transformations may make physical sense and allow meaningful interpretations to be made) and (c) outcomes from a certain process (for example, frequencies or counts may often suggest a Poisson distribution for which a square root transformation is suitable). The choice of a transformation may depend on the additional information that is known about the batch of data.\nCommon sense should prevail in this area as a transformation which brings only marginal improvement to symmetry, for example, should be balanced against other drawbacks such as the difficulty of interpreting the results. For example, the logarithm function turns multiplications to additions and powers to multiplications. Hence it turns divisions to subtractions and roots to divisions. Thus the geometric mean becomes the arithmetic mean and the ratio of geometric means becomes the difference between arithmetic means. Therefore reversing the transformation implies that a confidence interval for the difference between the means of the transformed data becomes a confidence interval for the ratio of the geometric means of the two groups of raw data.\nAlthough a transformation may lead to symmetry it may be better to consider other approaches. For example, the stem-and-leaf or dotplot display may show that there are at least two subgroups in the data. Transformations do not really solve the problem and we may have to subdivide the data into groups.\nNote that there are several other transformation functions available. For example, transformations such as arcsine are useful for proportion data."
  },
  {
    "objectID": "studyguide/4-inference.html#box-cox-normalising-transformations",
    "href": "studyguide/4-inference.html#box-cox-normalising-transformations",
    "title": "Chapter 4: Statistical Inference",
    "section": "Box-Cox Normalising transformations",
    "text": "Box-Cox Normalising transformations\nA systematic approach to power transformations was developed by Box and Cox (1976) and Box and Cox (1982). Their method produces a log-likelihood curve of possible values for the power \\(\\lambda\\). Without going into details, the higher the curve is for a particular value of \\(\\lambda\\), the more normal the transformed data will be. The plot of the log-likelihood curve of the Box-Cox method applied to the vehicle data is shown in Figure 11.\n\n\nCode\nlibrary(MASS, exclude = 'select')\n\nboxcox(rangitikei$vehicle ~ 1)\ntitle(\"Log-likelihood curve of Box-Cox power parameter\")\n\n\n\n\n\nFigure 11: Box-Cox Transformation\n\n\n\n\nThe curve peaks near zero indicating that a log transformation would be appropriate. In addition to the curve the plot contains a 95% confidence interval for the transformation parameter \\(\\lambda\\). The two vertical dotted lines are the endpoints of the confidence interval for \\(\\lambda\\). In this case the width of the confidence interval is quite small. The wider the confidence interval, the less obvious the choice for \\(\\lambda\\). If the confidence interval contains 1, then there is no need to perform a transformation. Note that the Box-Cox transformation is a normalising transformation. The EDA done in the previous sections relate to symmetry in the data.\nThe Box-Cox method, and other power transformations, should only be applied if the data are strictly positive. If all the data are negative one can of course take the absolute value of the data and then apply the transformation. If, however, only some of the data are not positive, we may add a constant to all of the data (to make the data positive) but the estimated power will vary depending on the constant."
  },
  {
    "objectID": "studyguide/4-inference.html#ranking-and-rank-correlation",
    "href": "studyguide/4-inference.html#ranking-and-rank-correlation",
    "title": "Chapter 4: Statistical Inference",
    "section": "Ranking and rank Correlation",
    "text": "Ranking and rank Correlation\nA nonparametric approach used very frequently, especially in the social sciences, is the Spearman’s Rank Correlation. To calculate it, first rank the \\(X\\) and \\(Y\\) variable, and then obtain usual correlation (the so-called Pearson correlation) coefficient. If there are a great many ties in the ranks then various corrections or modifications to the Spearman method have been suggested, but these are beyond the scope of this course. In principle then, one could simply apply the usual data analysis techniques to the \\(W= \\text {rank}(Y)\\) data and quote the \\(p\\)-values accordingly. We don’t usually do this in simple analyses, for reasons outlined below, but let’s explore this idea for a moment. Figure 17 shows the usual Pearson (lower diagonal) and Spearman rank correlations (upper diagonal) for the trees default R dataset.\n\n\nCode\nggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")), \n  lower = list(continuous = 'cor')\n  )\n\n\n\n\n\nFigure 17: Comparison of Pearson and Spearman rank correlations\n\n\n\n\nIt can be noted that the size of the estimates differ depending on the skew and relationship between the variables. For large samples it would be quite a reasonable approach, as the distribution of \\(W\\) values is symmetrical and therefore the usual data analysis methods - relying on \\(W\\) being Normally distributed - will work pretty well. The main difference to standard hypothesis tests would be that they would need to be expressed in terms of medians, say, rather than means.\nFor example, a two-group hypothesis test would be based on computing \\(W\\) for all the \\(n\\) data values together, and then comparing the mean of the \\(n_1\\) ranks in the first group of observations to the mean of the \\(n_2\\) ranks for the second group of observations. If there is no difference in population medians for the two groups, then we should not be able to reject the hypothesis that the mean \\(W\\) values are the same.\nIn practice there are a couple of complications. One is that for small samples the distribution of \\(W=\\text {rank}(Y)\\) is not Normal because it is discrete, taking only integers or averages of integers. But fortunately mathematical statisticians have long since worked out the exact distribution of \\(W\\) for many simple situations including two-sample tests, one-way ANOVA, two-way ANOVA with balanced numbers, correlation coefficients (e.g. the correlation between \\(\\text {rank}(Y)\\) and \\(\\text {rank}(X)\\)) and some others. So these exact distributions can be used for hypothesis tests, and are available. The second complication is that these exact distributions for \\(W= \\text {rank}(Y)\\) usually depend on the assumption of no ties, i.e. no equal ranks. Since ties do often occur in practice (if only because data are not measured exactly enough) then we need to use methods that are modified or corrected to handle ties. Fortunately the use of a software handles such issues as a matter of course in many cases, so it doesn’t take any extra time or effort on our behalf."
  },
  {
    "objectID": "studyguide/4-inference.html#wilcoxon-signed-rank-test",
    "href": "studyguide/4-inference.html#wilcoxon-signed-rank-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nFor the nonparametric equivalent of a one-sample \\(t\\)-test for \\(H_0:\\mu= \\mu_0\\), we use the Wilcoxon signed rank test for \\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median. Effectively this test is based on rank \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\). If the \\(\\eta_0\\) is in about the right place, then the distances to points above \\(\\eta_0\\) will tend to rank approximately the same as the distances to points below \\(\\eta_0\\). But if median is assumed too low, say, then the distances above \\(\\eta_0\\) will tend to be bigger (ranked higher) than the distances to points below \\(\\eta_0\\). A statistical test (Wilcoxon test) and the associated \\(p\\)-value follow. In theory, this test assumes a continuous symmetric distribution, but a correction is available in the case of ties. The following output shows the two-sample \\(t\\) test and Wilcoxon test results for testing the equality of median number of people for the time of day groups (morning & afternoon).\n\n\nCode\nwilcox.test(rangitikei$people ~ rangitikei$time, conf.int=T)\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\nThe t.test(rangitikei$people~rangitikei$time) test also gives the same conclusion for the equality of means."
  },
  {
    "objectID": "studyguide/4-inference.html#sign-test",
    "href": "studyguide/4-inference.html#sign-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Sign test",
    "text": "Sign test\nThere is an additional one-sample test available called the one-sample sign test, which is based on replacing \\(Y\\) not by \\(\\text {rank}(Y)\\) but simply by the sign of \\(Y-\\eta_0\\), i.e. whether it is positive or negative. This replacement represents an additional loss of detail in the data, but also requires no assumptions. The resulting test is based on a binomial distribution. For example, consider the television viewing time data. Suppose we wish to test the hypothesis that children watch 4 hours of television per day on average (1680 minutes per week). The one-sample sign test output follows:\n\n\nCode\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\nNotice the Wilcoxon test has about the same \\(p\\)-value as the Normal-based \\(t\\)-test. However as it assumes symmetry the estimated median is the same as the sample mean. The \\(p\\)-value for the sign test is similar, but not the same as the others, and the estimated median is the same. The Wilcoxon and Sign test procedure can also be used to generate approximate 95% confidence intervals for the median. Note that these are based on the sorted sample data, and so are discrete, so it is usually not possible to get exact 95% confidence intervals. Both intervals are wider than the confidence interval based on the mean. The loss of precision (longer interval) is reasonable as we are making much weaker assumptions."
  },
  {
    "objectID": "studyguide/4-inference.html#wilcoxon-rank-sum-or-mann-whitney-test",
    "href": "studyguide/4-inference.html#wilcoxon-rank-sum-or-mann-whitney-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Wilcoxon Rank-Sum or Mann-Whitney test",
    "text": "Wilcoxon Rank-Sum or Mann-Whitney test\nAs an alternative to the two-sample \\(t\\)-test is the Wilcoxon Rank-Sum test (also mathematically equivalent to a test known as the Mann-Whitney test). The assumptions of the test are that the data are continuous (or at least ordinal) from populations that have the same shape (e.g. same skewness and same variance) but just (possibly) different medians. For this test, the entire set of responses is ranked together and then the ranks for the first group are compared to the ranks for the second group. The null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\). The following R output shows the Wilcoxon Rank-Sum test results.\n\n\nCode\nkruskal.test(tv$TELETIME ~ factor(tv$SCHOOL))\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tv$TELETIME by factor(tv$SCHOOL)\nKruskal-Wallis chi-squared = 7.6389, df = 2, p-value = 0.02194\n\n\nAgain since our data are normally distributed we expect to get a similar result for the Mann-Whitney test as for the two-sample \\(t\\)-test of equal means \\(\\mu_1=\\mu_2\\). This is indeed the case."
  },
  {
    "objectID": "studyguide/4-inference.html#bootstrap-methods",
    "href": "studyguide/4-inference.html#bootstrap-methods",
    "title": "Chapter 4: Statistical Inference",
    "section": "Bootstrap methods",
    "text": "Bootstrap methods\nWhen a random sample is taken from a population, the expectation is that it is representative. So why not sample from the sample (i.e. resample) so that the quality of how well the sample is representative can be examined. We cannot gain extra information hugely by ordinary resampling and it is more like moving forward pulling the bootstrap! The methodology bootstrapping or resampling was introduced by Efron (1979). This computational intensive procedure can be implemented very mechanically and simpler. By computing the sampling distribution of a statistic of interest, issues such as its bias and the standard error can be addressed.\nR package boot has many features and several variations (types) of the bootstrap resampling method but harder to use. We will use the resample package instead because it is simpler and also includes simple permutation tests.\nUnder the simple bootstrap method, observations are resampled with replacement from the original sample to create a bootstrap sample. We can then compute a statistic such as the sample mean for this resample. This process can be repeated many times, say 10000, and we form the bootstrap distribution of the statistic. Consider the tv dataset. If we resample TELETIME, and compute the mean television viewing time for each sample, we construct the bootstrap distribution of mean. Using the resample package, we get-\n\n\nCode\nlibrary(resample)\nbootC &lt;- bootstrap(tv$TELETIME, mean)\nbootC\n\n\nCall:\nbootstrap(data = tv$TELETIME, statistic = mean)\nReplications: 10000\n\nSummary Statistics:\n     Observed       SE    Mean      Bias\nmean 1729.283 82.74484 1728.54 -0.742787\n\n\nThis output shows the observed sample mean 1729.283 and the mean of all bootstrap means which is 1729.114. The bias is the difference, which is -0.168. The main advantage of the bootstrap method is that it can quantify the bias that can occur due to sampling. Figure 18 and Figure 19, respectively, show the histogram and the normal quantile plots for the bootstrap means. Obviously the bootstrap means follow normal (due to CLT).\n\n\nCode\nhist(bootC)\n\n\n\n\n\nFigure 18: Histogram of the bootstrap means of TELETIME\n\n\n\n\n\n\nCode\nqqnorm(bootC)\n\n\n\n\n\nFigure 19: QQ plot of the bootstrap means of TELETIME\n\n\n\n\nThere are many versions of bootstrap confidence intervals depending on the way bootstrapping is done. Without going into details, the 95% confidence interval for the true mean viewing time is obtained as follows:\n\n\nCode\nCI.t(bootC)\n\n\n         2.5%    97.5%\nmean 1560.637 1897.928\n\n\nThis interval compares well with the confidence interval using t-distribution found earlier namely (1560.633, 1897.932). The same approach can be taken to construct a confidence interval for the mean of the paired differences and thereby perform a test analogous to paired t-test. See the testsmarks data example given below:\n\n\nCode\ndiffer &lt;- testmarks$Maths-testmarks$English\nbootC &lt;- bootstrap(differ, mean)\nCI.t(bootC)\n\n\n          2.5%    97.5%\nmean -4.175863 4.925863\n\n\nThe parametric (i.e. t-test based) and bootstrap results are very similar. The bootstrap method can obtain better confidence intervals for the mean when the population is skewed because resampling tends to adjust for the skew in the population when captured by the sample well. The bootstrap approach will work well only when reasonably large sample sizes are available because of the inherent uncertainty in the tails of the underlying distribution. Small samples are not sufficient to identify capture the tails of the distribution and hence certain types of inferences involving tail part of distribution will not work well. Parametric assumptions can be made to improve the bootstrap method and this approach is known as parametric bootstrapping. We will not study such methods in this course.\nThe resample package also has options to bootstrap from two vectors. Consider the television viewing times for the boys and girls groupings. We resample from the two groups to perform the two sample test.\n\n\nCode\nbootC &lt;- bootstrap2(tv$TELETIME, statistic=mean, treatment=tv$SEX)\nCI.t(bootC)\n\n\n               2.5%    97.5%\nmean: 1-2 -468.7219 224.6349\n\n\nThe bootstrap test conclusion again agrees with the Welch two-sample t-test conclusion.\nYou will not be examined on the use of permutation and bootstrap tests in the final exam. We may occasionally use this approach for assignments."
  },
  {
    "objectID": "studyguide/6-single.html",
    "href": "studyguide/6-single.html",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "",
    "text": "“The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.” — von Neumann"
  },
  {
    "objectID": "studyguide/6-single.html#displaying-interpreting-the-fitted-model",
    "href": "studyguide/6-single.html#displaying-interpreting-the-fitted-model",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Displaying & Interpreting the Fitted Model",
    "text": "Displaying & Interpreting the Fitted Model\nIt is fairly easy to display fitted simple regression line on a scatter plot; see Figure 2 and the R codes shown below:\n\n\nCode\nggplot(horsehearts) +\n  aes(x=EXTDIA, y=WEIGHT) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\nFigure 2: Simple regression line\n\n\n\n\nThe geoms geom_smooth() or stat_smooth() add the fitted regression line to the plot. For the simple regression of WEIGHT (weights of horses’ hearts) on EXTDIA (diastole exterior width), the fitted simple regression model is \\(\\hat{y}=a+bx\\) For horses heart data, the coefficient estimates are obtained as \\(a\\) = -2.0003 and \\(b\\) = 0.2996. That is, the fitted model is given by \\(fitted~~weight = -2.0003 + 0.2996\\times extdia\\) or after rounding\n\\[fitted~~weight = -2 +0.3\\times extdia\\]\nFor a given extdia \\(\\left(x\\right)\\) value, the expected weight is given by the fitted simple regression equation. For example, the exterior width (during diastole phase) for the \\(39^{th}\\) horse is 15.0mm and fitted weight is therefore\n\\[fitted~~weight =\\hat{y}_{39} = -2 + 0.3 \\times 15 = 2.5\\]\nThe observed weight of its heart \\(\\left(y_{39} \\right)\\) is 4.1kg. For this horse, we obtain the residual as\n\\[e_{39}= \\left(y_{39}-\\hat{y}_{39} \\right) = 4.1- 2.5 = 1.6.\\]\n\\(t\\)-test for Model Parameters\nIt is desirable to use the R package broom to get parts of the regression outputs. The function tidy() extracts the model and the significance testing results; see Table 1.\n\n\nCode\nlibrary(broom)\nlibrary(kableExtra)\n\nsimplereg &lt;- lm(WEIGHT~EXTDIA, data=horsehearts)\n\ntidy(simplereg)\n\n\n\n\n\n\nTable 1: t-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.00034\n0.55881\n-3.57965\n0.00085\n\n\nEXTDIA\n0.29963\n0.03877\n7.72793\n0.00000\n\n\n\n\n\n\n\n\nFor the horses hearts data, under the null hypothesis that the true (or population) slope \\(b\\) equals zero (i.e., \\(H_{0}:\\beta =0\\)), the test statistic becomes \\[t=\\frac{b}{s_{b} } =\\frac{0.29963}{0.03877} = 7.728.\\]\nThe \\(t\\)-statistic is clearly large enough to be considered significant for \\((n-2)\\) \\(df\\) and the \\(p\\)-value is close to zero. Thus, we would reject the null hypothesis \\(H_{0} :\\beta =0\\). In other words, the slope coefficient is significantly different from zero and hence the predictor variable extdia explains a significant amount of variation in the response variable weight.\nWe can also carry out a similar \\(t\\)-test for the \\(y\\)-intercept \\(\\alpha\\) based on the \\(t\\)-statistic \\[t = \\frac {a}{s_a}= \\frac{-2.00034}{0.55881}=-3.58.\\] but the main interest is in the slope parameter \\(\\beta\\). See Table 1."
  },
  {
    "objectID": "studyguide/6-single.html#model-summaries",
    "href": "studyguide/6-single.html#model-summaries",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Model Summaries",
    "text": "Model Summaries\nMany of the model summary measures can be obtained for assessing the quality of the fitted model using the glance() function from the broom package (Table 2).\n\n\nCode\nsimplereg |&gt; \n  glance() |&gt; \n  select(r.squared, sigma, statistic, p.value, AIC, BIC)\n\n\n\n\n\n\nTable 2: Model summary measures\n\n\nr.squared\n0.58\n\n\nsigma\n0.74\n\n\nstatistic\n59.72\n\n\np.value\n0.00\n\n\nAIC\n106.83\n\n\nBIC\n112.32\n\n\n\n\n\n\n\n\nHow to interpret entries appearing in the R output Table 2 is explained below. Note that the \\(R^{2}\\) statistic and the residual standard error are two common summary measures for the fitted model. The other measures such as the AIC and BIC shown in in Table 2 are useful for comparison of models, and selecting a best model, which will be discussed later on.\nResidual standard error\nThe size of the residual standard error \\(s_{e}\\), which is called sigma in Table 2, is important for many reasons. In general, we prefer to have \\(s_{e}\\) no more than 5% of \\(\\bar{y}\\) or small compared the range of \\(y\\) data. For the model fitted to horses hearts , the residual standard error is labelled as sigma in Table 2. This value of is rather large (compared to the range of \\(y\\) data), and hence the fitted model may not be good for prediction purposes.\nThe size of \\(s_{e}\\) also controls the size of the standard error of the slope estimate \\(b\\) (and hence its confidence interval).\nR-squared \\(\\left(R^{2} \\right)\\) statistic:\nThe R-Squared statistic, the proportion of the variation explained by the fitted model, is 0.58. This means that 58 percent of the total variation of the weights is explained by the exterior widths (diastole) using the fitted straight line model namely\n\\[\\text {fitted  weight = -2 +0.3}\\times \\text {extdia}\\]\nThe \\(R^{2}\\) value is also known as the coefficient of (multiple) determination. Notice that when there is only one explanatory variable \\(X\\), then the \\(R^{2}\\) is equal to the square of the \\((X,Y)\\) correlation coefficient, i.e. \\(R^{2} =\\left(r_{x,y} \\right)^{2}\\).\nWe usually require that \\(R^{2}\\) be at least 0.5 so that at least half of the variation is explained by the fit. For the Horse data the model \\(R^{2}\\) is not much better than this. However the scatterplot revealed that there may be two different groups of observations in the data and/or the curvature in the data may indicate that a transformation would be advisable so this low \\(R^{2}\\) is not really surprising.\nRemember that there is a difference between a meaningful model and a statistically significant model. An \\(R^{2}\\) of 0.5 or more indicates a meaningful model whereas the \\(t\\)-test for slope indicates a statistically significant model. A statistically significant model may not always be a meaningful model - in this case the significance is high but the \\(R^{2}\\) of 57.6% is barely adequate.\nANOVA and \\(F\\)-test\nTable 2 gives the F-statistic (labelled as just statistic) and the P-value for this F-statistic. The concept behind this statistic is explained below:\nThe variation in a data set can be measured as the sum of the squared deviation from its central value. This Sum of Squares is abbreviated as SS or SumSq in software regression outputs. For a regression model, we have\nI : total SumSq = variation in the observed values about the mean = \\(\\sum \\left(y-\\bar{y}\\right)^{2}\\).\nII : regression SumSq = Variation in the fitted values about the mean = \\(\\sum \\left(\\hat{y}-\\bar{y}\\right)^{2}\\) (Note that fit is denoted by \\(\\hat{y}\\)).\nIII : error or residual SumSq = Variation in the residuals \\(=\\sum \\left(y-\\hat{y}\\right)^{2} =\\sum e^{2}\\).\nThese sums of squares due to regression, error and total etc are usually displayed in the form of a table known as the analysis of variance table, which is usually shortened to ANOVA or anova. A typical ANOVA table for a simple regression model will appear as in Figure 3. Depending on the package used, the ANOVA table may differ slightly in style.\n\n\n\nFigure 3: A typical ANOVA table\n\n\nR does not print the last row while displaying the F-statistic and ANOVA table. The following codes can be used to obtain the ANOVA output for the regression model (Table 3).\n\n\nCode\nsimplereg |&gt; anova()\n\n# or \n\nsimplereg |&gt; anova() |&gt; tidy()\n\n\n\n\n\n\nTable 3: Analysis of Variance Table\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nEXTDIA\n1\n32.731\n32.731\n59.721\n0\n\n\nResiduals\n44\n24.115\n0.548\nNA\nNA\n\n\n\n\n\n\n\n\nEach sum of squares (source of variation) has associated with it a degrees of freedom \\(df\\)). For one explanatory variable, the regression \\(df\\) = 1. The total \\(df\\) is always one less than the sample size, that is \\(n-1.\\) In other words, residual \\(df\\) = \\(n-2\\). From the sums of squares, the variance estimates are calculated as SumSq/df which are called Mean Squares (MeanSq).\nFor the horses’ heart data, we have Regression SumSq = 32.731, Error SumSq= 24.115 and Total SumSq = 56.845. We also have regression \\(df\\) = 1, total \\(df\\) = \\(n-1 = 45\\) and by subtraction residual \\(df= 45-1= 44\\). Dividing the SumSq by the associated \\(df\\), we compute\n\nRegression Mean Sq = 32.731/1 = 32.731\n\nResidual Mean Sq = 24.115/44 = 0.548\n\nTotal Mean Sq = Total SumSq/45 = \\(\\sum(y-\\bar{y})^2/(n-1)=1.263\\)\n\nNote that the Total Mean Sq is nothing but the variance of \\(y\\), which is usually not displayed in the ANOVA table.\nIt is possible to formalise the goodness of fit of the model by carrying out an \\(F\\)-test. The \\(F\\) statistic is formed by the ratio of two estimates of variances, the regression Mean Sq or variance and the error Mean Sq or variance. The distribution of the \\(F\\) statistic is governed by the numerator \\(df\\) and the denominator \\(df\\). For the horses’ heart data, we obtain\n\\[F =\n\\frac{{\\text {regression MeanSq} }}{{\\text {residual MeanSq}}}\n= \\frac{32.731}{0.548} = 59.721 \\]\nwith 1 \\(df\\) for the numerator and 44 \\(df\\) for the denominator.\nThe null hypothesis is that the model does not fit the data well. That is, the model explains too little of the variation in the \\(y\\) values to be significant. This hypothesis is generally rejected whenever the \\(p\\)-value of the \\(F\\)-test is less than 0.05. In ANOVA table, the \\(F\\) statistic is displayed along with the \\(p\\)-value. Here the \\(p\\)-value is the probability of observing an \\(F\\) statistic larger than the computed value. For horses’ heart data, the computed \\(F\\) statistic is 59.721. \\(F\\) statistic is always positive (being the ratio of mean squares) and hence the alternative hypothesis must be one-sided. That is, the \\(p\\)-value is given by \\(\\Pr \\left(F_{1,44} &gt;59.721\\right)\\) which is very close to zero. This means that the null hypothesis is firmly rejected and we conclude that the regression model using the explanatory variable extdia explains a significantly large proportion of the variation of the response variable weight.\nNote that the \\(R^{2}\\) can also be calculated from the ANOVA table\n\\[R^{2} =\\frac{{\\text {regression SumSq}}}{{\\text {total SumSq}}} = \\frac{32.731}{56.845} = 0.5758\\] or alternatively\n\\[R^{2} = 1-\\frac{{\\text {residual SumSq}}}{{\\text {total SumSq}}} = 1 -- \\frac{24.115}{56.845} = 0.5758.\\] The degrees of freedom for residual SumSq and total SumSq are not identical; while they are very close this is not always the case (e.g. in multiple regression which we will meet in the next chapter). Hence we can adjust for this to obtain the adjusted R-Squared (\\(R_{adj}^{2}\\)):\n\\[\n\\begin{aligned}\nR_{adj}^{2}\n&= 1-\\frac{\\left(\\frac{{\\text {residual SumSq}}}{{\\text {residual df}}} \\right)}{\\left(\\frac{{\\text {totalSumSq}}}{{\\text {total df}}} \\right)\\, } \\\\\n&= 1-\\frac{24.115/44}{56.845/45} \\\\\n&=0.5661.\n\\end{aligned}\n\\]\nFor the simple regression, the \\(F\\)-test is equivalent to the previous \\(t\\)-test (and produces exactly the same \\(p\\)-value). In fact when there is only one explanatory variable the two test statistics are related by the equation \\(F\\) = \\(t^2\\). A further relationship between the two is that the residual standard error is the square root of the residual mean square.\nThe summary() function in base R gives rather a bulky output for the fitted regression model particularly when the number of predictors is large.\n\n\nCode\nsimplereg &lt;- lm(WEIGHT~EXTDIA, data=horsehearts) \nsummary(simplereg)\n\n\nThe R package lessR will get you even a bigger output of model quality and summary measures. We wont be covering all of them but only the essential ones obtained by the broom package function glance(). Try-\n\n\nCode\nlibrary(lessR)\n\n\n\nlessR 4.2.9                         feedback: gerbing@pdx.edu \n--------------------------------------------------------------\n&gt; d &lt;- Read(\"\")   Read text, Excel, SPSS, SAS, or R data file\n  d is default data frame, data= in analysis routines optional\n\nLearn about reading, writing, and manipulating data, graphics,\ntesting means and proportions, regression, factor analysis,\ncustomization, and descriptive statistics from pivot tables\n  Enter:  browseVignettes(\"lessR\")\n\nView changes in this and recent versions of lessR\n  Enter: news(package=\"lessR\")\n\nInteractive data analysis\n  Enter: interact()\n\n\n\nAttaching package: 'lessR'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    recode, rename\n\n\nCode\nreg(WEIGHT~EXTDIA, data=horsehearts)"
  },
  {
    "objectID": "studyguide/6-single.html#prediction-and-estimation",
    "href": "studyguide/6-single.html#prediction-and-estimation",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Prediction and Estimation",
    "text": "Prediction and Estimation\nRecall that the fitted value for the weight of a heart with an extdia value of 15.0mm was 2.5kg. This value can be interpreted as the predicted weight of a horse heart with extdia = 15mm, or as the estimated of all horse hearts with extdia = 15mm. The 95% confidence limits for the mean response for the mean weight of horses hearts with extdia = 15mm is given by \\(\\left(2.26kg,2.72kg\\right)\\). However the weight of any individual heart with extdia = 15mm could (with the same confidence) be as low as 0.98kg or as high as 4.00kg (the prediction interval being (0.98kg, 4.00kg). Notice that the approximate PI formula works as well here: \\(s\\) = 0.7403 and so 2.50 \\(\\pm\\) (2 \\(\\times\\) 0.74) = (1.0kg, 4.0kg). Note that manual computation of the prediction intervals is harder, and we prefer to use R for this.\n\n\nCode\n# confidence interval\npredict(simplereg, list(EXTDIA = 15), interval = \"confidence\")\n\n\n       fit      lwr      upr\n1 2.494161 2.264023 2.724299\n\n\n\n\nCode\n# prediction interval\npredict(simplereg, list(EXTDIA = 15), interval = \"prediction\")\n\n\n       fit       lwr      upr\n1 2.494161 0.9845172 4.003805\n\n\nWe can also create a dataset with the desired intervals using the augment() function from the broom package.\n\n\nCode\nhorseCI &lt;- augment(simplereg, interval = \"confidence\")\nhorsePI &lt;- augment(simplereg, interval = \"prediction\")\n\n\nLet’s visualise the confidence and prediction bands for the fitted line on a scatter plot using the geom_ribbon() function; see Figure 4.\n\n\nCode\np1 &lt;- simplereg |&gt; \n  augment(interval = \"confidence\") |&gt;\n  ggplot() +\n  aes(x = EXTDIA) +\n  geom_point(aes(y = WEIGHT)) + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.5) +\n  geom_line(aes(y = .fitted)) +\n  ggtitle(\"Confidence interval\") \n\np2 &lt;- simplereg |&gt; \n  augment(interval = \"prediction\") |&gt;\n  ggplot() +\n  aes(x = EXTDIA) +\n  geom_point(aes(y = WEIGHT)) + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.5) +\n  geom_line(aes(y = .fitted)) +\n  ggtitle(\"Prediction interval\") +\n  theme_light()\n\nlibrary(patchwork)\n\np1/p2\n\n\n\n\n\nFigure 4: Confidence intervals and prediction intervals\n\n\n\n\nThe R package visreg readily shows the confidence bands too. Try-\n\n\nCode\nlibrary(visreg)\nvisreg(lm(WEIGHT~EXTDIA, data=horsehearts),)"
  },
  {
    "objectID": "studyguide/6-single.html#improving-simple-regression",
    "href": "studyguide/6-single.html#improving-simple-regression",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Improving Simple Regression",
    "text": "Improving Simple Regression\nIf the fitted simple regression is rather poor, what can be done?\n(a) Use a different predictor variable\nFor this Chapter example, we could choose another one of the ultra-sound measurements to predict the weight of the horses heart.\n(b) Transform the \\(Y\\) variable\nWe could choose a transformation which makes physical sense. For example, we could argue that the weight of the heart should be closely related to the volume of the heart and volume is related to the product of three lengths or any one length cubed. Rather than cubing the \\(X\\) variable we usually transform the response variable \\(Y\\). Alternatively we might choose a transformation based on statistical grounds. Our previous discussion suggests that a shrinking transformation should be used, so suppose we take the logarithm of \\(Y\\). The distribution of weights is compared before and after transformation in Figure 13.\n\n\n\n\n\nFigure 13: Comparison of raw and log-transformed Weight data\n\n\n\n\nWe can see the effect of logarithmic transformation from the boxplot in Figure 13. Clearly the distribution has become more symmetric. In order to make the distribution even more symmetric we might also try a power transformation as shown in Figure 13. Here we have applied the negative reciprocal cubic root transformation \\(-\\frac {1}{Y^{1/3}}\\) which makes more physical sense. This yields a slightly symmetric distribution. A minimal summary output of the regression of \\(WEIGHT^{1/3}\\) on EXTDIA is shown in Table 4:\n\n\nCode\nlm(WEIGHT^(1/3)~EXTDIA, data=horsehearts) |&gt; \n  glance() |&gt; \n  select(r.squared, sigma, statistic, p.value, AIC, BIC)|&gt; \n  mutate_if(is.numeric, round,3) |&gt; \n  t() |&gt; \n  kable() |&gt; \n  kable_classic(full_width = F) \n\n\n\n\nTable 4: Cubic Root Transformed Response Model Summary\n\n\nr.squared\n0.616\n\n\nsigma\n0.128\n\n\nstatistic\n70.498\n\n\np.value\n0.000\n\n\nAIC\n-54.588\n\n\nBIC\n-49.102\n\n\n\n\n\n\n\n\nThis output shows an improvement in the \\(R^{2}\\) value. However, for technical reasons one cannot meaningfully compare \\(R^{2}\\) values for the raw and transformed data. The estimated standard deviation of the residuals is also meaningless when comparing raw and transformed data (recall that this quantity is the square root of the residual MeanSq). There is really only one way to confirm whether a transformed model is better than the original model and that is by analysing the residuals. Only if the residuals are better behaved, in that they comply more closely with the regression assumptions, can one claim that the transformed model is preferable. This is a very important point.\nIn the next Chapter, we will cover more on AIC and BIC values shown in@tbl-transweightreg but the simple thumb rule is to opt a model with the smallest AIC or BIC while we go for a model with the largest log-likelihood.\nThese values also support the model based on the negative reciprocal cubic root transformed data.\n(c) Add other explanatory variables to the model\nThis will cause the \\(R^{2}\\) to increase (although we may decide that the increase is not worth the effect of making the model more complicated). We will study multiple regression models having two or more predictors in the next chapter."
  },
  {
    "objectID": "studyguide/6-single.html#cross-validation-cv",
    "href": "studyguide/6-single.html#cross-validation-cv",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\nThis technique is commonly employed for validating models for prediction purposes. The available data is split randomly into \\(k\\) (equal) folds (parts), often by resampling. A model is fitted for the \\((k-1)\\) folds of the data, and then the prediction errors are calculated for the fold that was omitted for modelling. This process can be repeated omitting one subset (out of the \\(k\\) subsets) so that all the \\(k\\) subsets contribute to the estimation of prediction accuracy. This exercise is computationally intensive, and hence we will leave it to the software package such as caret, rsample or modelr to perform the cross validation. Consider the simple regression of WEIGHT on EXTDIA done with the horsesheart data. The following R code perform the 5-fold cross validation of the model for prediction purposes and compare the root mean square errors for both the regression and robust regression models.\n\n\nCode\nlibrary(caret)\nlibrary(MASS, exclude = \"select\")\n\nset.seed(123)\n\n# Set up cross validation\nfitControl &lt;- trainControl(method = \"repeatedcv\", \n                           number = 5, \n                           repeats = 100)\n\n# lmfit\nlmfit &lt;-  train(WEIGHT ~ EXTDIA,\n                data = horsehearts, \n                trControl = fitControl, \n                method = \"lm\")\n\n# rlmfit\nrlmfit &lt;- train(WEIGHT ~ EXTDIA, \n                data = horsehearts, \n                trControl = fitControl, \n                method = \"rlm\")\n\n# Extract the RMSE scores\ndfm &lt;- tibble(\n  lm = lmfit |&gt; pluck(\"resample\") |&gt; pull(RMSE),\n  rlm = rlmfit |&gt; pluck(\"resample\") |&gt; pull(RMSE)\n) |&gt; \n  pivot_longer(cols=everything(),\n               names_to = \"Method\",\n               values_to = \"RMSE\")\n  \n# Make plot of RMSE\nggplot(dfm) +\n  aes(x=Method, y=RMSE, col = Method) + \n  geom_boxplot() +\n  coord_flip() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 20: Comparison of Residual Mean Square Error (RMSE) of lm() vs rlm() fits\n\n\n\n\nFigure 20 shows that the robust rlm() fit slightly outperforms the simple regression fit in terms of RMSE. The number of folds fixed can affect the comparison. The choice of k=5 or 10 is usually recommended.\nFigure 21 shows the cross validation RMSEs for the lm() and rlm() fits for the Rangitikei dataset based on modelr package codes. The robust model again perform slightly better but this does not mean the fitted model is the best one for prediction.\n\n\nCode\nlibrary(purrr)\nlibrary(modelr)\n\nset.seed(123)\n\n# Set up cross validation\ncv2 &lt;- crossv_mc(rangitikei, 500)\n\n# Fit the models\nlm_models &lt;- map(cv2$train, ~ lm(people ~ vehicle, data = .))\nrlm_models &lt;- map(cv2$train, ~ rlm(people ~ vehicle, data = .))\n\n# Extract the RMSE scores\ndfm &lt;- tibble(\n  lm = map2_dbl(lm_models, cv2$test, rmse),\n  rlm = map2_dbl(rlm_models, cv2$test, rmse)\n) |&gt; \n  pivot_longer(cols=everything(),\n               names_to = \"Method\",\n               values_to = \"RMSE\")\n\n# Make a plot\nggplot(dfm) +\n  aes(x=Method, y=RMSE, col = Method) + \n  geom_boxplot() +\n  coord_flip() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 21: Comparison of RMSEs of lm() and rlm() fits under cross validation"
  },
  {
    "objectID": "studyguide/6-single.html#models-related-to-simple-regression-model",
    "href": "studyguide/6-single.html#models-related-to-simple-regression-model",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Models Related to Simple Regression Model",
    "text": "Models Related to Simple Regression Model\nFor some applications, the true \\(y\\)-intercept \\(\\alpha\\) is known to be zero. An example is when a model is fitted with compositional characteristics as predictors such as percentage fat, protein, moisture in milk powder. The sum of these compositional characteristics is always 100% and this constraint removes the intercept due to theoretical reasons. In such cases, we will be fitting only the slope(s). When the model without the \\(y\\)-intercept is the true or correct model, both the fitted models (with and without the \\(y\\)-intercept) provide unbiased estimates of the true parameters. However the model without the intercept will provide precise estimates. This regression model without the intercept must be used with caution because a very strong assumption is made on the true \\(y\\)-intercept. The first differences of certain time series data may be analysed with a model having no intercept term.\nFor some applications such as measurement system analysis in metrology orthogonal regression models are fitted because both X and Y variables are subject to measurement errors. In the least squares method, we minimise the sum of the squared vertical distances between the actual Y and fitted Y values. For orthogonal regression, we consider the perpendicular distances instead. This approach is a subset of a theory known as principal components. The orthogonal regression approach was popularised by Deming for industrial applications and hence this fit is also known as Deming regression. R software packages (such as MethComp, deming and mcr) will fit orthogonal regression models. Figure 22 shows the Deming regression model to testmarks data but you do not have to perform this regression for this course.\n\n\nCode\nmy.pca &lt;- prcomp( ~ Maths+English, data=testmarks)\n\nslp &lt;- my.pca$rotation[2,1] / my.pca$rotation[1,1]\nintr &lt;- my.pca$center[2] - slp*my.pca$center[1]\n\nmy.caption = paste(\"Slope=\", round(slp, 2), \"  \", \"Intercept\", round(intr, 2))\n\nggplot(testmarks) + \n  geom_point(aes(x=Maths,y=English)) + \n  geom_abline(intercept = intr, slope = slp) + \n  ggtitle(\"Orthogonal Regression\") + \n  labs(caption = my.caption)\n\n\n\n\n\nFigure 22: Orthogonal regression\n\n\n\n\nA number of relationships between variables can be linearised by transformations. For example, the relationship \\(y=\\alpha x^{\\beta }\\) is linearised by taking \\(\\log\\) on both sides as \\[\\log (y)=\\log (\\alpha )+\\beta \\log (x).\\] The methods discussed in this chapter will equally apply for models which can be rewritten as a straight line equation."
  },
  {
    "objectID": "studyguide/8-anova.html",
    "href": "studyguide/8-anova.html",
    "title": "Chapter 8: Analysis of Variance (ANOVA) and Covariance (ANCOVA)",
    "section": "",
    "text": "“To consult a statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.” – Sir RA Fisher"
  },
  {
    "objectID": "studyguide/8-anova.html#another-example",
    "href": "studyguide/8-anova.html#another-example",
    "title": "Chapter 8: Analysis of Variance (ANOVA) and Covariance (ANCOVA)",
    "section": "Another example",
    "text": "Another example\nConsider the pine tree data discussed earlier. Let us model the Top circumference (numerical response) using Area variable (which is categorical). A plot of the raw data and mean circumference along with the associated 95% confidence intervals are shown in Figure 13.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/pinetree.RData\",\n  destfile = \"pinetree.RData\")\n\nload(\"pinetree.RData\")\n\n\n\n\nCode\npinetree |&gt; \n  ggplot() +\n  aes(x = Area, y = Top, color = Area) + \n  geom_jitter(width = 0.15, height = 0, alpha = .6) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\", \n               size = 3, \n               position = position_nudge(x = 0.3)\n               ) + \n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"errorbar\", \n               size = 0.75, width = 0.075, \n               position = position_nudge(x = 0.3)\n               ) \n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 13: Interval Plot\n\n\n\n\nFor the three different areas of the forest, we define the following three indicator variables,\n\\[\\begin{array}{cccc}\nI_1 & = & 1 & \\text{for Area 1}\\\\\n& & 0& \\text{for Areas 2, and 3}\n\\end{array}\\] \\[\\begin{array}{cccc}\nI_2 & = & 1 & \\text{for Area 2}\\\\\n& & 0& \\text{for Areas 1, and 3}\n\\end{array}\\] \\[\\begin{array}{cccc}\nI_3 & = & 1 & \\text{for Area 3}\\\\\n& & 0& \\text{for Areas 1, and 2}\n\\end{array}\\]\nNote that we need only any three of the above indicator variables to identify the Area category. For example, with \\(I_1\\) and \\(I_2\\) variables, Area 3 is identified when \\(I_1 = I_2 = 0\\).\nLet us regress the pine tree Top circumference on the indicator variables \\(I_2\\) and \\(I_3\\) using the following R codes.\n\n\nCode\npinetree1 &lt;- pinetree |&gt; \n  select(Area, Top) |&gt; \n  mutate(I1 = as.numeric(Area == \"1\"),\n         I2 = as.numeric(Area == \"2\"), \n         I3 = as.numeric(Area == \"3\") )\n\nmdl &lt;- lm(Top ~ I2 + I3, data = pinetree1)\n\nlibrary(broom)\n\ntidy(mdl)\n\n\n\n\nCode\npinetree1 &lt;- pinetree |&gt; \n  select(Top, Area) |&gt; \n  mutate(I1 = as.numeric(Area == \"1\"),\n         I2 = as.numeric(Area == \"2\"), \n         I3 = as.numeric(Area == \"3\") )\n\nmdl &lt;- lm(Top ~ I2 + I3, data = pinetree1)\n\ntidy(mdl)\n\n\n\n\n\n\nTable 4: Regression of Top Circumference on Area Indicator Variables\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.025\n1.11375\n17.979805\n0.0000000\n\n\nI2\n-1.955\n1.57508\n-1.241207\n0.2196132\n\n\nI3\n-5.925\n1.57508\n-3.761714\n0.0004002\n\n\n\n\n\n\n\n\nThe following points are to be noted on the regression model shown in Table 4:\n\nThe \\(y\\)-intercept 20.025 is the mean Top circumference for Area 1 (i.e. the mean response for the omitted predictor category).\nThe mean Top circumference 2 and 3 can be found from the fitted model. For Area 2, \\(I_1\\) =0, \\(I_2\\)=1, and \\(I_3\\) =0. Substituting these values in the fitted model, we get \\(20.025 -1.955 \\times 1 -5.925 \\times 0 = 20.025-1.955 = 18.07\\), which is mean Top circumference for Area 1. For Area 3, the mean Top circumference for Area 3 is \\(20.025 -1.955 \\times 0 -5.925 \\times 1 = 20.025= 14.1\\). In other words, the fitted coefficient of an indicator variable is the difference between the response means of the category indicated by the variable and the ‘base’ category (indicated by the omitted indicator variable).\nThe significance of the slope coefficients implies a significant difference in means. For example, the coefficient of the indicator variable \\(I_2\\) is highly significant. This means that the mean Top circumference for Area 2 is significantly different from the mean Top circumference for Area 1. The negative sign of the coefficient means that Area 2 mean is significantly lower when compared Area 1 mean. The same is true when Area 3 is compared with Area 1.\nIn the above output, a multiple comparison of means is made keeping Area 1 as the base. A comparison of the mean Top circumference for Areas 2 & 3 is not possible with the above regression. We need to omit either \\(I_2\\) or \\(I_3\\) and fit a regression for such a comparison.\n\nIn practice, you will NOT be creating any indicator variables to perform the analysis and R does it for us when we use the lm() function but we have to make sure that categorical factors are stored as factors, not numerical variables.\nThe ANOVA of the regression model shown in Table 5 has a highly significant F value which means that at least one Area is different in terms of mean Top circumference.\n\n\nCode\nlm(Top ~ Area, data=pinetree) |&gt; anova()\n\n\n\n\n\n\nTable 5: Regression ANOVA\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nArea\n2\n364.5903\n182.29517\n7.348015\n0.0014482\n\n\nResiduals\n57\n1414.0995\n24.80876\nNA\nNA"
  },
  {
    "objectID": "studyguide/references.html",
    "href": "studyguide/references.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nReferences"
  },
  {
    "objectID": "workshops/ws01.html",
    "href": "workshops/ws01.html",
    "title": "Getting Started With R",
    "section": "",
    "text": "Tip\n\n\n\nYou are NOT expected to become an expert in R programming or required to be one for doing this course.\nFeel free to post your queries regarding R and RStudio on the class Stream discussion forum.\n\n\nR https://www.r-project.org/ is an open-source (i.e., free) software package. This software is available for essentially all computing platforms (e.g. Windows, Linux, Unix, Mac) is maintained and developed by an energetic community of users including many of the world’s foremost statisticians.\nR is also a programming language but you may not be required to do a lot of programming for your course work. R includes functions which enables us to perform a full range of statistical analyses.\nFor installing R software, please visit https://cran.stat.auckland.ac.nz/ and follow the instructions.\nNote that the R software will be sitting in the background in RStudio and you will not be using it directly for this course.\nRStudio https://www.rstudio.com/products/rstudio/ is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor (as well as to quickly jump to R function definitions) that supports direct R code execution. It also contains tools for plotting, history, debugging, workspace and project management. RStudio has many other features such as authoring HTML, PDF, Word Documents, and slide shows. In order to download RStudio (Desktop edition, open source), go to\nhttps://www.rstudio.com/products/rstudio/download/\nDownload the installation file and run it. Note that RStudio must be installed after installing R.\nR/RStudio can also be used using the cloud platform at https://rstudio.cloud/ after creating a free account but occasionally some of the packages covered in this course may fail to work in the cloud platform.\nIf you open RStudio, you will see something similar to the screen shot shown in Figure 1:\n\n\n\nFigure 1: An RStudio window\n\n\nRStudio has many options, such as uploading files to a server, creating documents etc. You will be using only a few of the options. You will not be using the menus such as Build, Debug, Profile at all in this course.\nYou can either type or copy and paste the R codes appearing in this section on to the R Script window and run them."
  },
  {
    "objectID": "workshops/ws01.html#grammar-of-graphics",
    "href": "workshops/ws01.html#grammar-of-graphics",
    "title": "Getting Started With R",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThe main idea behind the grammar of graphics of (Wilkinson 2005) is to mimic the manual graphing approach and define building blocks and combine them to create a graphical display. The building blocks of a graph are:\n\ndata\naesthetic mapping\ngeometric object\ntransformation or re-expression of data\nscales\ncoordinate system\nposition adjustments\nfaceting\n\nIf have not installed ggplot2 or tidyverse, install it with the following commands.\n\n\nCode\ninstall.packages(\"ggplot2\")\n\n\nWe can now load the ggplot2 library with the commands:\n\n\nCode\nlibrary(ggplot2)\n\n\nIn order to work with ggplot2, we must have a data frame or a tibble containing our data. We need to specify the aesthetics or how the columns of our data frame can be translated into positions, colours, sizes, and shapes of graphical elements.\nThe geometric objects and aesthetics of the ggplot2 system are explained below:"
  },
  {
    "objectID": "workshops/ws01.html#aesthetic-mapping-aes",
    "href": "workshops/ws01.html#aesthetic-mapping-aes",
    "title": "Getting Started With R",
    "section": "Aesthetic Mapping (aes)",
    "text": "Aesthetic Mapping (aes)\nIn ggplot land aesthetic means visualisation features or aesthetics. These are\n\nposition (i.e., on the x and y axes)\ncolor (“outside” color)\nfill (“inside” color)\nshape (of points)\nlinetype\nsize\n\nAesthetic mappings are set with the aes() function."
  },
  {
    "objectID": "workshops/ws01.html#geometric-objects-geom",
    "href": "workshops/ws01.html#geometric-objects-geom",
    "title": "Getting Started With R",
    "section": "Geometric Objects (geom)",
    "text": "Geometric Objects (geom)\nGeometric objects or geoms are the actual marking or inking on a plot such as:\n\npoints (geom_point, for scatter plots, dot plots, etc)\nlines (geom_line, for time series, trend lines, etc)\nboxplot (geom_boxplot, for boxplots)\n\nA plot must have at least one geom but there is no upper limit. In order to add a geom to a plot, the + operator is employed. A list of available geometric objects can be obtained by typing geom_&lt;tab&gt; in Rstudio. The following command can also be used which will open a Help window.\nhelp.search(\"geom_\", package = \"ggplot2\")\nConsider the study guide dataset rangitikei.txt (Recreational Use of the Rangitikei river). The first 10 rows of this dataset are shown below:\n\n\n   id loc time w.e cl wind temp river people vehicle\n1   1   1    2   1  1    2    2     1     37      15\n2   2   1    1   1  1    2    1     2     23       6\n3   3   1    2   1  1    2    2     3     87      31\n4   4   2    2   1  1    2    1     1     86      27\n5   5   2    1   1  1    2    2     2     19       2\n6   6   2    2   1  2    1    3     3    136      23\n7   7   1    2   2  2    2    2     3     14       8\n8   8   1    2   1  2    2    2     3     67      26\n9   9   1    1   2  1    3    1     2      4       3\n10 10   2    2   1  2    2    2     3    127      45\n\n\nThe description of the variables is given below:\nloc - two locations were surveyed, coded 1, 2\ntime - time of day, 1 for morning, 2 for afternoon\nw.e - coded 1 for weekend, 2 for weekday\ncl- cloud cover, 1 for &gt;50%, 2 for &lt;50%\nwind- coded 1 through 4 for increasing wind speed\ntemp - temperature, 1, 2 or 3 increasing temp\nriver- murkiness of river in 3 increasing categories\npeople - number of people at that location and time\nvehicle- number of vehicles at that location at that time\n\nThis dataset is downloaded from the web using the following commands.\n\n\nCode\nmy.data &lt;- read.csv(\n  \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\", \n  header=TRUE\n  )\n\n\n\n\nCode\nggplot(data = my.data,\n       mapping = aes(x = vehicle, y = people)\n       ) +\n  geom_point()\n\n\n\n\n\nThe aes part defines the “aesthetics”, which is how columns of the dataframe map to graphical attributes such as x and y position, colour, size, etc. An aesthetic can be either numeric or categorical and an appropriate scale will be used. After this, we add layers of graphics. geom_point layer is employed to map x and y and we need not specify all the options for geom_point.\nThe aes() can be specified within the ggplot function or as its own separate function. I prefer this format.\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point()\n\n\n\n\n\nWe can add a title using labs() or ggtitle() functions. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  ggtitle(\"No. of people vs No. of vehicles\")\n\n\nor\n\n\nCode\nggplot(my.data)+\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  labs(title = \"No. of people vs No. of vehicles\")\n\n\nNote that labs() allows captions and subtitles.\ngeom_smooth is additionally used to show trends.\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nSimilar to geom_smooth, a variety of geoms are available.\n\n\nCode\nggplot(my.data) + \n  aes(x = factor(wind), y = people) +\n  geom_boxplot()\n\n\n\n\n\nEach geom accepts a particular set of mappings;for example geom_text() accepts a labels mapping. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  geom_text(aes(label = w.e), \n            size = 5)\n\n\nThe faceting option allows a collection of small plots with the same scales. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x=vehicle, y=people) + \n  geom_point() +\n  facet_wrap(~ river) \n\n\n\n\n\nFaceting is the ggplot2 option to create separate graphs for subsets of data. ggplot2 offers two functions for creating small multiples:\n\nfacet_wrap(): define subsets as the levels of a single grouping variable\nfacet_grid(): define subsets as the crossing of two grouping variables\n\nThe following arguments are common to most scales in ggplot2:\n\nname: the first argument gives the axis or legend title\nlimits: the minimum and maximum of the scale\nbreaks: the points along the scale where labels should appear\nlabels: the labels that appear at each break\n\nSpecific scale functions may have additional arguments. Some of the available Scales are:\n\n\n\nScale\nExamples\n\n\n\n\nscale_color_\nscale_color_discrete\n\n\nscale_fill_\nscale_fill_continuous\n\n\nscale_size_\nscale_size_manual\n\n\n\nscale_size_discrete\n\n\n\n\n\n\nscale_shape_\nscale_shape_discrete\n\n\n\nscale_shape_manual\n\n\nscale_linetype_\nscale_linetype_discrete\n\n\n\n\n\n\nscale_x_\nscale_x_continuous\n\n\n\nscale_x_log\n\n\n\nscale_x_date\n\n\nscale_y_\nscale_y_reverse\n\n\n\nscale_y_discrete\n\n\n\nscale_y_datetime\n\n\n\nIn RStudio, we can type scale_ followed by TAB to get the whole list of available scales.\nTry-\n\n\nCode\nggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point() + \n  scale_x_continuous(name = \"No. of Vehicles\") + \n  scale_y_continuous(name = \"No. of people\") + \n  scale_color_discrete(name = \"Temperature\")\n\n\n\n\n\nThe other coding option is shown below:\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point() + \n  xlab(\"No. of Vehicles\") + \n  ylab(\"No. of people\") + \n  labs(colour=\"Temperature\") \n\n\nNote that a desired graph can be obtained in more than one way.\nThe ggplot2 theme system handles plot elements (not data based) such as\n\nAxis labels\nPlot background\nFacet label background\nLegend appearance\n\nBuilt-in themes include:\n\ntheme_gray() (default)\ntheme_bw()\ntheme_minimal()\ntheme_classic()\n\n\n\nCode\np1 &lt;- ggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point()\n\n\nNote that the graph is assigned an object name p1 and nothing will be printed unless we then print the object p1.\n\n\nCode\np1 &lt;- ggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point()\n\np1\n\n\n\n\n\nTry-\n\n\nCode\np1 + theme_light()\n\n\n\n\n\n\n\nCode\np1 + theme_bw()\n\n\n\n\n\nSpecific theme elements can be overridden using theme(). For example:\n\n\nCode\np1 + theme_minimal() +\n  theme(text = element_text(color = \"red\"))\n\n\n\n\n\nAll theme options can be seen with ?theme.\nTo specify a theme for a whole document, use\n\n\nCode\ntheme_set(theme_minimal())\n\n\nMinimal graphing can be done using the qplot option that will produce a few standard formatted graphs quickly.\n\n\nCode\nqplot(people, vehicle, data = my.data, colour = river)\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nTry-\nqplot(people, data = my.data)\nqplot(people, fill=factor(river), data=my.data)\nqplot(people, data = my.data, geom = \"dotplot\")\nqplot(factor(river), people, data = my.data, geom = \"boxplot\")\nA cheat sheet for ggplot2 is available at https://www.rstudio.com/resources/cheatsheets/ (optional to download). There are many other packages which incorporate ggplot2 based graphs or dependent on it.\nThe library patchwork allows complex composition arbitrary plots, which are not produced using the faceting option. Try\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- qplot(people, data = my.data, geom = \"dotplot\")\np2 &lt;- qplot(people, data = my.data, geom = \"boxplot\")\np3 &lt;- ggplot(my.data, aes(x = vehicle, y = people)) + geom_point()\n\n(p1 + p2) / p3 + \n  plot_annotation(\"My title\", caption = \"My caption\")\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "workshops/ws01.html#ggplot-builder",
    "href": "workshops/ws01.html#ggplot-builder",
    "title": "Getting Started With R",
    "section": "ggplot builder",
    "text": "ggplot builder\nA nice R package, known as esquisse is available to build few simple ggplot graphics interactively. This may help in the early stages of learning to use ggplot graphing.\nIf this package is not installed, install it first & then try.\nlibrary(esquisse)\noptions(\"esquisse.display.mode\" = \"browser\")\nesquisse::esquisser(data = iris)\nYou can also load the desired dataset within R studio and select the dataset.\nThe other option is to load a dataset from the course data web folder and then launch esquisse. Try-\nurl1 &lt;- \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\"\ndownload.file(url = url1, destfile = \"rangitikei.RData\")\nload(\"rangitikei.RData\")\nesquisse::esquisser(data = rangitikei, viewer = \"browser\")\nYou can also download the associated R codes or save the graph within the esquisse web app."
  },
  {
    "objectID": "workshops/ws01.html#dplyr",
    "href": "workshops/ws01.html#dplyr",
    "title": "Getting Started With R",
    "section": "dplyr",
    "text": "dplyr\nThe following six functions of dplyr are very useful for data wrangling :\n\nFor selecting columns, use select()\nFor subsetting data, use filter()\nFor re-ordering (e.g. ascending/descending), use arrange()\nFor augmenting new calculated columns, use mutate()\nFor computing summary measures, use summarise()\nFor group-wise computations (e.g. summary measures), use group_by()\n\nThere are many other functions such as transmute() which will add newly calculated columns to the existing data frame but drop all unused columns. The across() function extends group_by() and summarise() functions for multiple column and function summaries. For example, you like to report rounded data in a table, which calls for an operation across both rows and columns."
  },
  {
    "objectID": "workshops/ws01.html#piping",
    "href": "workshops/ws01.html#piping",
    "title": "Getting Started With R",
    "section": "Piping",
    "text": "Piping\n\n\n\n\n\n\nTip\n\n\n\nThe piping operation is a fundamental aspect of computer programming. The semantics of pipes is taking the output from the left-hand side and passing it as input to the right-hand side.\n\n\nThe R package magrittr introduced the pipe operator %&gt;% and can be pronounced as “then”. In RStudio windows/Linux versions, press Ctrl+Shift+M to insert the pipe operator. On a Mac, use Cmd+Shift+M.\nR also has its own pipe, |&gt;, which is an alternative to %&gt;%. I tend to use |&gt;. If you want to change the pipe inserted automatically with Ctrl+Shift+M, find on the menu Tools &gt; Global Options, then click on Code and check the box that says “Use Native Pipe Operator”.\nWe often pipe the dplyr functions, and the advantage is that we show the flow of data manipulation and subsequent graphing. This approach also helps to save memory, and dataframes are not unnecessarily created, a necessity for a big data framework.\nTry the following examples after loading the rangitikei dataset.\nselect()\n\n\nCode\nmy.data &lt;- read.csv(\"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\", header=TRUE)\n\nnames(my.data)\n\n\n [1] \"id\"      \"loc\"     \"time\"    \"w.e\"     \"cl\"      \"wind\"    \"temp\"   \n [8] \"river\"   \"people\"  \"vehicle\"\n\n\n\n\nCode\nlibrary(tidyverse)\n\nnew.data &lt;- my.data |&gt; \n  select(people, vehicle)\n\nnames(new.data)\n\n\n[1] \"people\"  \"vehicle\"\n\n\n\n\nCode\nmy.data |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() + \n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nWe select two columns and create a scatter plot with the above commands.\nfilter()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() +\n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nThe above commands filter the data for the low wind days and plots vehicle against people.\narrange()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  arrange(w.e) |&gt; \n  select(w.e, people, vehicle)\n\n\n  w.e people vehicle\n1   1    136      23\n2   1     50      22\n3   1    100      31\n4   1    470     122\n5   2     22      11\n\n\nmutate()\nAssume that a $10 levy is collected for each vehicle. We can create this new levy column as follows.\n\n\nCode\nmy.data |&gt; \n  mutate(levy = vehicle*10) |&gt; \n  select(people, levy) |&gt; \n  ggplot() +\n  aes(x = people, y=levy) +\n  geom_point()\n\n\n\n\n\nNote that the pipe operation was used to create a scatter plot using the newly created column.\nsummarise()\n\n\nCode\nmy.data |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total      avg\n1    33 71.72727\n\n\nWe obtain the selected summary measures namely the total and the mean number of people. Try-\n\n\nCode\nmy.data |&gt; \n  filter(wind == 1) |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total   avg\n1     5 155.6\n\n\ngroup_by()\nWe obtain the wind group-wise summaries below:\n\n\nCode\nmy.data |&gt; \n  group_by(wind) |&gt; \n  summarise(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 3 × 3\n   wind total   avg\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     5 156. \n2     2    26  59.7\n3     3     2  19  \n\n\nThere are many more commands such as the transmute function which conserves the only the needed columns. Try\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  transmute(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 33 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e total   avg\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     2     1    18  72.1\n 2     2     1    18  72.1\n 3     2     1    18  72.1\n 4     2     1    18  72.1\n 5     2     1    18  72.1\n 6     1     1     4 189  \n 7     2     2     8  31.8\n 8     2     1    18  72.1\n 9     3     2     1   4  \n10     2     1    18  72.1\n# ℹ 23 more rows\n\n\nA simple frequency table is found using count(). Try-\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp)\n\n\n# A tibble: 10 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e  temp     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1\n 2     1     1     3     3\n 3     1     2     3     1\n 4     2     1     1     4\n 5     2     1     2    12\n 6     2     1     3     2\n 7     2     2     2     6\n 8     2     2     3     2\n 9     3     1     2     1\n10     3     2     1     1\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp, river)\n\n\n# A tibble: 16 × 5\n# Groups:   wind, w.e [6]\n    wind   w.e  temp river     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1     1\n 2     1     1     3     3     3\n 3     1     2     3     3     1\n 4     2     1     1     1     1\n 5     2     1     1     2     1\n 6     2     1     1     3     2\n 7     2     1     2     1     3\n 8     2     1     2     2     2\n 9     2     1     2     3     7\n10     2     1     3     3     2\n11     2     2     2     1     2\n12     2     2     2     3     4\n13     2     2     3     2     1\n14     2     2     3     3     1\n15     3     1     2     2     1\n16     3     2     1     2     1\n\n\nThe count() is useful to check the balanced nature of the data when many subgroups are involved."
  },
  {
    "objectID": "workshops/ws01.html#tidyr",
    "href": "workshops/ws01.html#tidyr",
    "title": "Getting Started With R",
    "section": "tidyr",
    "text": "tidyr\nBy the phrase tidy data, it is meant the preferred way of arranging data that is easy to analyse. The principles of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nThe hospital admissions dataset is untidy because it does allocate many columns for a variable.\n\n\nCode\nmy.data &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/hospital.txt\",\n  header=TRUE, sep=\",\")\n  \nhead(my.data)\n\n\n  YEAR PERI NORTH1 NORTH2 NORTH3 SOUTH1 SOUTH2 SOUTH3\n1 1980    1      0      4     27      4     16     27\n2 1980    2      6     11     31      8     18     21\n3 1980    3      6      4     25     20     16     24\n4 1980    4      1     10     31     22     17     20\n5 1980    5      4     16     22     21     30     31\n6 1980    6      3      8     28     31     20     30\n\n\nThe main response variable namely the number of admissions is allocated different columns depending on the North and South locations. This format is also called wide format which can be made into a tidy long format. Try-\n\n\nCode\nlibrary(tidyr)\n\nmy.data |&gt; \n  gather(NORTH1, NORTH2, NORTH3, \n         SOUTH1, SOUTH2, SOUTH3)\n\n\nThe command spread() does the opposite to gather(). The tidyr package many other functions such as unite(), separate() etc to deal with columns. A better approach would be to use the dplyr function pivot_longer(). Try-\n\n\nCode\nmy.data |&gt; \n  pivot_longer(cols = NORTH1:SOUTH3, \n               names_to = \"location\", \n               values_to = \"Admissions\")\n\n\nThe command pivot_wider() does the opposite to pivot_longer()\nThe dplyr package also has functions to deal with two-tables which can be joined either conditionally or unconditionally using commands such as full_join(). For a detailed notes and examples, you may visit https://dplyr.tidyverse.org/articles/two-table.html but we will be using such functions very occasionally in this course.\nThe reshape2 and data.table packages also have functions to do the same task."
  },
  {
    "objectID": "workshops/ws01.html#data-quality-checks",
    "href": "workshops/ws01.html#data-quality-checks",
    "title": "Getting Started With R",
    "section": "Data Quality Checks",
    "text": "Data Quality Checks\nIt is a good idea to check the quality of secondary data sourced from elsewhere. For example, there could be missing values in the dataset. Consider the Telomeres data downloaded from http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\n\n\nCode\nurl &lt;- \"http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\"\ndestfile &lt;- \"rsos192136_si_001.xlsx\"\n\ndownload.file(url, destfile)\n\n\n\n\nCode\nlibrary(readxl)\nrsos192136_si_001 &lt;- read_excel(\"rsos192136_si_001.xlsx\")\n\n\nThe missingness of data can be quickly explored using many R packages. The downloaded Telomeres dataset contain many missing values.\n\n\nCode\nlibrary(VIM)\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\nres &lt;- rsos192136_si_001 |&gt; \n  aggr(sortVar=TRUE) |&gt; \n  summary() |&gt; \n  pluck(\"combinations\")\n\n\n\n\n\nor\n\n\nCode\nlibrary(naniar)\ngg_miss_var(rsos192136_si_001) \n\n\nThe term Missing completely at random (MCAR) is often used to mean there is there is no pattern to the missing data themselves or alternatively the missingness is not related to any other variable or data in the dataset. In other words, the probability of missingness is the same for all units. So no bias is caused by the missing data, and we can discard cases with missing data when we fit models.\nIn practice, we often find missing data do have a relationship with other variables in the dataset but the actual missing values are random. This situation of data conditionally missing at random is called Missing at random (MAR) data. For a particular survey question, the response rate may differ depending on the respondent’s gender. In this situation, the actual missingness may be random but still related to the gender variable.\nMissing not at random (MNAR) is the pattern when missingness is related to other variables in the dataset, as well as the values of the missing data are not random. In other words, there is a predictable pattern in the missingness. So we cannot avoid the bias when missing cases are omitted.\nThere are also situations such as censoring where we just record a single value without actually measuring the variable of interest.\nImputation of data can be made except for the case of MCAR type. A number of R packages are available for data imputation; see https://cran.r-project.org/web/views/MissingData.html or https://stefvanbuuren.name/fimd/. We may occasionally cover data imputation issue in an assignment question.\nThere are also R packages to perform automatic investigation for data cleaning. Try-\n\n\nCode\nlibrary(dataMaid)\n\nmakeDataReport(rsos192136_si_001, output=\"html\", replace=TRUE)\n\n# or\nlibrary(DataExplorer)\n\ncreate_report(rsos192136_si_001)\n\n\nRule based validation is enabled in the R package validate. The R package janitor has a function get_dupes() to find duplicate entries in the dataset. Cleaner package will allow to clean the variables so that the columns are consistent in terms of the factor, date, numerical variable types. You will be largely using data that are already cleaned for your assignments but be aware that you have to perform data cleaning and perform necessary quality checks before analysis."
  },
  {
    "objectID": "workshops/ws03.html",
    "href": "workshops/ws03.html",
    "title": "Chapter 3 Workshop",
    "section": "",
    "text": "Dataset Prestige\nWe will be using a well-known dataset called Prestige from the car R package. This dataset deals with prestige ratings of Canadian Occupations. The Prestige dataset has 102 rows and 6 columns. The observations are occupations.\nThis data frame contains the following columns:\n\neducation - Average education of occupational incumbents, years, in 1971.\nincome - Average income of incumbents, dollars, in 1971.\nwomen - Percentage of incumbents who are women.\nprestige - Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.\ncensus - Canadian Census occupational code.\ntype - Type of occupation. A factor with levels: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar. (includes four missing values).\n\n\n\nExercise 3.1\nFor a standard normal variable \\(z\\) , obtain the area between -1.8 and 2.1.\n\n\nCode\npnorm(2.1, mean=0, sd=1) - pnorm(-1.8, mean=0, sd=1)\n\n\nNote that the mean=0, sd=1 are the defaults for pnorm function, so don’t need to be specified.\n\n\nCode\npnorm(2.1) - pnorm(-1.8)\n\n\n\n\nExercise 3.2\nPlot the prestige scores data as a histogram and show the theoretical normal curve fitted to the data.\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\n\nPrestige |&gt; \n  ggplot() + \n  aes(prestige) +\n  geom_histogram(aes(y=after_stat(density)), bins=10) +\n  stat_function(fun = dnorm,\n                args=list(mean=mean(Prestige$prestige),\n                          sd=sd(Prestige$prestige)), \n                geom = \"line\")\n\n\nor\n\n\nCode\nlibrary(car)\n\nhist(Prestige$prestige, probability=T)\n\ncurve(dnorm(x, \n            mean(Prestige$prestige), \n            sd(Prestige$prestige)), \n      add= T, lty=2)\n\n\n\n\nExercise 3.3\nObtain the normal quantile plot and test for the normality for prestige scores data.\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(sample=prestige) + \n  stat_qq() + \n  stat_qq_line()\n\n\nCode\n# or\nqqnorm(Prestige$prestige)\nqqline(Prestige$prestige)\n\n\n\n\nCode\nshapiro.test(Prestige$prestige)\n\nks.test(Prestige$prestige, \n        \"pnorm\", \n        mean(Prestige$prestige), \n        sd(Prestige$prestige) )\n\n\n\n\nExercise 3.4\nExamine the fit of non-normal distributions for prestige scores data.\n\n\nCode\nlibrary(fitdistrplus)\n\nm1 &lt;- fitdist(Prestige$prestige, \"lnorm\")\n\nplot(m1)\n\n\n\n\nCode\nlibrary(fitdistrplus)\n\nm2 &lt;- fitdist(Prestige$prestige, \"gamma\")\n\nplot(m2)\n\n\n\n\nCode\nlibrary(fitdistrplus)\n\nm3 &lt;- fitdist(Prestige$prestige, \"weibull\")\n\nplot(m3)\n\n\n\n\nCode\ndescdist(Prestige$prestige)\n\n\nMore graphing examples are here (R codes file)."
  },
  {
    "objectID": "workshops/ws05.html",
    "href": "workshops/ws05.html",
    "title": "Chapter 5 Workshop",
    "section": "",
    "text": "Dataset Toxaemia\nThis dataset is from the vcdExtra package. Two signs of toxaemia, an abnormal condition during pregnancy characterized by high blood pressure (hypertension) and high levels of protein in the urine. If untreated, both the mother and baby are at risk of complications or death. The dataset Toxaemia represents 13384 expectant mothers in Bradford, England in their first pregnancy, who were also classified according to social class and the number of cigarettes smoked per day.\nThe dataset is a a 5 x 3 x 2 x 2 contingency table, with 60 observations on the following 5 variables:\nclass - Social class of mother, a factor with levels: 1, 2, 3, 4, 5\nsmoke - Cigarettes smoked per day during pregnancy, a factor with levels: 0, 1-19, 20+\nhyper - Hypertension level, a factor with levels: Low, High\nurea - Protein urea level, a factor with levels: Low, High\nFreq - frequency in each cell, a numeric vector\n\n\nExercise 5.1\nObtain relevant graphical displays for this dataset.\nBar charts-\n\n\nCode\nlibrary(tidyverse)\n\nlibrary(vcdExtra)\ndata(Toxaemia)\n\nToxaemia |&gt; \n  ggplot() + \n  aes(x=smoke, y=Freq, fill=hyper) + \n  geom_bar(stat='identity')\n\n\nCode\nToxaemia |&gt; \n  ggplot() + \n  aes(x=smoke, y=Freq, fill=hyper) + \n  geom_bar(stat='identity', \n           position = \"dodge\"\n           )\n\n\nCode\nToxaemia |&gt; \n  ggplot() + \n  aes(x=smoke, y=Freq, fill=hyper) + \n  geom_bar(stat ='identity', \n           position = \"dodge\") + \n  facet_grid(urea ~ ., scales = \"free\")\n\n\nMosaic type charts\n\n\nCode\ntab.data &lt;- xtabs(Freq ~ smoke + hyper + urea, data=Toxaemia)\n\nplot(tab.data)\n\n\nCode\nmosaic(tab.data, shade=TRUE, legend=TRUE)\n\n\nCode\nassoc(tab.data, shade=TRUE) \n\n\nCode\nstrucplot(tab.data)\n\n\nCode\nsieve(tab.data)\n\n\n\n\nExercise 5.2\nThe genetic information of an organism is stored in its Deoxyribonucleic acid (DNA). DNA is a double stranded helix made up of four different nucleotides. These nucleotides differ in which of the four bases Adenine (A), Guanine (G), Cytosine (C), or Thymine (T) they contain. A simple pattern that we may want to detect in a DNA sequence is that of the nucleotide at position i+1 based on the nucleotide at position i. The nucleotide positional data collected by a researcher in a particular case is given in the following table:\n\n\n\ni\\(i+1)\nA\nC\nG\nT\n\n\n\n\nA\n622\n316\n328\n536\n\n\nC\n428\n262\n204\n306\n\n\nG\n354\n294\n174\n266\n\n\nT\n396\n330\n382\n648\n\n\n\nPerform a test of association and then obtain the symmetric plot.\n\n\nCode\ntabledata &lt;- data.frame(\n  A = c(622, 428, 354, 396),\n  C = c(316, 262, 294, 330),\n  G = c(328, 204, 174, 382),\n  T = c(536, 306, 266, 648), \n  row.names = c(\"A\", \"C\", \"G\", \"T\")\n  )\n\n\n\n\nCode\nchisq.test(tabledata)$exp\nchisq.test(tabledata)\nchisq.test(tabledata, simulate.p.value = T)\n\nlibrary(MASS)\ncorresp(tabledata)\nplot(corresp(tabledata, nf=2))\nabline(v=0)\nabline(h=0)\n\n\nCode\n#or\nlibrary(FactoMineR)\nCA(tabledata)\n\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws07.html",
    "href": "workshops/ws07.html",
    "title": "Chapter 7 Workshop",
    "section": "",
    "text": "Dataset Prestige\nWe will continue to use dataset Prestige from the car R package.\n\n\nExercise 7.1\nObtain the matrix plot of the numerical variables education, income, women, and prestige.\nObtain their correlation matrix.\nFit a (full) multiple regression of prestige on education, income, & women.\nObtain the plots for residual diagnostics.\n\n\nCode\nlibrary(car)\nlibrary(tidyverse)\nlibrary(GGally)\n\nPrestige |&gt; \n  select(prestige, education, income, women) |&gt; \n  ggpairs(aes(colour=Prestige$type))\n\n\n\n\nCode\n# Old style pairs plot\nPrestige |&gt; \n  select(prestige, education, income, women) |&gt;\n  pairs()\n\n\nCode\nPrestige |&gt; \n  select(prestige, education, income, women) |&gt;\n  cor()\n\n\nRegression outputs\n\n\nCode\nfull.reg &lt;- lm(prestige ~ education + income + women,\n               data = Prestige)\n\nsummary(full.reg)\n\nanova(full.reg)\n\nextractAIC(full.reg)\n\n\nResidual plots\n\n\nCode\nlibrary(ggfortify)\n\nautoplot(full.reg, 1:6)\n\n\n\n\nCode\n# Old style plots\nplot(full.reg, 1) # the argument 1 can be changed up to 6\n\n\nCode\n# or just use\npar(mfrow=c(2,2))\nplot(full.reg)\n\n\n\n\nExercise 7.2\nPerform stepwise regression analysis of prestige on education, income, & women.\n\n\nCode\nfull.reg = lm(prestige ~ education + income + women,\n              data = Prestige)\n\nstep(full.reg)\n\nstep(full.reg, direction=\"backward\")\n\nstep(full.reg, direction=\"both\")\n\n\nYou can also use the MASS package.\n\n\nCode\nlibrary(\"MASS\")\n\nstepAIC(full.reg, direction=\"backward\")\n\nstepAIC(full.reg, direction=\"both\")\n\n\nThe function update() will be handy. For example, see try the following codes:\n\n\nCode\nm1 = update(full.reg,.~.-women)\n\nsummary(m1)\n\n\nNote that .~.-women means that the model is fitted without the women variable.\nFurther options are available in leaps and HH packages (installation commands are given below).\ninstall.packages(\"leaps\", repos = \"https://cran.r-project.org\") install.packages(\"HH\", repos = \"https://cran.r-project.org\")\n\n\nCode\nlibrary(leaps)\n\nmodel = regsubsets(prestige ~ education + income + women, \n                   data=Prestige)\n\nlibrary(HH)\n\nsummaryHH(model)\n\nplot(summaryHH(model))\n\n\n\n\nExercise 7.3\nPerform a polynomial regression of prestige on income.\n\n\nCode\n# Cubic fit\np.model &lt;- lm(prestige ~ poly(income,3),\n              data = Prestige)\n\nsummary(p.model)\n\nextractAIC(p.model)\n\nplot(p.model)\n\n\nCode\nautoplot(p.model)\n\n\n\nMore R code examples are here"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-4",
    "href": "slides/Chapter03.html#continuous-probability-distributions-4",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  annotate(geom = \"path\", \n    x = c(19.3, 19.3, 13), \n    y = c(0, rep(dnorm(19.3,20,2),2) ),\n    arrow = arrow(),\n    colour = \"dodgerblue4\", size = 1.1)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe density at 19.3 is \\(f(19.3) = 0.1876\\)).\n\ndnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.1876202"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-5",
    "href": "slides/Chapter03.html#continuous-probability-distributions-5",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  geom_area(\n    data = d |&gt; filter(x &lt;= 19.3),\n    fill = \"dodgerblue4\",\n    size  = 1.1, alpha = .6)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe area under the curve to the left of the value 19.3 is given by the Cumulative Density Function (CDF), or \\(F(x)\\). It gives the probability that x &lt; 19.3; \\(F(19.3) = 0.3632\\).\n\npnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.3631693"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-6",
    "href": "slides/Chapter03.html#continuous-probability-distributions-6",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe cumulative distribution function, CDF, \\(F(x)\\) gives the left tail area or probability up to \\(x\\). This is probability is found as\n\\[F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt\\] The relationship between the density function \\(f(x)\\) and the distribution function \\(F(x)\\) is given by the Fundamental Theorem of Calculus.\n\\[f(x)={dF(x) \\over dx}\\]"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-7",
    "href": "slides/Chapter03.html#continuous-probability-distributions-7",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe total area under the PDF curve is \\(1\\). The probability of obtaining a value between two points (\\(a\\) and \\(b\\)) is the area under the PDF curve between those two points. This probability is given by \\(F(b)-F(a)\\).\nFor the uniform distribution \\(U(0,1)\\), \\(f(x)=1\\). So\n\\[F_{X}(x)=\\int _{-\\infty }^{x}\\,dt=x\\]\nFor example, the probability of a randomly drawn fraction from the interval \\([0,1]\\) to fall below \\(x=0.5\\) is 50%.\nThe probability of a random fraction falling between \\(a=0.2\\) and \\(b=0.8\\) is\n\\[F(b)-F(a)=0.8-0.2=0.6\\]\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "href": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Which distribution is most appropriate?",
    "text": "Which distribution is most appropriate?\n\nRemember, theoretical distributions aren’t real—they’re just models—but they can be useful. Keep your purpose in mind.\nChoose the simplest distribution that provides an adequate fit.\nData may be best served by a mixture of two or more distributions rather than a single distribution.\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter03.html#sensitivity-specificity-and-prevalence",
    "href": "slides/Chapter03.html#sensitivity-specificity-and-prevalence",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Sensitivity, specificity, and prevalence",
    "text": "Sensitivity, specificity, and prevalence\nLet \\(D\\) be the event of a person having the Disease and \\(H\\) be the event of a person being Healthy, that is, not having the disease.\nWe can test for whether someone has the disease. The outcome can be positive \\((T_+)\\) or negative \\((T_-)\\).\nConsider the following definitions:\n\nsensitivity of the test is the probability that the test is positive given the subject has the disease, or \\(P(T_+ | D)\\).\nspecificity of the test is the probability that the test is negative given the subject does not have the disease, or \\(P(T_- | H)\\).\nprevalence of the disease is the probability of a randomly selected subject from the population having the disease, or , or \\(P(D)\\).\n\nBoth specificity and sensitivity are conditional probabilities.\nPrevalence is the marginal or prior probability."
  },
  {
    "objectID": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "href": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Prevalence, sensitivity, specificity, PPV, and NPV",
    "text": "Prevalence, sensitivity, specificity, PPV, and NPV\nLet \\(D\\) be the event of a person having the Disease and \\(H\\) be the event of a person being Healthy (i.e., not having the disease). The outcome of a test for the disease can be either positive \\((T_+)\\) or negative \\((T_-)\\).\nConsider the following definitions of conditional probabilities:\n\nprevalence is the overall probability one has the disease, or \\(P(D)\\).\nsensitivity the probability that one tests positive given one has the disease, or \\(P(T_+ | D)\\).\nspecificity the probability that one tests negative given one does not have the disease, or \\(P(T_- | H)\\).\npositive predictive value of a test is the probability one has the disease given that one has tested positive, or \\(P(D \\mid T_{+})\\)\nnegative predictive value of a test is the probability that one is healthy given that one has tested negative, or \\(P(H \\mid T_{-})\\)"
  },
  {
    "objectID": "slides/Chapter03.html#table-of-probabilities",
    "href": "slides/Chapter03.html#table-of-probabilities",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Table of probabilities",
    "text": "Table of probabilities\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97\n\n\n\n0.0779\n0.9221\n1"
  },
  {
    "objectID": "slides/Chapter03.html#probability-tree",
    "href": "slides/Chapter03.html#probability-tree",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability tree",
    "text": "Probability tree\nIt can be useful to visualise the probabilities of the four possible states using a tree diagram.\n\n\n\n\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches.\n\n\n\n\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued-1",
    "href": "slides/Chapter03.html#example-continued-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat are the implications of a positive or negative test result?\nAccording to Bayes rule, the probability of a random person having the disease given they’ve tested positive is given by:\n\\[\n\\begin{aligned}\nP(D\\mid T_{+}) &= \\frac {P(T_{+}\\mid D)P(D)} {P(T_{+})} \\\\\n&= \\frac{0.98 \\times 0.03}  {0.0779} \\\\\n&= 0.3774\n\\end{aligned}\n\\]\nAccording to Bayes rule, the probability of a random person not having the disease given they’ve tested negative is given by:\n\\[\n\\begin{aligned}\nP(H \\mid T_{-}) &= \\frac {P(T_{-} \\mid H)P(H)} {P(T_{-})} \\\\\n&= \\frac{0.95 \\times 0.97}  {0.9221} \\\\\n&= 0.9993\n\\end{aligned}\n\\]\nThe positive predictive value of the test is poor—only 38% of the subjects who tested positive will have the disease.\nThe negative predictive value is better—if a random subject tests negative, they’re very unlikely to have the disease."
  },
  {
    "objectID": "studyguide/4-inference.html#footnotes",
    "href": "studyguide/4-inference.html#footnotes",
    "title": "Chapter 4: Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBear in mind though that no real population of values (except for synthetic ones, simulated by a computer) is normally distributed in reality. A variable can only be normally distributed in theory. The real world just is what it is. Recall Box’s adage: “All models are wrong, but some are useful”. When we test for a departure of a variable from, say, a normal distribution, we are simply testing whether the normal distribution provides an adequate model for the data.↩︎"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-1",
    "href": "slides/Chapter04.html#statistical-inference-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nThe term statistical inference means that we are using properties of a sample to make statements about the population from which the sample was drawn.\nFor example, say you wanted to know the mean length of the leaves on a tree (\\(\\mu\\)). You wouldn’t want to (nor need to) measure every single leaf! You would take a random sample of leaves and measure their lengths (\\(x_i\\)), calculate the sample mean (\\(\\bar x\\)), and use \\(\\bar x\\) as an estimate of \\(\\mu\\)."
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-2",
    "href": "slides/Chapter04.html#statistical-inference-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nWhenever we use sample data to make statements about a population (i.e., do inference):\n\nThe sample mean \\(\\bar x\\) is a single draw from a random variable \\(\\bar X\\). It will never be exactly equal to \\(\\mu\\) unless you measure every single leaf.\nEach sample yields a different \\(\\bar x\\). This is called sampling error.\nTherefore, there is uncertainty in our estimate of \\(\\mu\\).\n\n\n\n\n\nThe variability of the sample means from sample to sample, \\(\\text{Var}(\\bar X)\\), depends on two things: the variability of the population values, \\(\\text{Var}(x)\\), and the size of the sample, \\(n\\).\nThe variability of the sample estimate of a parameter is usually expressed as a standard error, which is simply the theoretical standard deviation of \\(\\bar X\\) from sample to sample."
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset",
    "href": "slides/Chapter04.html#quetelets-dataset",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\nIn 1846, a Belgian scholar Adolphe Quetelet published an analysis of the chest sizes of a full population of 5,738 Scottish soldiers.\nThe distribution of the measurements (in inches) in his database has a mean of \\(\\mu\\) = 39.83 inches, a standard deviation of \\(\\sigma\\) = 2.05 inches, and is well approximated by a normal distribution.\n\n\n\n\nAdolphe Quetelet1796-1874\n\n\n\n\n\n\nqd &lt;- tibble(\n  Chest = 33:48, \n  Count = c(3, 18, 81, 185, 420, 749, 1073, 1079, \n            934, 658, 370, 92, 50, 21, 4, 1),\n  Prob = Count / sum(Count)\n  )\n\nqd |&gt; \n  ggplot() +\n  aes(x = Chest, y = Count) +\n  geom_col() +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"Chest circumferences of Scottish soldiers\")"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-1",
    "href": "slides/Chapter04.html#quetelets-dataset-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# A single sample\nsample(qd_long, size = 6)\n\n[1] 39 42 38 37 40 36\n\n\n\n\n\n# Ten samples\nmap(1:7, ~ sample(qd_long, size = 6))\n\n[[1]]\n[1] 40 40 40 40 41 41\n\n[[2]]\n[1] 40 41 39 41 42 40\n\n[[3]]\n[1] 42 43 41 43 38 37\n\n[[4]]\n[1] 40 38 41 41 38 39\n\n[[5]]\n[1] 42 46 39 38 39 42\n\n[[6]]\n[1] 39 39 43 43 42 40\n\n[[7]]\n[1] 40 39 40 41 40 43"
  },
  {
    "objectID": "slides/Chapter04.html#sampling-variation",
    "href": "slides/Chapter04.html#sampling-variation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Variation",
    "text": "Sampling Variation"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-2",
    "href": "slides/Chapter04.html#quetelets-dataset-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\nhead(q_samples, 15)\n\n# A tibble: 15 × 2\n   sample values\n   &lt;fct&gt;   &lt;int&gt;\n 1 1          39\n 2 1          39\n 3 1          42\n 4 1          39\n 5 1          41\n 6 1          41\n 7 2          36\n 8 2          40\n 9 2          39\n10 2          38\n11 2          41\n12 2          44\n13 3          41\n14 3          40\n15 3          39"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-3",
    "href": "slides/Chapter04.html#quetelets-dataset-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\n\n\n# Calculate means of each sample\nsample_means &lt;- q_samples |&gt; group_by(sample) |&gt; \n  summarise(mean = mean(values))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample  mean\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 1       40  \n 2 2       38.5\n 3 3       40.5\n 4 4       40  \n 5 5       38.2\n 6 6       39.5\n 7 7       40.8\n 8 8       39.7\n 9 9       39.8\n10 10      39.8"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-4",
    "href": "slides/Chapter04.html#quetelets-dataset-4",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\n\n# Make a function to do it all and plot\nplot_samples &lt;- function(dat = qd_long,\n                         no_samples = 12,\n                         sample_size = 6) {\n\n  # Put samples in a tibble\n  q_samples &lt;- tibble( \n    sample = as_factor(1:no_samples),\n    values = map(sample, ~ sample(dat, size = sample_size))\n    ) |&gt; unnest()\n  \n  # Calculate means of each sample\n  sample_means &lt;- q_samples |&gt; group_by(sample) |&gt; \n    summarise(mean = mean(values))\n  \n  ggplot() + \n    xlim(min(dat), max(dat)) +\n    geom_vline(xintercept = mean(dat), alpha = .4) +\n    geom_jitter(\n      data = q_samples,\n      mapping = aes(y = sample, x = values),\n      width = 0, height = 0.1, alpha = .8\n      ) + \n    geom_point(\n      data = sample_means,\n      mapping = aes(y = sample, x = mean),\n      shape = 15, size = 3, \n      colour = \"dark orange\"\n      ) +\n    ggtitle(paste(\"Sample size =\", sample_size))\n}\n\n\n\nplot_samples(sample_size = 6)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-5",
    "href": "slides/Chapter04.html#quetelets-dataset-5",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.8 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n\nggplot() +\n  geom_jitter(\n    data = q_samples,\n    mapping = aes(x = sample, y = values),\n    width = 0.05, height = 0\n    ) + \n  geom_point(\n    data = sample_means,\n    mapping = aes(x = sample, y = mean),\n    shape = 15, size = 3,\n    colour = \"dark orange\"\n    ) + \n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "href": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sample means and sample sizes",
    "text": "Sample means and sample sizes\n\n\nLet’s compare the distribution of means for different sample sizes.\n\nplot_samples(sample_size = 3)\nplot_samples(sample_size = 5)\nplot_samples(sample_size = 10)\nplot_samples(sample_size = 20)\nplot_samples(sample_size = 50)\nplot_samples(sample_size = 100)\nplot_samples(sample_size = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe larger the \\(n\\), the less the sample means vary."
  },
  {
    "objectID": "slides/Chapter04.html#standard-error",
    "href": "slides/Chapter04.html#standard-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Standard error",
    "text": "Standard error\nIf is distributed as a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[X \\sim \\text{Normal}(\\mu, \\sigma)\\]\nthen sample means of size \\(n\\) are distributed as\n\\[\\bar X \\sim \\text{Normal}(\\mu_\\bar{X} = \\mu, \\sigma_\\bar{X} = \\sigma/\\sqrt{n})\\] \\(\\text{SE}(\\bar{X}) = \\sigma_\\bar{X}= \\sigma/\\sqrt{n})\\) is known as the standard error of the sample mean.\nMore generally, a standard error is the standard deviation of an estimated parameter over \\(\\infty\\) theoretical samples.\nAccording to the Central Limit Theorem (CLT), raw \\(X\\) values don’t have to be normally distributed for the sample means to be normally distributed (for any decent sample size)!"
  },
  {
    "objectID": "slides/Chapter04.html#standard-error-of-means-for-samples-of-scottish-soldiers",
    "href": "slides/Chapter04.html#standard-error-of-means-for-samples-of-scottish-soldiers",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Standard error of means for samples of Scottish soldiers",
    "text": "Standard error of means for samples of Scottish soldiers\nIf Quetelet’s dataset of chest circumferences is distributed as \\(\\text{N}(\\mu=39.8, \\sigma=2.05)\\), then how variable are the means of samples of size \\(n\\) = 6?\n\\[\n\\begin{aligned}\nSE(\\bar{X}_{n=6})&=\\sigma/\\sqrt{n} \\\\\n&=2.05/\\sqrt{6} \\\\\n&=0.8369\n\\end{aligned}\n\\] So sample means of size \\(n\\) = 6 would be distributed as \\(\\bar{X}_{n=6} \\sim \\text{N}(\\mu=39.8, \\sigma=0.8369)\\)."
  },
  {
    "objectID": "slides/Chapter04.html#standard-error-of-means-for-samples-of-scottish-soldiers-1",
    "href": "slides/Chapter04.html#standard-error-of-means-for-samples-of-scottish-soldiers-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Standard error of means for samples of Scottish soldiers",
    "text": "Standard error of means for samples of Scottish soldiers"
  },
  {
    "objectID": "slides/Chapter04.html#distribution-of-means-for-samples-of-scottish-soldiers",
    "href": "slides/Chapter04.html#distribution-of-means-for-samples-of-scottish-soldiers",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Distribution of means for samples of Scottish soldiers",
    "text": "Distribution of means for samples of Scottish soldiers"
  },
  {
    "objectID": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "href": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Estimating the distribution of sample means from data",
    "text": "Estimating the distribution of sample means from data\n\n\nIt’s all very well deriving the distribution of sample means when we know the population parameters (\\(\\mu\\) and \\(\\sigma\\)) but in most cases we only have our one sample.\nWe don’t know any of the population parameters. We have to estimate the population mean (usually denoted \\(\\hat\\mu\\) or \\(\\bar x\\)) and estimate of the population standard deviation (usually denoted \\(\\hat\\sigma\\) or \\(s\\)) from the sample data.\nThis additional uncertainty complicates things a bit. In fact, we can’t even use the normal distribution any more!"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Standard error of means of samples",
    "text": "Quetelet’s chests: Standard error of means of samples\nIf Quetelet’s chest circumferences are distributed as \\(X \\sim \\text{N}(\\mu=39.83, \\sigma=2.05)\\), then how variable are the means of samples of size \\(n\\) = 6?\n\\[\n\\begin{aligned}\nSE(\\bar{X}_{n=6})&=\\sigma/\\sqrt{n} \\\\\n&=2.05/\\sqrt{6} \\\\\n&=0.8369\n\\end{aligned}\n\\] Sample means of size \\(n\\) = 6 would be distributed as\n\\(\\bar{X}_{n=6} \\sim \\text{N}(\\mu=39.83, \\sigma=0.8369)\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Distribution of means of samples",
    "text": "Quetelet’s chests: Distribution of means of samples"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals",
    "href": "slides/Chapter04.html#confidence-intervals",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nAnother common way of expressing uncertainty when making statistical inferences is to present:\n\na point estimate (e.g., the sample mean) and\na confidence interval around that estimate.\n\nA confidence interval gives an indication of the sampling error.\nA 95% confidence interval is constructed so that intervals from 95% of samples will contain the true population parameter.\nIn reality the interval either contains the true value or not, so you must not interpret a confidence interval as “there’s a 95% probability that the interval contains the true parameter”.\nWe can say:\n\n“95% of so-constructed intervals will contain the true value of the parameter” or\n“with 95% confidence, the interval contains the true value of the parameter”."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-1",
    "href": "slides/Chapter04.html#confidence-intervals-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nBecause we know the population parameters for the Quetelet dataset, we can calculate were 95% of sample means will lie for any particular sample size.\nRecall that, for a normal distribution, 95% of values lie between \\(\\mu \\pm 1.96\\times\\sigma\\)\n(i.e., \\(\\mu - 1.96\\times\\sigma\\) and \\(\\mu + 1.96\\times\\sigma\\)).\nIt follows that 95% of means of samples of size \\(n\\) will lie within \\(\\mu \\pm 1.96 \\times \\sigma/\\sqrt{n}\\).\nFor example, for samples of size 6 from the Quetelet dataset, 95% of means will lie within \\(39.83 \\pm 1.96\\times 2.05 / \\sqrt 6\\),\nso \\(\\{ 37.02 , 42.64\\}\\)."
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution",
    "href": "slides/Chapter04.html#the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\n\n\nIntroducing William Sealy Gosset, who described the t distribution after working with random numbers.\nGosset was a chemist and mathematician who worked for the Guinness brewery in Dublin.\nGuinness forbade anyone from publishing under their own names so that competing breweries wouldn’t know what they were up to, so he published his discovery in 1908 under the penname “Student”. Student’s identity was only revealed when he died. It is therefore often called “Student’s t distribution”.\n\n\n\n\n\nWillam Sealy Gosset1876-1937"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-1",
    "href": "slides/Chapter04.html#the-t-distribution-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\nThe t distribution is like the standard normal. It is bell-shaped and symmetric with mean = 0, but it has fatter tails (greater uncertainty) due to the fact that we do not know \\(\\sigma\\); we have to estimate it.\nThe t distribution is not just a single distribution. It is really an entire series of distributions which is indexed by something called the “degrees of freedom” (or \\(df\\)).\n\n\n\n\nAs \\(df \\rightarrow \\infty\\), \\(t \\rightarrow Z\\).\n\n\np &lt;- expand_grid(\n  df = c(2, 5, Inf),\n  x = seq(-4, 4, by = .01)\n  ) |&gt; \n  mutate(\n    Density = dt(x = x, df = df),\n    `degrees of freedom` = as_factor(df)\n  ) |&gt; \n  ggplot() +\n  aes(x = x, y = Density, \n      group = `degrees of freedom`, \n      colour = `degrees of freedom`) +\n  geom_line()"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-2",
    "href": "slides/Chapter04.html#the-t-distribution-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\nFor a sample of size \\(n\\), if\n\n\\(X\\) is a normal random variable with mean \\(\\mu\\),\n\\(\\bar X\\) is the sample mean, and\n\\(s\\) is the sample standard deviation,\n\nthen the variable:\n\\[\nT = \\frac{\\bar X - \\mu} {s/\\sqrt{n}}\n\\]\nis distributed as a \\(t\\) distribution with \\((n – 1)\\) degrees of freedom."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "href": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals and the t distribution",
    "text": "Confidence intervals and the t distribution\nThe process of using sample data to try and make useful statements about an unknown parameter, \\(\\theta\\), is called statistical inference.\nA confidence interval for the true value of a parameter is often obtained by:\n\\[\n\\hat \\theta \\pm t \\times \\text{SE}(\\hat \\theta)\n\\] where \\(\\hat \\theta\\) is the sample estimate of \\(\\theta\\).\nThe piece that is being added and subtracted, \\(t \\times \\text{SE}(\\hat \\theta)\\), is often called the margin of error."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution-1",
    "href": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals and the t distribution",
    "text": "Confidence intervals and the t distribution"
  },
  {
    "objectID": "slides/Chapter04.html#t-as-a-test-statistic",
    "href": "slides/Chapter04.html#t-as-a-test-statistic",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "t as a test statistic",
    "text": "t as a test statistic\nThis method can be used when the estimator \\(\\hat\\theta\\) is approximately normally distributed and\n\\[\n\\frac{\\hat\\theta - \\theta} {\\text{SE}(\\hat \\theta)}\n\\]\nhas approximately a Student’s t distribution.\nThis paves the way for hypothesis testing for specific values of \\(\\theta\\). Many of the methods you will learn in this course are based on this general rule."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "href": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for a sample mean",
    "text": "Confidence intervals for a sample mean\nWe can calculate a 95% confidence interval for a sample mean with the following information:\n\n\nsample mean \\(\\bar x\\),\nsample standard deviation \\(s\\),\nthe sample size \\(n\\), and\nthe 0.025th quantile of the \\(t\\) distribution with degrees of freedom \\(df=n-1\\).\n\n\n\n\nA sample of size \\(n\\) = 6 from Quetelet data\n\nn1 &lt;- 6; df1 &lt;- n1-1\n( dq1 &lt;- sample(qd_long, size = n1) )\n\n[1] 39 42 38 36 40 38\n\n( mean1 &lt;- mean(dq1) )\n\n[1] 38.83333\n\n( sd1 &lt;- sd(dq1) )\n\n[1] 2.041241\n\n( t1 &lt;- qt(c(0.025, 0.975), df = df1) )\n\n[1] -2.570582  2.570582\n\nmean1 + t1 * sd1 / sqrt(n1)\n\n[1] 36.69118 40.97548\n\n\n\n\n\n\nOr, more simply…\n\nt.test(dq1)\n\n\n    One Sample t-test\n\ndata:  dq1\nt = 46.6, df = 5, p-value = 8.595e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.69118 40.97548\nsample estimates:\nmean of x \n 38.83333"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean-1",
    "href": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for a sample mean",
    "text": "Confidence intervals for a sample mean\nWe can calculate a 95% confidence interval for a sample mean with the following information:\n\n\nsample mean \\(\\bar x\\),\nsample standard deviation \\(s\\),\nthe sample size \\(n\\), and\nthe 0.025th quantile of the \\(t\\) distribution with degrees of freedom \\(df=n-1\\).\n\n\nOr, more simply…\n\nn1 &lt;- 6; df1 &lt;- n1-1\n( dq1 &lt;- sample(qd_long, size = n1) )\n\n[1] 43 43 41 40 38 40\n\nt.test(dq1)\n\n\n    One Sample t-test\n\ndata:  dq1\nt = 51.536, df = 5, p-value = 5.2e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 38.79660 42.87007\nsample estimates:\nmean of x \n 40.83333"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean-2",
    "href": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for a sample mean",
    "text": "Confidence intervals for a sample mean"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "href": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for sample means",
    "text": "Confidence intervals for sample means"
  },
  {
    "objectID": "slides/Chapter04.html#testing-hypotheses",
    "href": "slides/Chapter04.html#testing-hypotheses",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Testing hypotheses",
    "text": "Testing hypotheses\n\nTesting hypotheses underpins a lot of scientific work.\nA hypothesis is a proposition, a specific idea about the state of the world that can be tested with data.\nFor logical reasons, instead of measuring evidence for a hypothesis of interest, scientists will often:\n\nspecify a null hypotheses, a hypothesis that is true if our hypothesis of interest is false, and\nmeasure the evidence against the null hypothesis, usually in the form of a p-value."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nSay a farmer has 9 paddocks of alfalfa. He’s hired a new manager, and wants to know if, on average, this year’s crop is different to last year’s. He measures the yield for each paddock in year 1 and year 2, and calculates the difference.\n\nyear1 &lt;- c(0.8, 1.3, 1.7, 1.7, 1.8, 2.0, 2.0, 2.0, 2.2)\nyear2 &lt;- c(0.7, 1.4, 1.8, 1.8, 2.0, 2.0, 2.1, 2.1, 2.2)\ndiff &lt;- year2 - year1\n( xbar &lt;- mean(diff) ) ; ( s &lt;- sd(diff) )\n\n[1] 0.06666667\n\n\n[1] 0.08660254\n\n\nThe mean difference is 0.067. On average, yields were 0.067 greater than last year.\nIs that convincing different from zero?\nHow likely is such a difference to have arisen just by chance, and really, if we had a million paddocks, there would be no difference?\nWhat is the probability of seeing a difference of 0.067 or more in our dataset if the true mean were zero?\nThese questions can be addressed with a \\(p\\)-value from a hypothesis test."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nWe can test the null hypothesis using a t statistic \\(t_0 = \\frac{\\bar x - \\mu_0} {text{SE}(\\bar x)}\\), where\n\\(\\ \\ \\ \\ \\ \\mu_0 = 0\\) is the hypothesised value of the mean and\n\\(\\ \\ \\ \\ \\ \\text{SE}(\\bar x) = s/\\sqrt{n}\\) is the standard error of the sample mean.\n\n( t0 &lt;- (xbar - 0) / ( s / sqrt(9) ) )\n\n[1] 2.309401\n\n\nThe p-value is \\(\\text{Pr}(|t_{df=8}| &gt; t_0)\\)\n\n2 * pt(t0, df = 8, lower = F)\n\n[1] 0.04973556"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nThis can be done more easily:\n\n\n\n    One Sample t-test\n\ndata:  diff\nt = 2.3094, df = 8, p-value = 0.04974\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 9.806126e-05 1.332353e-01\nsample estimates:\n mean of x \n0.06666667"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe \\(p\\)-value of 0.05 means that, if the null hypothesis were true, only 5% of sample means would be as or more extreme than the observed value of 0.067. It is the area in orange in the graph below.\nWe can therefore reject the null hypothesis at the conventional 5% level, and conclude that, on average, yields were indeed higher this year.\nThis is an example of a paired t test. They two samples (year 1 and year 2) are not independent of one another because we have the same paddocks in both years. So, we take the differences, treat them like a single sample, and do a one-sample t test for the mean difference being zero.\n\\(p\\)-values are random variables too.\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/"
  },
  {
    "objectID": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "href": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Shiny apps (some not currently working)",
    "text": "Shiny apps (some not currently working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/\nFor more on non-parametric tests, see Study Guide."
  },
  {
    "objectID": "slides/Chapter04.html#tests-for-normality",
    "href": "slides/Chapter04.html#tests-for-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Tests for normality",
    "text": "Tests for normality\n\nEDA approaches\n\nnormal quantile plot, Boxplots,mids vs. spreads plot etc.\n\nHypothesis tests\n\nKolmogorov-Smirnov test (based on the biggest difference between the empirical and theoretical cumulative distributions)\nShapiro-Wilk test (based on variance of the difference)\n\nExample: N(100,1) random data of size \\(n=50\\)\n\nThe null hypothesis of normality must be justified on empirical grounds\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rnorm(50, mean = 100)\nW = 0.94504, p-value = 0.02142\n\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  rnorm(50)\nD = 0.088169, p-value = 0.7995\nalternative hypothesis: two-sided"
  }
]