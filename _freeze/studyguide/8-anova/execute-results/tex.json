{
  "hash": "372027b60c25bb1f9c916394525505c0",
  "result": {
    "markdown": "---\ntitle: \"Chapter 8: Analysis of Variance (ANOVA) and Covariance (ANCOVA)\"\n---\n\n\n\n> \"*To consult a statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.*\"\n> â€“ Sir RA Fisher\n\n# Introduction to Analysis of Variance (ANOVA)\n\nWe consider **ANOVA models** for data which have, in many cases, been collected using experimental designs. The model provides a quantitative assessment of the treatment effects. The first model we shall consider is the **One-Way ANOVA** model. The data for these models are used when only one set of treatments is to be compared. Here the set of treatments is called a factor and the various treatments are referred to as the factor levels.\n\nA One-Way ANOVA model is expressed in terms of an overall effect (the overall response mean) and a treatment effect (the average effect for a particular treatment or level on the response variable). The model has the form\n\n**fit = overall effect + treatment effect**.\n\nThe second model we shall consider later is a Two-Way Model. The experiments used to produce these data may be from two sets of treatments (that is, two factors). Again we refer to the treatments for each factor as the levels for each factor. We shall assume that the data are fully balanced in that the same number of observations is obtained for each combination of factor levels. For instance, if factor 1 has 3 levels and factor 2 has 2 levels then the total number of observations must be a multiple of 6.\n\nThe computer output for an ANOVA often includes the mean response for each combination of factor levels. These means are often represented as a table with the rows corresponding to the levels of the first factor and the columns corresponding to the levels of the second factor. For convenience we shall refer to the levels of factor 1 as the `rows` and we shall refer to the levels of factor 2 as the `columns`. There are two possible Two-Way ANOVA models. The first model assumes that the effects of factors 1 and 2 are independent. The appropriate model is expressed in terms of the overall effect (overall mean), the row effect and the column effect. That is\n\n**fit = overall effect + row effect + column effect**.\n\nThe second Two-Way ANOVA allows for an **interaction** between the row and column effects. That is\n\n**fit = overall effect + row effect + column effect + interaction effect**.\n\nFinally we shall consider an Analysis of Covariance (ANCOVA) model. In these experiments paired ($X$, $Y$) measurements are obtained for each observation. $X$ is the explanatory variable and $Y$ is the response variable. When there is only one experimental factor the following model is applied,\n\n**fit = overall effect + treatment effect +** $X$-effect.\n\nANCOVA models are, in essence, a mixture of regression and ANOVA models. For all the above models\n\n**observed = fitted + residual**.\n\nThe residuals are used to test the adequacy of the model fitted as in regression.\n\n# One Way ANOVA\n\nAs part of a multi-lab study, four fabrics were tested for flammability at the National Bureau of Standards. The burn times in seconds were recorded after a paper tab was ignited on the hem of each fabric. The experimental data are presented below (@tbl-fabricdata; taken from p.496 of @bhattacharyya1977.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_minimal())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/fabric.RData\",\n  destfile = \"fabric.RData\")\n\nload(\"fabric.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndtf <- fabric |> unstack(burntime ~ Fabric)\n\ndtf\n```\n:::\n\n::: {#tbl-fabricdata .cell tbl-cap='Fabric Burntime data'}\n::: {.cell-output-display}\n| Fabric.1| Fabric.2| Fabric.3| Fabric.4|\n|--------:|--------:|--------:|--------:|\n|     17.8|     11.2|     11.8|     14.9|\n|     16.2|     11.4|     11.0|     10.8|\n|     17.5|     15.8|     10.0|     12.8|\n|     17.4|     10.0|      9.2|     10.7|\n|     15.0|     10.4|      9.2|     10.7|\n:::\n:::\n\n\n\nNote that the response variable $Y$ is quantitative in that they are measurements of burn times in seconds. The five measurements for each fabric are assumed to be a random sample for all possible burn times for that fabric so that the five readings are true replications.\n\nIn this example, each of the four fabrics is replicated the same number of times, namely five, so the experiment is said to be \"balanced\". The approaches explained in this section on One-Way ANOVA can easily be extended to unbalanced experiments. This is not the case for Two-Way ANOVA.\n\nUsing the same approach as in Chapter 4, we could compare any two fabrics by a $t$-test or by considering the confidence interval for the difference in means. With 4 fabrics, there are 6 comparisons which could be made (Fabric 1 vs. Fabric 2 etc.).\n\nHow should we proceed? Should we test each of these 6 differences in means? Unfortunately, there is a problem in this procedure. One tends to find too many significant differences between means. There are two reasons for this:\n\n1.  The comparisons are not independent. If one fabric mean happened to be unduly large by chance, every comparison involving that mean would tend to yield a significant (say, at the 5% level) difference with each of the other means.\n\n2.  For one comparison, the 95% confidence level (5% level of the $t$-test) would yield a significant difference between the means in about 5% of samples of these two fabrics **even if the fabrics responded equally to the flammability test.** The more comparisons that are made on pairs of fabrics, the more likely it is that sooner or later some of the difference will appear significant at the 5% level, even when there is no difference between the fabrics. This is true for any statistical test and not just $t$-tests. If a number of tests are carried out on the same set of data, eventually some false significant results will be obtained. It has been suggested that if you torture data enough, it will confess, whether or not it is guilty.\n\nOne way out of this dilemma is to carry out a single test, an $F$-test, which can be thought of as giving an average value of the comparisons between treatments. The way it does this should become clear in the next few sections.\n\nThe burn times are plotted in @fig-fabricstrip in which the scale on the horizontal axis indicates the number of the fabric. The times for fabrics 2 and 4 appear fairly similar to the times for fabric 3. But the burn times for fabric 1 appear to be higher.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfabric |> \n  ggplot() +\n  aes(x = Fabric, y = burntime) +\n  geom_hline(aes(yintercept = mean(burntime)), alpha = .7)  + \n  geom_point() + \n  annotate(\"text\",\n           x = .7,\n           y = mean(fabric$burntime) + 0.3,\n           label = \"Overall mean\",\n           alpha=.7\n           ) \n```\n\n::: {.cell-output-display}\n![Burn time vs fabric Id](8-anova_files/figure-pdf/fig-fabricstrip-1.pdf){#fig-fabricstrip fig-pos='H'}\n:::\n:::\n\n\n\nLet $y_{ij}$ be the $j^{th}$ observed value in the $i^{th}$ treatment (fabric type in the above example); $\\bar{y}_{i}$ be the mean of the $i^{th}$ treatment; $\\bar{y}$ be the overall mean of all the observations; $k$ is the total number of treatments being compared. As usual the observed data can be modelled by\n\nobserved = fit + residual.\n\nHowever, the appropriate model for the fit is different for these data. For a treatment $i$ observation\n\nfit =$\\bar{y}_{i}$, the mean of the $i^{th}$ treatment\n\nor, adding and subtracting $\\bar{y}$ ,\n\nfit =$\\bar{y} + (\\bar{y}_{i} - \\bar{y})$ =overall mean + treatment effect\n\nClearly, residual = observed - fit; so the residual for $y_{ij}$ is equal to\n\nresidual = $y_{ij}-(\\bar{y} + (\\bar{y}_{i}- \\bar{y})) = y_{ij} - \\bar{y}_{i}$\n\nSince observed = fit + residual, we have $$y_{ij} = \\bar{y} + (\\bar{y}_{i} -\\bar{y}) + (y_{ij} - \\bar{y}_{i} )$$\n\nobserved = overall mean + treatment effect + residual.\n\nThat is $$y_{ij} = m + t_{i} + e_{ij}$$ where the three terms on the right are:\n\n$m$ =: the overall mean, $\\bar{y}$,\n\n$t_{i}$ =: the treatment effect, **adjusted** for the mean. i.e. ($\\bar{y}_{i} -\\bar{y}$),\n\n$e_{ij}$ =: the residual or error, ($y_{ij}$- $\\bar{y}_{i}$). See @fig-6-3 (produced manually) for an illustration of the treatment effect for the fabric data.\n\n\n![Explanation  of factor effects](images/6-3.png){#fig-6-3}\n\n\nFrom the above model we obtain the equation $$(y_{ij}-\\bar{y}) = (\\bar{y}_{i}-\\bar{y}) + (y_{ij}- \\bar{y}_{i} ).$$ It can be shown that the sums of squares follow from this equation if each term is squared and then summed over **all** the values in the sample. Note, the summation is over both $i$ and $j$.\n\n$$\\sum _{i,j}\\left(y_{ij} -\\bar{y}\\right)^{2}   = \\sum _{i,j}\\left(\\bar{y}_{i} -\\bar{y}\\right)^{2}   + \\sum _{i,j}\\left(y_{ij} -\\bar{y}_{i} \\right)^{2}$$\n\nThat is, SS TOTAL = SS FACTOR + SS ERROR.\n\nMean squares (MeanSq or MS) are obtained by dividing the sums of squares (SS) by their respective degrees of freedom (df), as shown below:\n\n| SS     | df  |     MeanSq      |\n|:-------|:----|:---------------:|\n| FACTOR | k-1 | FACTOR SS/(n-1) |\n| ERROR  | n-k |  ERROR SS/(n-k  |\n| TOTAL  | n-1 | TOTAL SS/(n-1)  |\n\nFinally, the $F$ statistic for the FACTOR is obtained by dividing the FACTOR MeanSq by the ERROR MeanSq. The associated degrees of freedom are $k-1$ for the numerator and $n-k$ for the denominator. We display the SS, MS in the form of a table called the one-way ANOVA table.\n\nFor fabric data, the one-way ANOVA `R` output is shown below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.model <- aov(burntime ~ Fabric, data = fabric)\n\nsummary(oneway.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nFabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nIf there is no real difference between the fabrics, we would expect the $F$ Statistic to be about 1. If there is a real difference, we would expect the $F$ statistics to be larger than 1. From tables, the 95th percentile of the $F$ statistic with 3 and 16 degrees of freedom is 3.24. As our value of 13.89 is more than this, the $F$ statistic is significant at the 5% level. This is also confirmed by the $p$-value, which is 0.000. So there is a significant difference between the mean burning times of the fabrics. This obviously doesn't mean that **all** the fabrics differ significantly from each other. It means that there is a significant difference for **at least** one pair of fabrics.\n\nThe individual confidence intervals for the fabric means may be obtained using the pooled error estimate S of 1.7 (the square root of MS Error of 2.891). For example, the confidence interval for the mean burn time of fabric 1 is $mean \\pm t_{error~df, 0.975}\\times S$ or $16.78\\pm 2.12\\times 1.7$ or (15.17, 18.39).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfabric |> \n  group_by(Fabric) |> \n  summarise(\n    mean = mean(burntime),\n    sd = sd(burntime),\n    n = n(),\n    se = sd / sqrt(n),\n    ci = qt(0.975, df = n - 1) * se\n  ) |> \n  ggplot() +\n  aes(x = Fabric, y = mean, group = factor(1)) +\n  geom_line() + \n  geom_point() +\n  geom_errorbar(aes(ymin = mean - ci, \n                    ymax = mean + ci), \n                width = .1)\n```\n\n::: {.cell-output-display}\n![CIs using pooled SD](8-anova_files/figure-pdf/fig-ciplot-1.pdf){#fig-ciplot fig-pos='H'}\n:::\n:::\n\n\n\nThe 95% confidence intervals shown in @fig-ciplot indicate that fabric 1 differs significantly from the other three fabrics. Note that each CI in the above figure has individual Type I error rate, which makes the overall comparison difficult.\n\nFor comparing many treatments, we may perform multiple comparison tests. The following `R` output shows the adjusted confidence intervals for the difference in fabric means.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(oneway.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ Fabric, data = fabric)\n\n$Fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476\n```\n:::\n:::\n\n\n\nThis adjustment in constructing the confidence interval for the difference in the treatment means was proposed by Tukey (known as the Tukey's Honest Significant Difference (HSD)). The idea behind the multiple comparison test proposed Tukey (or by others such as Duncan) is that the error rate is controlled for the family of interval as against individual intervals. We will rely on the software programs to perform such treatment comparisons and obtain plots for multiple comparison; see @fig-hsdplot. Note that we used the `par()` function because the Y-axis labels are bit long.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mar=c(5.1, 9, 4.1, 2.1))\n\nplot(TukeyHSD(oneway.model), las=1 , col=\"brown\")\n```\n\n::: {.cell-output-display}\n![Tukey HSD plot](8-anova_files/figure-pdf/fig-hsdplot-1.pdf){#fig-hsdplot fig-pos='H'}\n:::\n:::\n\n\n\n# Assumptions and Transformations\n\nFor the one way ANOVA, it is assumed that the burn time variances for each treatment are the same. Although we can never be sure whether assumptions are valid, we can look for indications that the assumptions may not be realistic.\n\nThe sample standard deviations of the treatment groups could be plotted against the sample means of the treatment groups. For the fabric example the means and standard deviations of the fabrics are given below.\n\n| summary                  | Fabric 1 | Fabric 2 | Fabric 3 | Fabric 4 |\n|:-------------------------|:---------|:---------|---------:|---------:|\n| Mean $\\bar {y}_i$        | 16.78    | 11.76    |    10.24 |    11.98 |\n| Standard deviation $S_i$ | 1.167    | 2.330    |    1.144 |    1.862 |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfabric |> \n  group_by(Fabric) |> \n  summarise(\n    mean = mean(burntime),\n    sd = sd(burntime)\n  ) |> \n  ggplot() +\n  aes(x = mean, y = sd, group = factor(1)) +\n  geom_point() \n```\n\n::: {.cell-output-display}\n![SD vs mean for four fabrics](8-anova_files/figure-pdf/fig-meansd-1.pdf){#fig-meansd fig-pos='H'}\n:::\n:::\n\n\n\nThe plot of SD vs. mean is shown in @fig-meansd. With only four fabrics in the sample, it is difficult to make any definitive claim. If the assumptions were valid, we would expect the four points to fall approximately along a horizontal band indicating constant standard deviations, and hence variances, regardless of the means of the groups. @fig-meansd suggests that this is the case, so the assumption of equal variances appears to be valid.\n\nA formal test for equal variances may also be performed. In **Bartlett's test**, the null hypothesis of equal variances is tested. This test requires the response variable to be normally distributed where as the alternative test known as the **Levene's test** is applicable for any continuous distribution. The $p$-value for the Bartlett's test for the fabric data is 0.447. This suggests that the null hypothesis of equal variances for the four fabric burn times be accepted.\n\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nbartlett.test(burntime ~ Fabric, data = fabric)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  burntime by Fabric\nBartlett's K-squared = 2.6606, df = 3, p-value = 0.447\n```\n:::\n\n```{.r .cell-code}\ncar::leveneTest(burntime ~ Fabric, data = fabric)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  3  0.1788 0.9092\n      16               \n```\n:::\n:::\n\n\n\nIf the standard deviations seem to be increasing with the means, a transformation such as the square root or logarithm could be tried. The comments of chapter 4 also apply to the one way ANOVA. However, before performing such a data transformation it is important that the data be checked for outliers. Outliers may inflate the standard deviation for one or more treatments. Such outliers should be checked for accuracy. Even when there is no obvious reason for an outlier, it is probably best to delete such a value if it is obvious that this value is causing a major distortion of the data. This will make our data unbalanced, but that does not matter for one way ANOVA.\n\nThe $F$ statistic is based on the null hypothesis that there is no difference (in the population) between treatment means. As we have noted before, a large value of $F$ would indicate that the null hypothesis is not valid and that differences between treatments do exist. However, if the variances for the different treatments vary considerably, then this test is no longer valid.\n\n# Two-Way ANOVA\n\nWe discuss **two-way ANOVA** model using the following example. The strength of concrete ($Y$, measured in MPa) is affected by the type of aggregate (stone, a blocking variable) and the type of cement (the treatment factor) used. An experiment was conducted in order to investigate the effect of aggregate and cement on concrete strength. In all other respects the concrete was the same. That is, same mixing ratios, the same water and the same mixing procedure was used. Ignoring any interaction between aggregate and cement the following model is to be fitted to the data:\n\nfitted = overall effect + aggregate effect + cement effect.\n\nA single testing was done for each cement and each aggregate, producing the balanced set of results shown in @tbl-concretedata.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconcrete <- read_csv(\"http://www.massey.ac.nz/~anhsmith/data/concrete.txt\",\n                     col_types = \"dff\") # double, factor, factor\n```\n:::\n\n::: {#tbl-concretedata .cell tbl-cap='Concrete strength data'}\n::: {.cell-output-display}\n| Strength|Cement |Aggregate |\n|--------:|:------|:---------|\n|        4|1      |1         |\n|        5|2      |1         |\n|        6|3      |1         |\n|        7|4      |1         |\n|       10|1      |2         |\n|        8|2      |2         |\n|        9|3      |2         |\n|       10|4      |2         |\n|       22|1      |3         |\n|       28|2      |3         |\n|       34|3      |3         |\n|       30|4      |3         |\n:::\n:::\n\n\n\nIn @fig-6-8, the computation of row (cement) and column (Aggregate) effects is explained. Clearly the rows correspond to the four cements and the columns correspond to the three aggregates.\n\n\n![Explanation  of row and column factor effects](images/6-8.png){#fig-6-8}\n\n\n\nThe **row effect** is taken as the additional row effect after the overall value 14.42 has been fitted. Similarly the **column effect** is taken as the additional column effect after the overall value has been fitted. Note the following points:\n\na.: Overall value is the overall mean, 14.42.\n\nb.: The row effect for Row 1 is $12-14.42 = -2.42$. In other words, the row effect is the row mean minus the overall mean. The column effects are calculated similarly.\n\nc.: The row effects add to zero, although in practice, as in this example, there is some rounding error. Likewise, the column effects add to zero. If the data were not balanced the effects would not necessarily add to zero.\n\nd.: In each cell, fitted values and residuals could be calculated. For example, for cell (1, 1), we obtain the fitted value as $14.42 + (-2.42) + (-8.92) = 3.08$. Hence,\nresidual = observed-fitted $= 4-3.08 = 0.92$.\n\n**ANOVA** tables are a tidy way of writing down the sums of squares, degrees of freedom and mean squares. We are basically concerned with the variation in the dependent variable about its mean and the amount of this variation explained by the explanatory variables, which in this case are the row and column effects. In the SS column in the ANOVA table we break down the total variation according to its source, namely ROWS, COLUMNS and ERROR.\n\n\n![Formation of ANOVA table](images/6-9.png){#fig-6-9}\n\n\nIn the ANOVA table, (given as @fig-6-9) the number of rows is denoted by $r$ and the number of columns is denoted by $c$. The Error degrees of freedom are obtained by subtracting the ROW and COLUMN degrees of freedom from the total degrees of freedom. The mean squares are obtained by dividing the sums of squares by their respective degrees of freedom. Note that 118.27 (Mean Square corresponding to Total) is the variance of the original observations.\n\nThe `R` output for the two-way ANOVA fit is given below. The entries relating to TOTAL are not displayed in `R`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadditive.model <- aov(Strength ~ Cement + Aggregate, data = concrete)\n\nsummary(additive.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nCement       3   34.9    11.6    1.46    0.317    \nAggregate    2 1218.2   609.1   76.40 5.39e-05 ***\nResiduals    6   47.8     8.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nClearly a large variance (MS) for the rows (`Cement`) would suggest a significant difference between the `Cement`-group means and a large variance (MS) for the columns (`Aggregate`) would suggest a significant difference between the `Aggregate`-group means.\n\nTo test whether a factor has a significant influence, we calculate the $F$ test statistic. As before, this statistic is the ratio of two mean squares. Hence, to test whether the rows have the same means in the population (i.e., there's no \"row effect\"), the $F$ statistic is $$F = \\frac{\\text{Row MS}}{\\text{Error MS}}$$ with the corresponding numerator and denominator degrees of freedom. Likewise, to test whether the columns have the same means in the population (i.e., there's no \"coumn effect\"), the $F$ statistic is $$F = \\frac{\\text{Column MS}}{\\text{Error MS}}$$ with the corresponding numerator and denominator degrees of freedom.\n\nTo test the significance of the `Cement` effect, $$F = \\frac{11.64}{7.97} = 1.5$$ with 3 and 6 d.f. Not significant. To test the significance of the `Aggregate` effect, $$F = \\frac{609.08}{7.97}= 76.4$$ with 2 and 6 d.f. Very significant.\n\nThese results wouldn't be surprising to us had we explored the mean concrete strength for the 3 levels of `Aggregate` and 4 levels of `Cement` as shown in @fig-main2, which are connected by a line, is known as the **main effects** plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- concrete |> \n  group_by(Cement) |> \n  summarise(Mean = mean(Strength)) |> \n  ggplot() +\n  aes(y = Mean, \n      x = Cement) + \n  geom_point() + \n  geom_line(aes(group = 1)) +\n  xlab(\"Cement\") +\n  ggtitle(\"Main effects plot of Cement\") +\n  geom_hline(yintercept = mean(concrete$Strength), alpha = .7) +\n  ylim(5,30)\n\np2 <- concrete |> \n  group_by(Aggregate) |> \n  summarise(Mean = mean(Strength)) |> \n  ggplot() +\n  aes(y = Mean, \n      x = Aggregate) +\n  geom_point() + \n  geom_line(aes(group = 1)) +\n  xlab(\"Aggregate\") +\n  ggtitle(\"Main effects plot of Aggregate\") + \n  geom_hline(yintercept = mean(concrete$Strength), alpha = .7) +\n  ylim(5,30)\n\ngridExtra::grid.arrange(p1, p2, ncol=2)\n```\n\n::: {.cell-output-display}\n![Main effects plots](8-anova_files/figure-pdf/fig-main2-1.pdf){#fig-main2 fig-pos='H'}\n:::\n:::\n\n\n\n# Two Way ANOVA with Interaction\n\n@Brook provided the following data from a milk mixing time experiment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaddle <- tibble(\n  Speed = c(70, 70, 70, 110, 110, 110, 150, 150, 150, \n            70, 70, 70, 110, 110, 110, 150, 150, 150) |> factor(),\n  Diameter = c(90, 120, 150, 90, 120, 150, 90, 120, 150, \n               90, 120, 150, 90, 120, 150, 90, 120, 150) |> factor(),\n  MixTime = c(1490,780,825,575,385,315,195,117,225,\n              1100,620,690,480,385,400,230,190,145)\n)\n\npaddle\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 x 3\n   Speed Diameter MixTime\n   <fct> <fct>      <dbl>\n 1 70    90          1490\n 2 70    120          780\n 3 70    150          825\n 4 110   90           575\n 5 110   120          385\n 6 110   150          315\n 7 150   90           195\n 8 150   120          117\n 9 150   150          225\n10 70    90          1100\n11 70    120          620\n12 70    150          690\n13 110   90           480\n14 110   120          385\n15 110   150          400\n16 150   90           230\n17 150   120          190\n18 150   150          145\n```\n:::\n:::\n\n\n\nWhen whole milk is left standing in a vat, the cream rises to the top, making sampling difficult. A rotating paddle can be used to mix the milk, but too much mixing can damage it. The aim of the experiment was to see how paddle size and speed of rotation affect the optimum mixing time. There were two independent runs at three levels of both speed and paddle size. The data are set out in a two way table in which the rows and columns relate to different speeds and diameters of the paddle. Again, we have balanced data in that the same number of measurements occur in each cell of the table. The experimental data may be treated as separate one way ANOVAs for the Rotational Speed or Paddle Diameter as shown below:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(MixTime ~ Speed, data = paddle) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df  Sum Sq Mean Sq F value   Pr(>F)    \nSpeed        2 1680304  840152   21.72 3.72e-05 ***\nResiduals   15  580184   38679                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\naov(MixTime ~ Diameter, data = paddle) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df  Sum Sq Mean Sq F value Pr(>F)\nDiameter     2  261871  130936   0.983  0.397\nResiduals   15 1998618  133241               \n```\n:::\n:::\n\n\n\nClearly, the rotational speed has more influence than diameter of paddle on optimum mixture time ($p = 0.000$ for Rotational Speed while $p = 0.397$ for Paddle Diameter). Note that the true effect of Paddle Diameter may be masked due to the overestimate of the Error under the (incorrect) one-way ANOVA model. The mean mixing time is obviously higher for a Rotational Speed of 70 rpm, whereas Paddle Diameter appears to have a little effect on the mixing time (see @fig-mainmixtime).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- paddle |> \n  group_by(Speed) |> \n  summarise(mean = mean(MixTime)) |> \n  ggplot() +\n  aes(y = mean, x = Speed) +\n  geom_point() + \n  geom_path(aes(group = 1)) +\n  xlab(\"Speed\") + \n  ggtitle(\"Main effects plot of Speed\") +\n  geom_hline(yintercept = mean(paddle$MixTime), alpha = .7) +\n  ylim(150,1000)\n\np2 <- paddle |> \n  group_by(Diameter) |> \n  summarise(mean = mean(MixTime)) |> \n  ggplot() +\n  aes(y = mean, x = Diameter) +\n  geom_point() + \n  geom_path(aes(group = 1)) +\n  xlab(\"Diameter\") + \n  ggtitle(\"Main effects plot of Diameter\") +\n  geom_hline(yintercept = mean(paddle$MixTime), alpha = .7) +\n  ylim(150,1000)\n\ngridExtra::grid.arrange(p1, p2, ncol=2)\n```\n\n::: {.cell-output-display}\n![Main effects plots](8-anova_files/figure-pdf/fig-mainmixtime-1.pdf){#fig-mainmixtime fig-pos='H'}\n:::\n:::\n\n\n\nThe `visreg` package is useful for automatically plotting linear models, including ANOVA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(visreg)\n\naovs <- aov(MixTime ~ Speed, data = paddle)\naovd <- aov(MixTime ~ Diameter, data = paddle)\n\np1 <- aovs |> \n  visreg(gg=T) +\n  ylim(10,1200)\n\np2 <- aovd |> \n  visreg(gg=T) +\n  ylim(10,1200)\n\ngridExtra::grid.arrange(p1, p2, ncol=2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](8-anova_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWhether the effect of Speed on mixing time is **independent** of the paddle diameter or not cannot be explored using the above plot. We can explore interactions graphically using **interaction plots**, as shown in @fig-intplot1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npmeans <- paddle |> \n  group_by(Speed, Diameter) |> \n  summarise(MixTime = mean(MixTime))\n\np1 <- pmeans |> \n  ggplot(aes(Speed, MixTime)) +\n  geom_line(linewidth = 1, aes(group = Diameter, color = Diameter)) +\n  geom_point(size = 3, aes(color = Diameter))+\n  scale_color_brewer(palette=\"Set1\")\n\np2 <- pmeans |> \n  ggplot(aes(Diameter, MixTime)) +\n  geom_line(linewidth = 1, aes(group = Speed, color = Speed)) +\n  geom_point(size = 3, aes(color = Speed)) +\n  scale_color_brewer(palette=\"Set2\")\n\ngridExtra::grid.arrange(p1, p2, nrow=2)\n```\n\n::: {.cell-output-display}\n![Speed*Diameter interaction plot](8-anova_files/figure-pdf/fig-intplot1-1.pdf){#fig-intplot1 fig-pos='H'}\n:::\n:::\n\n\n\n\n\nIn @fig-intplot1, the mean mixing times have been plotted and connected for all factor level combinations. In the first plot, we've put `Speed` on the x-axis and coloured the points and lines by `Diameter`, and then swapped the roles. The lines in the interaction plots are not parallel. This suggests that the effect of rotational speed is determined, to some extent, by the effect of paddle diameter (and vice versa). In other words, the effect of rotational speed on mixing time is not the same for the various levels of paddle diameter. \n\nThese interaction plots suggest that we should allow for an interaction term in our Two-Way ANOVA model. \n\nA *two-way ANOVA interaction model* is of the form\n\nfit = overall mean + row effect + column effect + (row$\\times$column effect).\n\nThis model has been fitted to mixing times using `R` below. Note the syntax `*` which fits both the interaction and main effect terms.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naovdsx <- aov(MixTime ~ Speed * Diameter, data = paddle)\n\nsummary(aovdsx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Df  Sum Sq Mean Sq F value   Pr(>F)    \nSpeed           2 1680304  840152  67.174 3.89e-06 ***\nDiameter        2  261871  130936  10.469  0.00448 ** \nSpeed:Diameter  4  205749   51437   4.113  0.03635 *  \nResiduals       9  112565   12507                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n\nWith this model object, we can make interaction plots automatically using `visreg` (@fig-intplot1b).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisreg(aovdsx, \"Speed\", by = \"Diameter\", gg=T) + theme_bw()\n```\n\n::: {.cell-output-display}\n![Speed * Diameter interaction plot from visreg](8-anova_files/figure-pdf/fig-intplot1b-1.pdf){#fig-intplot1b fig-pos='H'}\n:::\n:::\n\n\n\n\nNote that the interaction effect is tested by comparing the **interaction** (i.e. Speed$\\times$Diameter) MS against the residual (i.e. Error) MS using an $F$ statistic with 4 and 9 degrees of freedom (i.e. $F$ = 4.11). This $F$-value exceeds the tabulated value 3.63 at the 5% level, so that this interaction effect is significant at the 5% level (also note that the corresponding $p$-value is smaller than 0.05). In other words, there is evidence that the row effects (i.e. *Speed effects*) are NOT constant over columns (i.e. *Diameters*), or the column effects are not constant over rows.\n\nThe degrees of freedom, d.f., for the numerator, 4, is the product of 2 times 2.\n\nd.f.(rows$\\times$columns) = d.f.(rows)$\\times$ d.f.(columns)\n\nThe error degrees of freedom are obtained by subtraction. That is\n\nd.f.(Error) = d.f.(Total)-d.f.(rows)-d.f.(column)-d.f.(rows$\\times$columns) $= (n-1)-(r-1)-(c-1)-(r-1)\\times (c-1)= 17- 2-2-4 = 9.$\n\nIf it could be assumed that the interaction between rows and columns was not large, an **additive (non-interaction)** model could be fitted, namely\n\nobserved = fit + residual = (overall mean + row effect + column effect) + e.\n\nConsider the additive ANOVA model fitted below. Note the syntax `+` which fits only the main effect terms.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naovds <- aov(MixTime ~ Speed + Diameter, data = paddle)\n\nsummary(aovds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df  Sum Sq Mean Sq F value   Pr(>F)    \nSpeed        2 1680304  840152  34.312 6.51e-06 ***\nDiameter     2  261871  130936   5.347   0.0202 *  \nResiduals   13  318313   24486                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nNotice that the degrees of freedom for the error term in the additive model is the sum of the d.f. for error and interaction. Similarly,\n\nSS (Error) of Additive Model = SS (Interaction) + SS (Error) of Interaction Model.\n\nNotice that if there was only **ONE** observation in each cell, it would not be possible to test for the significance of an interaction. This is the case with the two-way ANOVA we considered in the previous section (see concrete strength ANOVA output).\n\nThe mixtime experimental setup is known as a factorial design where the factor levels are crossed to form the treatments. The treatments are usually completely randomised. When factor levels are quantitative, we can also fit a regression model. The set levels of Speed and Diameter are quantitative. So we can regress the mixing time on the speed and diameter levels. The advantage with this approach is that we will be able to fit a *response surface* of mixing time so that we can look for optimum settings on some cost or practical grounds. @fig-rsmplot shows the contour plot of the regression (response) surface of mixing time. This plot suggests that both speed and diameter must be set at higher levels in general to reduce the mixing time. But we can choose practically convenient settings such as higher speed but smaller diameter. The curvature in the contour plot is due to the interaction effect but it tends to be smaller at higher speeds. The text by @box2005statistics contains many examples on using experimental designs for response surface optimisation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisreg2d(aovdsx, \"Speed\", \"Diameter\")\n```\n\n::: {.cell-output-display}\n![2d contour visreg plot of mixing time](8-anova_files/figure-pdf/fig-rsmplot-1.pdf){#fig-rsmplot fig-pos='H'}\n:::\n:::\n\n\n\n\n# Indicator Variables\n\nIndicator variable (also known as a dummy or binary variable) is one whose possible values are either 1 or zero. A value of 1 means a particular attribute or characteristic is present and a value of zero means the absence. For example, we may define indicator variables for males and females. Consider- Indicator variable for the male category $$\\begin{array}{cccc}\nI_{\\text {male}} & = & 1 & \\text{for males}\\\\\n & & 0& \\text{for females}\n\\end{array}$$\n\nIndicator variable for the female category $$\\begin{array}{cccc}\nI_{\\text{female}} & = & 1 & \\text{for females}\\\\\n & & 0& \\text{for males}\n\\end{array}$$\n\nIn order identify the gender category, we either need either $I_{male}$ or $I_{female}$ and not both. This is because for the two gender categories, we have only a single degree of freedom. Using indicator variables, we will be able to include categorical predictors in a regression model. It is also possible to approach the ANOVA test for equality of means through linear regression.\n\nConsider the fabric burn time data discussed earlier. For the four fabric types, we define the following four indicator variables,\n\n$$\\begin{array}{cccc}\nI_1 & = & 1 & \\text{for Fabric 1}\\\\\n & & 0& \\text{for Fabric 2, 3 and 4}\n\\end{array}$$ $$\\begin{array}{cccc}\nI_2 & = & 1 & \\text{for Fabric 2}\\\\\n & & 0& \\text{for Fabric 1, 3 and 4}\n\\end{array}$$ $$\\begin{array}{cccc}\nI_3 & = & 1 & \\text{for Fabric 3}\\\\\n & & 0& \\text{for Fabric 1, 2 and 4}\n\\end{array}$$ $$\\begin{array}{cccc}\nI_4 & = & 1 & \\text{for Fabric 4}\\\\\n & & 0& \\text{for Fabric 1, 2 and 3}\n\\end{array}$$\n\nNote that we need only any three of the above indicator variables to identify the fabric category. For example, with $I_1$, $I_2$, and $I_3$ variables, fabric 4 is identified when $I_1 = I_2 = I_3 = 0$. See @tbl-invartab.\n\nLet us regress the burn times on the indicator variables $I_1$, $I_2$ and $I_3$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfabric <- fabric |> \n  select(burntime, Fabric) |> \n  mutate(I1 = as.numeric(Fabric==\"Fabric 1\"),\n         I2 = as.numeric(Fabric==\"Fabric 2\"),\n         I3 = as.numeric(Fabric==\"Fabric 3\"),\n         I4 = as.numeric(Fabric==\"Fabric 4\")\n  )\n\nfabric\n```\n:::\n\n::: {#tbl-invartab .cell tbl-cap='Indicators of Fabrics'}\n::: {.cell-output-display}\n| burntime|Fabric   | I1| I2| I3| I4|\n|--------:|:--------|--:|--:|--:|--:|\n|     17.8|Fabric 1 |  1|  0|  0|  0|\n|     16.2|Fabric 1 |  1|  0|  0|  0|\n|     17.5|Fabric 1 |  1|  0|  0|  0|\n|     17.4|Fabric 1 |  1|  0|  0|  0|\n|     15.0|Fabric 1 |  1|  0|  0|  0|\n|     11.2|Fabric 2 |  0|  1|  0|  0|\n|     11.4|Fabric 2 |  0|  1|  0|  0|\n|     15.8|Fabric 2 |  0|  1|  0|  0|\n|     10.0|Fabric 2 |  0|  1|  0|  0|\n|     10.4|Fabric 2 |  0|  1|  0|  0|\n|     11.8|Fabric 3 |  0|  0|  1|  0|\n|     11.0|Fabric 3 |  0|  0|  1|  0|\n|     10.0|Fabric 3 |  0|  0|  1|  0|\n|      9.2|Fabric 3 |  0|  0|  1|  0|\n|      9.2|Fabric 3 |  0|  0|  1|  0|\n|     14.9|Fabric 4 |  0|  0|  0|  1|\n|     10.8|Fabric 4 |  0|  0|  0|  1|\n|     12.8|Fabric 4 |  0|  0|  0|  1|\n|     10.7|Fabric 4 |  0|  0|  0|  1|\n|     10.7|Fabric 4 |  0|  0|  0|  1|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmdl <- lm(burntime ~ I1 + I2 + I3, data = fabric)\n\nsummary(mdl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = burntime ~ I1 + I2 + I3, data = fabric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.780 -1.205 -0.460  0.775  4.040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.9800     0.7604  15.754 3.65e-11 ***\nI1            4.8000     1.0754   4.463 0.000392 ***\nI2           -0.2200     1.0754  -0.205 0.840485    \nI3           -1.7400     1.0754  -1.618 0.125206    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.7 on 16 degrees of freedom\nMultiple R-squared:  0.7226,\tAdjusted R-squared:  0.6706 \nF-statistic: 13.89 on 3 and 16 DF,  p-value: 0.0001016\n```\n:::\n:::\n\n\n\nThe following points are to be noted on the above regression model:\n\n1.  The $y$-intercept 11.98 is the mean burn time of the fourth fabric (i.e. the mean response for the omitted predictor category).\n\n2.  The mean burn time of fabrics 1, 2 and 3 can be found from the fitted model. For fabric 1, $I_1$ =1, $I_2$=0, and $I_3$ =0. Substituting these values in the fitted model, we get $11.98 + 4.80\\times 1 - 0.22\\times 0 - 1.74\\times 0 = 12+4.8 = 16.78$, which is mean burn time of fabric 1. For fabric 2, the mean burn time is $11.98 + 4.80\\times 0 - 0.22\\times 1 - 1.74\\times 0 = 11.76$. For fabric 3, the mean burn time is $11.98 + 4.80\\times 0 - 0.22\\times 0 - 1.74\\times 1 = 10.24$. In other words, the fitted coefficient of an indicator variable is the difference between the response means of the category indicated by the variable and the 'base' category (indicated by the omitted indicator variable).\n\n3.  The significance of the slope coefficients implies a significant difference in means. For example, the coefficient of the indicator variable $I_1$ is highly significant. This means that the mean burn time of fabric 1 is significantly different from the mean burn time of fabric 4. Note that this is not the case when fabrics 2 and 3 are compared with fabric 4.\n\n4.  In the above output, a multiple comparison of means is made keeping fabric 4 as the base. A comparison of the mean burn time of fabrics 2 & 3 is not possible with the above regression. We need to omit either $I_2$ or $I_3$ and fit a regression of burn time on the predictors $I_1$, $I_3$, and $I_4$ or $I_1$, $I_2$, and $I_4$ for such a comparison.\n\nThe ANOVA of the regression model is the same as the one-way ANOVA fit (compare F, mean squares etc.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mdl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: burntime\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nI1         1 111.521 111.521 38.5718 1.248e-05 ***\nI2         1   1.408   1.408  0.4871    0.4952    \nI3         1   7.569   7.569  2.6179    0.1252    \nResiduals 16  46.260   2.891                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n## Another example\n\nConsider the pine tree data discussed earlier. Let us model the Top circumference (numerical response) using Area variable (which is categorical). A plot of the raw data and mean circumference along with the associated 95% confidence intervals are shown in @fig-errplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/pinetree.RData\",\n  destfile = \"pinetree.RData\")\n\nload(\"pinetree.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npinetree |> \n  ggplot() +\n  aes(x = Area, y = Top, color = Area) + \n  geom_jitter(width = 0.15, height = 0, alpha = .6) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\", \n               size = 3, \n               position = position_nudge(x = 0.3)\n               ) + \n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"errorbar\", \n               size = 0.75, width = 0.075, \n               position = position_nudge(x = 0.3)\n               ) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![Interval Plot](8-anova_files/figure-pdf/fig-errplot-1.pdf){#fig-errplot fig-pos='H'}\n:::\n:::\n\n\n\nFor the three different areas of the forest, we define the following three indicator variables,\n\n$$\\begin{array}{cccc}\nI_1 & = & 1 & \\text{for Area 1}\\\\\n & & 0& \\text{for Areas 2, and 3}\n\\end{array}$$ $$\\begin{array}{cccc}\nI_2 & = & 1 & \\text{for Area 2}\\\\\n & & 0& \\text{for Areas 1, and 3}\n\\end{array}$$ $$\\begin{array}{cccc}\nI_3 & = & 1 & \\text{for Area 3}\\\\\n & & 0& \\text{for Areas 1, and 2}\n\\end{array}$$\n\nNote that we need only any three of the above indicator variables to identify the Area category. For example, with $I_1$ and $I_2$ variables, Area 3 is identified when $I_1 = I_2 = 0$.\n\nLet us regress the pine tree Top circumference on the indicator variables $I_2$ and $I_3$ using the following `R` codes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npinetree1 <- pinetree |> \n  select(Area, Top) |> \n  mutate(I1 = as.numeric(Area == \"1\"),\n         I2 = as.numeric(Area == \"2\"), \n         I3 = as.numeric(Area == \"3\") )\n\nmdl <- lm(Top ~ I2 + I3, data = pinetree1)\n\nlibrary(broom)\n\ntidy(mdl)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npinetree1 <- pinetree |> \n  select(Top, Area) |> \n  mutate(I1 = as.numeric(Area == \"1\"),\n         I2 = as.numeric(Area == \"2\"), \n         I3 = as.numeric(Area == \"3\") )\n\nmdl <- lm(Top ~ I2 + I3, data = pinetree1)\n\ntidy(mdl)\n```\n:::\n\n::: {#tbl-indicat .cell tbl-cap='Regression of Top Circumference on Area Indicator Variables'}\n::: {.cell-output-display}\n|term        | estimate| std.error| statistic|   p.value|\n|:-----------|--------:|---------:|---------:|---------:|\n|(Intercept) |   20.025|   1.11375| 17.979805| 0.0000000|\n|I2          |   -1.955|   1.57508| -1.241207| 0.2196132|\n|I3          |   -5.925|   1.57508| -3.761714| 0.0004002|\n:::\n:::\n\n\n\nThe following points are to be noted on the regression model shown in @tbl-indicat:\n\n1.  The $y$-intercept 20.025 is the mean Top circumference for Area 1 (i.e. the mean response for the omitted predictor category).\n\n2.  The mean Top circumference 2 and 3 can be found from the fitted model. For Area 2, $I_1$ =0, $I_2$=1, and $I_3$ =0. Substituting these values in the fitted model, we get $20.025 -1.955 \\times 1 -5.925 \\times 0 = 20.025-1.955 = 18.07$, which is mean Top circumference for Area 1. For Area 3, the mean Top circumference for Area 3 is $20.025 -1.955 \\times 0 -5.925 \\times 1 = 20.025= 14.1$. In other words, the fitted coefficient of an indicator variable is the difference between the response means of the category indicated by the variable and the 'base' category (indicated by the omitted indicator variable).\n\n3.  The significance of the slope coefficients implies a significant difference in means. For example, the coefficient of the indicator variable $I_2$ is highly significant. This means that the mean Top circumference for Area 2 is significantly different from the mean Top circumference for Area 1. The negative sign of the coefficient means that Area 2 mean is significantly lower when compared Area 1 mean. The same is true when Area 3 is compared with Area 1.\n\n4.  In the above output, a multiple comparison of means is made keeping Area 1 as the base. A comparison of the mean Top circumference for Areas 2 & 3 is not possible with the above regression. We need to omit either $I_2$ or $I_3$ and fit a regression for such a comparison.\n\nIn practice, you will NOT be creating any indicator variables to perform the analysis and `R` does it for us when we use the `lm()` function but we have to make sure that categorical factors are stored as factors, not numerical variables. \n\nThe ANOVA of the regression model shown in @tbl-indanova has a highly significant F value which means that at least one Area is different in terms of mean Top circumference.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Top ~ Area, data=pinetree) |> anova()\n```\n:::\n\n::: {#tbl-indanova .cell tbl-cap='Regression ANOVA'}\n::: {.cell-output-display}\n|          | Df|    Sum Sq|   Mean Sq|  F value|    Pr(>F)|\n|:---------|--:|---------:|---------:|--------:|---------:|\n|Area      |  2|  364.5903| 182.29517| 7.348015| 0.0014482|\n|Residuals | 57| 1414.0995|  24.80876|       NA|        NA|\n:::\n:::\n\n\n\n# Analysis of Covariance (ANCOVA)\n\nIndicator variables are particular useful when both a dependent variable and an independent variable are measured. One would like to use regression to show the relationship between the dependent or response variable and the explanatory variable (covariate) separately for each factor level. However, both the slope and the intercept may differ for the various factor levels. What sort of model does one fit to such data? Consider the Restaurant Sales Data presented in the first four columns of @tbl-Restaurant. This dataset is from @Neter.\n\nA study for a chain of fast-food restaurants examined the relationship between restaurant sales ($Y$, in thousands of dollars) and the number of households in the restaurant's trading area ($X$, in thousands) and location of restaurant (Highway, shopping Mall, Street).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrestaurant <- \n  read_table(\"http://www.massey.ac.nz/~anhsmith/data/restaurant.txt\",\n             col_types = \"fddf\") |> \n  mutate(Restaurant = factor(Restaurant),\n         I1 = as.numeric(Location==\"Mall\"),\n         I2 = as.numeric(Location==\"Street\"),\n         I3 = as.numeric(Location==\"Highway\"),\n         I1.X = I1 * Households,\n         I2.X = I2 * Households)\n\nrestaurant\n```\n:::\n\n::: {#tbl-Restaurant .cell tbl-cap='Restaurant Sales Data'}\n::: {.cell-output-display}\n|Restaurant |  Sales| Households|Location | I1| I2| I3| I1.X| I2.X|\n|:----------|------:|----------:|:--------|--:|--:|--:|----:|----:|\n|1          | 135.27|        155|Highway  |  0|  0|  1|    0|    0|\n|2          |  72.74|         93|Highway  |  0|  0|  1|    0|    0|\n|3          | 114.95|        128|Highway  |  0|  0|  1|    0|    0|\n|4          | 102.93|        114|Highway  |  0|  0|  1|    0|    0|\n|5          | 131.77|        158|Highway  |  0|  0|  1|    0|    0|\n|6          | 160.91|        183|Highway  |  0|  0|  1|    0|    0|\n|7          | 179.86|        178|Mall     |  1|  0|  0|  178|    0|\n|8          | 220.14|        215|Mall     |  1|  0|  0|  215|    0|\n|9          | 179.64|        172|Mall     |  1|  0|  0|  172|    0|\n|10         | 185.92|        197|Mall     |  1|  0|  0|  197|    0|\n|11         | 207.82|        207|Mall     |  1|  0|  0|  207|    0|\n|12         | 113.51|         95|Mall     |  1|  0|  0|   95|    0|\n|13         | 203.98|        224|Street   |  0|  1|  0|    0|  224|\n|14         | 174.48|        199|Street   |  0|  1|  0|    0|  199|\n|15         | 220.43|        240|Street   |  0|  1|  0|    0|  240|\n|16         |  93.19|        100|Street   |  0|  1|  0|    0|  100|\n:::\n:::\n\n\n\n@tbl-Restaurant also shows the indicator variables for the Location factor. Just two indicator variables (say $I_1$ and $I_2$) are needed in order to define the three different locations uniquely. In order to permit a separate line for each location we need a model of the following form\n\n$$Y = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)X$$\n\nThis model provides a separate model for each location as well as allows for the interaction between the location of the restaurant and the number of households through the slope coefficients.\n\nFor Highway locations, $I_1$ and $I_2$ are both equal to zero. Hence the model simplifies to $Y = \\beta_0 + (\\beta_3)X$. For Mall locations, $I_1$ is equal to one and $I_2$ is equal to zero. So the model becomes $Y = (\\beta_0 + \\beta_1 ) + (\\beta_3 + \\beta_4)X$. For Street locations, $I_1$ is equal to zero and $I_2$ is equal to one. So the model becomes $Y = (\\beta_0 + \\beta_2 ) + (\\beta_3 + \\beta_5)X$.\n\nThe newly created product variables $I_1.X=I_1\\times X$ and $I_2.X=I_2\\times X$ shown in @tbl-Restaurant allow us to fit separate slopes for the Locations. `R` does not require creation of data for the interaction terms $I_1$$\\times X$ and $I_2$$\\times X$ and it does internally. The regression of Sales $(Y)$ on Households $(X)$, $I_1$, $I_2$, $I_1\\times X$ and $I_2\\times X$ is shown in @tbl-anovatab.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nancova.model <- lm(Sales ~ Households + I1 + I2 + I1.X + I2.X,\n                   data=restaurant)\n\ntidy(ancova.model)\n```\n:::\n\n::: {#tbl-anovatab .cell tbl-cap='Analysis of Covariance (ANCOVA)'}\n::: {.cell-output-display}\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -6.203|    11.818|    -0.525|   0.611|\n|Households  |    0.909|     0.083|    10.906|   0.000|\n|I1          |   39.223|    16.451|     2.384|   0.038|\n|I2          |    8.036|    16.273|     0.494|   0.632|\n|I1.X        |   -0.074|     0.104|    -0.710|   0.494|\n|I2.X        |   -0.012|     0.101|    -0.120|   0.907|\n:::\n:::\n\n\n\nThis `R` output suggests that the coefficients for $I_2$, $I_1\\times X$ and $I_2\\times X$ are not significantly different from zero. This suggests that a simpler model can be fitted to these data omitting the category indicated by $I_2$. The significance of the $I_1$ coefficient means that the $y$-intercept of the model for Mall location is different. The significance of the coefficients of the product variables will imply significant differences in slopes for the locations but this is not the case here. In general, a stepwise regression is needed to obtain a simpler model having the best subset of predictors (rather than judging from the multiple regression model given here).\n\nThe final or simpler model suggested after step-wise analysis (results not shown) is:\n\n$Y = -3.298 + 0.906 X + 23.8\\times I_1$.\n\nBecause $I_2$ does not appear in this equation, it means that the same regression line can be used to relate sales to number of households for the Highway and Street Locations. The fact that $I_1$ is included in the final model but $I_1$$\\times X$ is excluded means that the line for Malls has a different intercept but the same slope as the Highway/Street Line.\n\n$I_1$=0 for both Highway and Street locations so the fitted values for Highway/Street locations is,\n\n$Y = -3.298 + 0.906 X$.\n\n$I_1$=1 for Mall locations so the fitted model for Mall locations is,\n\n$Y = (-3.298 + 23.81) + (0.906-0.074)X = 20.512 + 0.832X$.\n\nThese two lines are plotted in @fig-ancovaplot. The top line is (obviously) the fitted model for Mall locations and the bottom one for Highway and Street locations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrestaurant |> \n  mutate(`Mall location` = Location == \"Mall\") |> \n  ggplot() +\n  aes(y = Sales, x = Households, linetype = `Mall location`) +\n  geom_point(aes(shape = Location)) + \n  geom_smooth(method = lm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![Plot of Fitted Lines](8-anova_files/figure-pdf/fig-ancovaplot-1.pdf){#fig-ancovaplot fig-pos='H'}\n:::\n:::\n\n\n\nIn practice, you will NOT be creating any indicator variables to perform the analysis of covariance and `R` does it for us when we use the `lm()` with the syntax `*` which stands for interaction. Try-\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmdl <- lm(Top ~ First * Area, data = pinetree)\n\ntidy(mdl) # also summary(mdl)  or anova(mdl)\n```\n:::\n\n\n\n\n\n# Summary\n\n\n\nVarious ANOVA models have been considered in this chapter. The goodness of fit is indicated by the $R$-Squared value or $F$ statistic. Most of the ANOVA examples given in this chapter are based on experimental data but ANOVA models can also be fitted to survey data, provided that the observations represent a random sample. When the observations are time series or other data forms chosen in a non-random fashion, the assumptions of these models may not be satisfied. In particular it is unlikely that the residuals will be independent, following no clear pattern. It is important that such assumptions be checked; otherwise the results of these analyses may be misleading. It is also important that the residual variances are reasonably similar for all treatment levels. If this is not the case, some transformation of the response variable may be required in order to ensure that the ANOVA $F$ tests are valid.\n\nIndicator variables can be used to perform ANOVA using regression. However, it is more common to use indicator variables when one wants to use both a categorical factor and a continuous measurement variable $X$ to model the $Y$ responses. Such an analysis is called an ANCOVA and the model fitted using regression may consist of a separate model for each treatment.\n\n",
    "supporting": [
      "8-anova_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\r\n\\usepackage{longtable}\r\n\\usepackage{array}\r\n\\usepackage{multirow}\r\n\\usepackage{wrapfig}\r\n\\usepackage{float}\r\n\\usepackage{colortbl}\r\n\\usepackage{pdflscape}\r\n\\usepackage{tabu}\r\n\\usepackage{threeparttable}\r\n\\usepackage{threeparttablex}\r\n\\usepackage[normalem]{ulem}\r\n\\usepackage{makecell}\r\n\\usepackage{xcolor}\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}