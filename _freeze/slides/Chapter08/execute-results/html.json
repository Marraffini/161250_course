{
  "hash": "e4d8f27d1dbf4058f92a6cf03a00cc8a",
  "result": {
    "markdown": "---\ntitle: \"Chapter 8:<br>Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)\"\nimage: img/ano.png\nformat: \n  revealjs:\n    width: 1050\n    height:\t700\n    scrollable: true\n    transition: fade\n    theme: [default, myquarto.scss]\n    slide-number: c/t  \n    logo: img/L_Color.png\n    footer: \"[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)\"\n    styles:\n      - revealjs.dark:\n        background-color: #222\n        color: #fff\n    \nexecute:\n  echo: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n:::\n\n\n## Learning Objectives\n\n-   Define and understand an ANOVA\n-   Use a one-way ANOVA on example data\n-   Understand and calculate an ANOVA table\n-   Describe Tukey's post hoc tests and assumptions of ANOVAs\n-   Understand the difference between one-way and two-way ANOVAs\n\n## ANOVA\n\n-   Analysis of variance, or ANOVA, is an approach to comparing data\n    with multiple means across different groups, and allows us to see\n    patterns and trends within complex and varied data.\n\n-   Used for categorical or grouped data.\n\n-   Often data from experiments (treatments make good factors).\n\n-   EDA bar, points, or box plots are options to show differences\n    between groups.\n\n## One-way (single factor) ANOVA model\n\n-   fabric burn-time data\n\n| fabric 1 | fabric 2 | fabric 3 | fabric 4 |\n|----------|----------|----------|----------|\n| 17.8     | 11.2     | 11.8     | 14.9     |\n| 16.2     | 11.4     | 11       | 10.8     |\n| 17.5     | 15.8     | 10       | 12.8     |\n| 17.4     | 10       | 9.2      | 10.7     |\n| 15       | 10.4     | 9.2      | 10.7     |\n\n-   Can we regard the mean burn times of the four fabrics as equal?\\\n    -   fabric 1 seems to take longer time to burn\n\n- Start with hypotheses and EDA to visualize groups\n\n## One-way ANOVA Hypotheses \n\n-   $H_0$: The mean burn times are equal for the four fabrics\n-   $H_a$: The mean burn time of at least one fabric is different.\n\n## One-way ANOVA \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfabric=read.table(\"../data/fabric.txt\", header=TRUE, sep=\"\\t\")\nfabric |>\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary(fun.data = \"mean_cl_boot\", colour = \"blue\", linewidth = 2, size = 3)\n```\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n\n```{.r .cell-code}\nfabric |>\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary_bin(fun = \"mean\", geom = \"bar\", orientation = 'x')\n```\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-1-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n## One-way (single factor) ANOVA model\n\n![](img/oneway.png)\n\n## ANOVA table\n\n> observation = mean + effect + error\n\n> SS Total = SS Factor + SS Error\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(aov(burntime ~ fabric, data = fabric))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n-   fabric effect on burntime is highly significant.\n    -   In other words, the null hypothesis of equal mean burntime is\n        rejected.\n        -   Or alternatively the mean burntime is different for **at\n            least** one fabric\n\n## ANOVA table\n\nHow are these values calculated?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n| SS     | df  |     MeanSq      |\n|:-------|:----|:---------------:|\n| FACTOR | k-1 | FACTOR SS/(k-1) |\n| ERROR  | n-k | ERROR SS/(n-k)  |\n| TOTAL  | n-1 | TOTAL SS/(n-1)  |\n\n## ANOVA table\n\nCalculating F values of a factor:\n\n$F = \\frac{MS_{Factor}}{MS_{Error}}$\n\n| SS     | df  |     MeanSq      |\n|:-------|:----|:---------------:|\n| FACTOR | k-1 | FACTOR SS/(k-1) |\n| ERROR  | n-k | ERROR SS/(n-k)  |\n| TOTAL  | n-1 | TOTAL SS/(n-1)  |\n\n## ANOVA table practice\n\n| SS     | df  | MeanSq          | F- value | P-value |\n|:-------|:----|:----------------|:---------|:-------:|\n| FACTOR | k-1 | FACTOR SS/(k-1) | MSF/MSE  |         |\n| ERROR  | n-k | ERROR SS/(n-k)  |          |         |\n| TOTAL  | n-1 | TOTAL SS/(n-1)  |          |         |\n\n|           | SS    | df  | MeanSq | F- value | P-value |\n|:----------|:------|:----|:-------|:---------|:-------:|\n| fabric    | 120.5 |     |        |          |         |\n| Residuals | 46.3  |     |        |          |         |\n|     TOTAL |       |     |        |          |         |\n\n## Reminder on P values\nIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. \n\nA very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis.\n\nWe calculate them based on our theoretical sampling distributions (normal, $t$, $F$, $\\chi^2$)\n\n## Reminder of Sampling Distributions\n\nA sampling distribution is a probabilistic model of sampling variation–it describes the behaviour of some sample statistic\n\nFor a normal population, when the population parameters \n and are known, we can easily derive the sampling distributions of the sample mean or sample variance.\n\nWhen the population parameters are unknown, we have to estimate them from data.\n\n## Reminder of Sampling Distributions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Graphical comparison of means\n\n-   The graph below shows individual 95% confidence intervals for the\n    fabric means\n   \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## One-way ANOVA model Assumptions\n\n-   Residuals are randomly and normally distributed\n-   Residuals must be independent of means.\n\n    -   If SD increases with mean, try square root or logarithmic\n        transformation.\n-   The ANOVA model assumes equal SD for the treatments.\n-   If experimental errors are more in some subgroups, divide the\n    problem into separate ones.\n-   Positive correlation among residuals leads to under estimation of\n    error variance; negative correlation leads to overestimation.\\\n\n*These assumptions are harder to validate to small experimental design\ndata*\n\n## Visualize assumptions\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(aov(burntime ~ fabric, data = fabric))\n```\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-7-2.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-7-3.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-7-4.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Visualize assumptions\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![SD vs mean for four fabrics](Chapter08_files/figure-revealjs/fig-meansd-1.png){#fig-meansd fig-align='center' width=960}\n:::\n:::\n\n-   Residuals must be independent of means.\n\n    -   If SD increases with mean, try square root or logarithmic\n        transformation.\n        \nWith only four fabrics in the sample, it is difficult to make any definitive claim. \n\nIf the assumptions were valid, we would expect the four points to fall approximately along a horizontal band indicating constant standard deviations, and hence variances, regardless of the means of the groups. \n\nThis figure suggests that this is the case, so the assumption of equal variances appears to be valid.\n\n## Equal variance\n\n**Bartlett's test**:\n-   null hypothesis: equal variances\n-   but it has an assumption of its own (response variable must be normally distributed)\n\n**Levene's test** \n-   null hypothesis: equal variances\n-   is applicable for any continuous distribution\n\n\n::: {.cell layout-align=\"center\" warnings='false'}\n\n```{.r .cell-code}\nbartlett.test(burntime ~ fabric, data = fabric)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  burntime by fabric\nBartlett's K-squared = 2.6606, df = 3, p-value = 0.447\n```\n:::\n\n```{.r .cell-code}\ncar::leveneTest(burntime ~ fabric, data = fabric)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  3  0.1788 0.9092\n      16               \n```\n:::\n:::\n\n\n## What if the equal variance assumption is violated?\n\nANOVA’s are considered to be fairly robust against violations of the equal variances assumption as long as each group has the same sample size.\n\nIf this assumption is violated, the most common way to deal with it is to transform the response variable using one of the three transformations:\n\n1. Log Transformation: Transform the response variable from y to log(y).\n\n2. Square Root Transformation: Transform the response variable from y to √y.\n\n3. Cube Root Transformation: Transform the response variable from y to y1/3.\n\nBy performing these transformations, the problem of heteroscedasticity typically goes away.\n\n\n## Normality\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naov_fabric <- aov(burntime ~ fabric, data = fabric)\nshapiro.test(aov_fabric$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  aov_fabric$residuals\nW = 0.88926, p-value = 0.02606\n```\n:::\n:::\n\n\nANOVAs are robust to mild issues of non-normality \n\nBut if have issues with normality and unequal variance try transformations\n\n## When transformations do not help:\n\n- Weighted least squares regression: This type of regression assigns a weight to each data point based on the variance of its fitted value.\n\n    - Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.\n    \n- Non-parametric test: \n\n    -  Kruskal-Wallis Test is the non-parametric version of a one-way ANOVA\n\n## Tukey HSD\n\n-   Tukey HSD (Honest Significant Differences) plot allows pairwise\n    comparison of treatment means.\n-   Which fabric types have different burn times? (remember the\n    alternative hypothesis of a one-way ANOVA is *at least* one of the\n    means are different)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476\n```\n:::\n:::\n\n## Tukey HSD (Interval Plot)\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Two way (two factor) ANOVA\n\n-   Two factors are present.\n\n-   A two-way ANOVA is used to estimate how the mean of a continuous\n    variable changes according to the levels of two categorical\n    variables.\n\n-   Use a two-way ANOVA when you want to know how two independent\n    variables, in combination, affect a dependent variable.\n\n-   Very similar to multiple regression but with categorical variables.\n\n## Two way (two factor) ANOVA example\n\n-   Example: We will use the built in data ToothGrowth. It contains data from a study evaluating the effect of vitamin C on tooth growth in Guinea pigs. The experiment has been performed on 60 pigs, where each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, (orange juice or ascorbic acid (a form of vitamin C and coded as VC). Tooth length was measured and a sample of the data is shown below. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n```\n:::\n:::\n\n\n## Run a Two-way ANOVA\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(aov(len~dose+supp, data=ToothGrowth))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ndose         2 2426.4  1213.2   82.81  < 2e-16 ***\nsupp         1  205.3   205.3   14.02 0.000429 ***\nResiduals   56  820.4    14.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nA two-way ANOVA tests two null hypotheses at the same time:\n\n-   All group means are equal at each level of the first variable\n-   All group means are equal at each level of the second variable\n\n\n\n## Two-way model fit\n\n> Model:\\\n> Observation = Overall Mean + Factor 1 Effect + Factor 2 Effect + Error\n\n-   $H_0$: factor 1 means are equal; factor 2 means are equal\n\n\\scriptsize\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ndose         2 2426.4  1213.2   82.81  < 2e-16 ***\nsupp         1  205.3   205.3   14.02 0.000429 ***\nResiduals   56  820.4    14.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nSupplement and dose effects are significant at 5% level.\n\n## Main effect plots\n\n-   Simply plot of response means for factor levels\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Tukey HSD\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nteeth <- aov(len~supp+dose, data=ToothGrowth)\nTukeyHSD(teeth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ supp + dose, data = ToothGrowth)\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.679762 -1.720238 0.0004293\n\n$dose\n          diff       lwr       upr p adj\nD1-D0.5  9.130  6.215909 12.044091 0e+00\nD2-D0.5 15.495 12.580909 18.409091 0e+00\nD2-D1    6.365  3.450909  9.279091 7e-06\n```\n:::\n:::\n\n\n## Summary so far\n\n-   ANOVA: 1 and 2 factor\n-   ANOVA table\n-   Assumptions\n-   Tukey's HSD\n\n## Learning Objectives: Part 2\n\n-   Understand the use of interactions in linear models\n-   Use a two-way ANOVA with interactions on example data\n-   Understand and calculate an ANOVA table when interactions are present\n-   Describe Tukey's post hoc tests and assumptions of ANOVAs with\n    interactions\n-   Understand the difference and similarities between this chapter and\n    previous linear models covered \n-   Review non-parametric options\n\n## Interaction effect\n\n-   Whether Factor A effects are constant over Factor B  effects or Factor B effects are constant over Factor A effects?\n    -   If the answer is no, then there is an interaction between A & B.\n-   Example:\n    -   Temperature and pressure are factors affecting the yield in\n        chemical experiments.\n    -   They do `interact` in a mechanistic sense.\n\n`Interaction` may or may not have physical meaning.\n\n## Two-way ANOVA and Interaction effects\n\nA two-way ANOVA with interaction tests three null hypotheses at the same time:\n\n-   All group means are equal at each level of the first variable\n-   All group means are equal at each level of the second variable\n-   There is no interaction effect between the two variables\n\n## Interaction Plots\n\n-   In the absence of interaction, the plotted means of factor will be roughly parallel\n    -   see Plot 1. A & B do not interact.\n-   If the the plotted means of factor crossings are far from parallel,\n    then there is interaction\n    -   Plot 2 shows extreme (antagonistic) interaction between A & B.\n\n![](img/interaction.png)\n\n## Interaction Plot for Zooplankton data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\np1= ggplot(data = ToothGrowth, aes(x = supp, y = len, group=dose, colour=dose)) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(ToothGrowth$len), slope=0)+ \n  theme_bw()+ggtitle(\"Dose*Supplement Interaction effect\")\n\np2= ggplot(data = ToothGrowth, aes(x = dose, y = len, group=supp, colour=supp)) +stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(ToothGrowth$len), slope=0)+ \n  theme_bw()+ggtitle(\"Dose*Supplement Interaction effect\")\nlibrary(patchwork)\np1+p2\n```\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n-   Interaction effect may be present\n\n## Two-way model fit\n\n> observation = mean + Factor A + Factor B + interaction effect + error\n\n-   The above model is known as `multiplicative` model\n\n-   If interaction effect is ignored, we deal with an `additive` model\n\n\n## Two-way model fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nToothGrowth |> \n  aov(formula = len ~ supp * dose) |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nsupp         1  205.4   205.4  15.572 0.000231 ***\ndose         2 2426.4  1213.2  92.000  < 2e-16 ***\nsupp:dose    2  108.3    54.2   4.107 0.021860 *  \nResiduals   54  712.1    13.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Residual diagnostics\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Interpreting interactions\n\n-   If an interaction term is significant, the effect of Factor A **depends** on Factor B (and vice versa)\n\n-   Use interaction plots to visualize\n\n-   Posthoc tests to examine different level combinations\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nTukeyHSD(modl)$`supp:dose`\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 diff        lwr        upr        p adj\nVC:D0.5-OJ:D0.5 -5.25 -10.048124 -0.4518762 2.425209e-02\nOJ:D1-OJ:D0.5    9.47   4.671876 14.2681238 4.612304e-06\nVC:D1-OJ:D0.5    3.54  -1.258124  8.3381238 2.640208e-01\nOJ:D2-OJ:D0.5   12.83   8.031876 17.6281238 2.125153e-09\nVC:D2-OJ:D0.5   12.91   8.111876 17.7081238 1.769939e-09\nOJ:D1-VC:D0.5   14.72   9.921876 19.5181238 2.985967e-11\nVC:D1-VC:D0.5    8.79   3.991876 13.5881238 2.100948e-05\nOJ:D2-VC:D0.5   18.08  13.281876 22.8781238 4.855005e-13\nVC:D2-VC:D0.5   18.16  13.361876 22.9581238 4.821699e-13\nVC:D1-OJ:D1     -5.93 -10.728124 -1.1318762 7.393032e-03\nOJ:D2-OJ:D1      3.36  -1.438124  8.1581238 3.187361e-01\nVC:D2-OJ:D1      3.44  -1.358124  8.2381238 2.936430e-01\nOJ:D2-VC:D1      9.29   4.491876 14.0881238 6.908163e-06\nVC:D2-VC:D1      9.37   4.571876 14.1681238 5.774013e-06\nVC:D2-OJ:D2      0.08  -4.718124  4.8781238 1.000000e+00\n```\n:::\n:::\n\n## Interpreting interactions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Importance of interactions\n\n-   When you have statistically significant interaction effects, you can’t interpret the main effects without considering the interactions\n\n-   Interactions can be used to control for unhelpful variation (blocking effect in experiments)\n\n-   Interactions can be mechanistically meaningful \n\n## ANOVA extensions and inference\n\nLinear models can theoretically be performed with an infinite amount of predictors\n\nThe more predictors the more samples needed, your effect sample number is no longer your total sample size but the lowest treatment\n\nInteraction effects with more than 3 (sometimes even more than 2) become very difficult to interpret and visualize\n\nGood experimental design and causal thinking can aid in analysis design, design test then collect data, it is much harder the other way around\n\n## Indicator variables\n\n-   Indicator variables are used if the predictor is qualitative rather\n    than quantitative.\n    -   Consider gender, a categorical variable.\n        -   Let $I_1$ be an indicator variable that takes a value 1 for\n            males and 0 for females.\\\n        -   Let $I_2$ takes 1 for females and 0 for males.\n        -   Note only one of $I_1$ & $I_2$ is sufficient.\n-   The minimum number of indicator variables needed is related to\n    degrees of freedom.\n\n## ANOVA through regression\n\n-   Consider the burn-time data for the four fabrics.\n    -   The four fabric types are categorical.\n    -   Define-\n        -   $I_1$ = 1 for fabric 1 and 0 otherwise\n        -   $I_2$ = 1 for fabric 2 and 0 otherwise\n        -   $I_3$ = 1 for fabric 3 and 0 otherwise\n        -   $I_4$ = 1 for fabric 4 and 0 otherwise\n-   Note that any THREE indicator variables are sufficient for the four\n    fabrics.\n    -   3 df for 4 fabrics\n\n## Regression summary\n\n-   Regress the burn-time response on the indicator variables $I_1$,$I_2$, $I_3$ & $I_4$\n\n\\tiny\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n   burntime   fabric I1 I2 I3 I4\n1      17.8 Fabric 1  1  0  0  0\n2      16.2 Fabric 1  1  0  0  0\n3      17.5 Fabric 1  1  0  0  0\n4      17.4 Fabric 1  1  0  0  0\n5      15.0 Fabric 1  1  0  0  0\n6      11.2 Fabric 2  0  1  0  0\n7      11.4 Fabric 2  0  1  0  0\n8      15.8 Fabric 2  0  1  0  0\n9      10.0 Fabric 2  0  1  0  0\n10     10.4 Fabric 2  0  1  0  0\n11     11.8 Fabric 3  0  0  1  0\n12     11.0 Fabric 3  0  0  1  0\n13     10.0 Fabric 3  0  0  1  0\n14      9.2 Fabric 3  0  0  1  0\n15      9.2 Fabric 3  0  0  1  0\n16     14.9 Fabric 4  0  0  0  1\n17     10.8 Fabric 4  0  0  0  1\n18     12.8 Fabric 4  0  0  0  1\n19     10.7 Fabric 4  0  0  0  1\n20     10.7 Fabric 4  0  0  0  1\n```\n:::\n:::\n\n\n## Regression and ANOVA\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = burntime ~ I1 + I2 + I3, data = fabric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.780 -1.205 -0.460  0.775  4.040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.9800     0.7604  15.754 3.65e-11 ***\nI1            4.8000     1.0754   4.463 0.000392 ***\nI2           -0.2200     1.0754  -0.205 0.840485    \nI3           -1.7400     1.0754  -1.618 0.125206    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.7 on 16 degrees of freedom\nMultiple R-squared:  0.7226,\tAdjusted R-squared:  0.6706 \nF-statistic: 13.89 on 3 and 16 DF,  p-value: 0.0001016\n```\n:::\n:::\n\n\n-   Compare with the one-way output\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n## Regression and ANOVA\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfabric |> lm(formula = burntime~I1+I2+I3) |> anova()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: burntime\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nI1         1 111.521 111.521 38.5718 1.248e-05 ***\nI2         1   1.408   1.408  0.4871    0.4952    \nI3         1   7.569   7.569  2.6179    0.1252    \nResiduals 16  46.260   2.891                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n-   Compare with the one-way output\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476\n```\n:::\n:::\n\n\n## Analysis of Covariance (ANCOVA)\n\nAnalysis of covariance (ANCOVA) is a statistical method that combines linear regression and analysis of variance (ANOVA) to evaluate the relationship between a response variable and various independent variables while controlling for covariates.\n\n\n-   Indicator variables are used as additional regressors along with a\n    quantitative predictor (covariate).\n\n\n## Data example: test anxiety\n\nResearchers investigated the effect of exercises in reducing the level of anxiety, where they measured the anxiety score of three groups of individuals practicing physical exercises at different levels (grp1: low, grp2: moderate and grp3: high).\n\nThe anxiety score was measured pre- and 6-months post-exercise training programs. It is expected that any reduction in the anxiety by the exercises programs would also depend on the participant’s basal level of anxiety score.\n\nIn this analysis we use the pretest anxiety score (`pretest`) as the covariate and are interested in possible differences between group with respect to the post-test anxiety scores.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  id    group pretest posttest\n  <fct> <fct>   <dbl>    <dbl>\n1 15    grp1     19.8     19.4\n2 30    grp2     19.3     17.7\n3 33    grp3     15.5     11  \n```\n:::\n:::\n\n\n## Interaction\nAre the regression slopes the same?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n              Df Sum Sq Mean Sq F value Pr(>F)    \ngroup          2  72.13   36.07 203.973 <2e-16 ***\npretest        1 101.29  101.29 572.828 <2e-16 ***\ngroup:pretest  2   0.04    0.02   0.127  0.881    \nResiduals     39   6.90    0.18                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nInteraction term is not significant (at sig. level of 5%). \n\n\n## Check assumptions\n\nLinearity:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nAppears to be a linear relationship in each training group\n\n\n## Assumptions\n\nResiduals:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-31-2.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-31-3.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-31-4.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Assumptions\n\nNormality:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  variable                 statistic p.value\n  <chr>                        <dbl>   <dbl>\n1 summary(model$residuals)     0.964   0.849\n```\n:::\n:::\n\nThe Shapiro Wilk test was not significant ($\\alpha = 0.05$), so we can assume normality of residuals\n\nEqual variance:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2  0.2604  0.772\n      42               \n```\n:::\n:::\n\nThe Levene’s test was not significant ($\\alpha = 0.05$), so we can assume homogeneity of the residual variances for all groups.\n\n## ANCOVA fit\nEstimates are the slope in each treatment (`group`). Remember that R assigns the first (alphabetical) level of the treatment to `Intercept` \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = posttest ~ pretest + group, data = anxiety)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00467 -0.29485 -0.02866  0.33846  0.68799 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.94088    0.72574  -1.296 0.202074    \npretest      1.02775    0.04202  24.461  < 2e-16 ***\ngroupgrp2   -0.64112    0.15137  -4.235 0.000126 ***\ngroupgrp3   -2.98463    0.15027 -19.862  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4114 on 41 degrees of freedom\nMultiple R-squared:  0.9615,\tAdjusted R-squared:  0.9587 \nF-statistic: 341.5 on 3 and 41 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n## ANCOVA fit\nUse ANOVA table to display\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: posttest\n          Df Sum Sq Mean Sq F value    Pr(>F)    \npretest    1 99.400  99.400  587.16 < 2.2e-16 ***\ngroup      2 74.023  37.011  218.63 < 2.2e-16 ***\nResiduals 41  6.941   0.169                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nPost-hoc tests to see which group is different\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = posttest ~ group + pretest, data = anxiety)\n\n$group\n               diff       lwr        upr p adj\ngrp2-grp1 -1.093333 -1.458662 -0.7280048     0\ngrp3-grp1 -3.060000 -3.425329 -2.6946715     0\ngrp3-grp2 -1.966667 -2.331995 -1.6013381     0\n```\n:::\n:::\n\n\n\n## Data example: fast-food resturants \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n Restaurant  Sales Households Location I1 I2\n          1 135.27        155  Highway  0  0\n          2  72.74         93  Highway  0  0\n          3 114.95        128  Highway  0  0\n          4 102.93        114  Highway  0  0\n          5 131.77        158  Highway  0  0\n          6 160.91        183  Highway  0  0\n          7 179.86        178     Mall  1  0\n          8 220.14        215     Mall  1  0\n          9 179.64        172     Mall  1  0\n         10 185.92        197     Mall  1  0\n         11 207.82        207     Mall  1  0\n         12 113.51         95     Mall  1  0\n         13 203.98        224   Street  0  1\n         14 174.48        199   Street  0  1\n         15 220.43        240   Street  0  1\n         16  93.19        100   Street  0  1\n```\n:::\n:::\n\n\n-   Do we need three separate models?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-38-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n##  ANCOVA \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-39-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n-   In order to allow for different slopes for each location, we define\n    the product (or interaction) variables \n \n\n## Data set up\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  Restaurant  Sales Households Location I1 I2\n1          1 135.27        155  Highway  0  0\n2          2  72.74         93  Highway  0  0\n3          3 114.95        128  Highway  0  0\n4          4 102.93        114  Highway  0  0\n5          5 131.77        158  Highway  0  0\n6          6 160.91        183  Highway  0  0\n```\n:::\n:::\n\n\n\n\n\n\n##  ANCOVA model\n\nWe examine the relationship between restaurant sales (response variable, in thousands of dollars) and the number of households (H) in the restaurant's trading area and the location of the restaurant (Mall, Street, and Highway). We can use the indicator variables ($I_1$ and $I_2$) to define our three locations uniquely. \n\n$Sales = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households$\n\nThis model provides a separate model for each location as well as allows for the interaction between location of the restaurant and the number of households through the slope coefficient\n\n## ANCOVA model\n\n$Sales = \\beta_0 + \\beta_1 I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households$\n\n-   For Highway Locations: $I_1=0$ & $I_2=0$ hence our model simplifies to\n\n    $Sales = \\beta_{0} + (\\beta_3)Households$\n\n-   For Mall Locations: $I_1=1$ & $I_2=0$ so the model becomes\n\n    $Sales = \\beta_{0} + \\beta_{1} + (\\beta_3 + \\beta_4)Households$\n\n-   For Street Locations: $I_1=0$ & $I_2=1$ so the model becomes\n    $Sales = \\beta_{0} + \\beta_2 + (\\beta_3 + \\beta_5) Households$\n    \n    \n\n *note* R does not *require* us to code Location as an indicator variable\n\n## ANCOVA fit \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodl = lm(Sales~Households*Location, data = restaurant)\nouts <- summary(modl)$coefficients\nround(outs[, 1:4], digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          Estimate Std. Error t value Pr(>|t|)\n(Intercept)                 -6.203     11.818  -0.525    0.611\nHouseholds                   0.909      0.083  10.906    0.000\nLocationMall                39.223     16.451   2.384    0.038\nLocationStreet               8.036     16.273   0.494    0.632\nHouseholds:LocationMall     -0.074      0.104  -0.710    0.494\nHouseholds:LocationStreet   -0.012      0.101  -0.120    0.907\n```\n:::\n:::\n\nWe can plug these numbers into our equations \n$Sales = \\beta_{0} + \\beta_{1} I_1 + \\beta_2 I_2 + (\\beta_3 + \\beta_4 I_1 + \\beta_5 I_2)Households$\n\n$Sales = -6.2 + 39.22 + 8.04 + ( 0.909  -0.074 -0.012) Households$\n\n## ANCOVA model\n\n$Sales = -6.2 + 39.22 + 8.04 + ( 0.909  -0.074 -0.012) Households$\n\n-   For Highway Locations: $I_1=0$ & $I_2=0$ hence our model simplifies to\n\n    $Sales =  -6.2 + (0.909)Households$\n\n-   For Mall Locations: $I_1=1$ & $I_2=0$ so the model becomes\n\n    $Sales = -6.2 + 39.22 + (0.909  -0.074)Households$\n\n-   For Street Locations: $I_1=0$ & $I_2=1$ so the model becomes\n    $Sales = -6.2 + 8.04 + (0.909 -0.012) Households$\n\n## Graphing the model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter08_files/figure-revealjs/unnamed-chunk-42-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Summary\n\nANOVA models study categorical predictors (factors).\n     \n    -   Interaction between factors is important. \n    -   ANOVA models and regression models are related and fall under a general family of linear models.\n\n\nANCOVA models employs both numerical variables (covariates) and\nqualitative factors for modelling.\n       \n    -   Interaction between factors and covariates is important.\n\n## Review of non-parametric tests\nNon-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations). \n\nMany non-parametric methods rely on replacing the observed data by their *ranks*. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Spearman's Rank Correlation\n\n\n::::{.columns}\n\n:::{.column}\n\nRank the $X$ and $Y$ variables, and then obtain usual Pearson correlation coefficient.\n\nThe plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(GGally)\n\np <- ggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")),\n  lower = list(continuous = 'cor') \n  )\n```\n:::\n\n\n:::\n\n:::{.column}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Comparison of Pearsonian and Spearman's rank correlations](Chapter08_files/figure-revealjs/fig-spear-1.png){#fig-spear fig-align='center' width=480}\n:::\n:::\n\n\n:::\n::::\n\n## Wilcoxon signed rank test\n\nA non-parametric alternative to the one-sample t-test\n\n$H_0: \\eta=\\eta_0$ where $\\eta$ (Greek letter 'eta') is the population median\n\nBased on based on ranking $(|Y-\\eta_0|)$, where the ranks for data with $Y<\\eta_0$ are compared to the ranks for data with $Y>\\eta_0$\n\n\n::::{.columns}\n\n:::{.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n```\n:::\n:::\n\n\n:::\n\n:::{.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(tv$TELETIME, mu=1680)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283 \n```\n:::\n:::\n\n\n:::\n::::\n\n\n## Non-parametric ANOVA\n\nKruskal-Wallis test: allows to compare three or more groups\n\nMann-Whitney test: allows to compare 2 groups under the non-normality assumption.\n\n\n## Kruskal-Wallis test\nThe null and alternative hypotheses of the Kruskal-Wallis test are:\n\nH0: The 3 groups are equal in terms of the variable\nH1: At least one group is different from the other 2 groups in terms of variable\n\nSimilar to an ANOVA the alternative hypothesis is not that all groups are different. The opposite of all groups being equal (H0) is that at least one group is different from the others (H1).\n\nIn this sense, if the null hypothesis is rejected, it means that at least one group is different from the other 2, but not necessarily that all 3 groups are different from each other. Post-hoc tests must be performed to test whether all 3 groups differ.\n\n## Mann-Whitney test\n\nFor two group comparison, pool the two group responses and then rank the\npooled data\n\nRanks for the first group are compared to the ranks for the second group\n\nThe null hypothesis is that the two group medians are the same:     $H_0: \\eta_1=\\eta_2$.\n\n\n::::{.columns}\n\n:::{.column}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"../data/rangitikei.Rdata\")\nwilcox.test(rangitikei$people~rangitikei$time, conf.int=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n```\n:::\n:::\n\n\n:::\n\n:::{.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(rangitikei$people~rangitikei$time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308 \n```\n:::\n:::\n\n\n:::\n::::\n\n## Another form of test\n\n\n::::{.columns}\n\n:::{.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkruskal.test(rangitikei$people~rangitikei$time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n```\n:::\n:::\n\n\n:::\n\n:::{.column}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwilcox.test(rangitikei$people~rangitikei$time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n```\n:::\n:::\n\n\n\n:::\n::::\n\n## Linear models Review\n\nFamily tree\n\nT-tests, regressions, ANOVAs, and ANCOVAs are related\n\nSimilarities:\n\n    -   1 continuous response variable\n    -   Assumptions: normality, equal variance, and independence of residuals\n\nDifferences:\n\n    - EDA\n    - test statistics\n    - number and type of predictors\n    \n## What's the difference between a t.test and an ANOVA?\n\nA t-test is used to determine whether or not there is a statistically\nsignificant difference between the means of two groups\n\n         \n  -   sample group vs hypothetical population **One-sample t-test**\n  -   two independent samples **Two-sample t-test**\n        - also called independent t-test because the two samples are considered independent\n  -   samples linked in some why, not independent **Paired t-test**\n\n\nAn ANOVA is used to determine whether or not there is a statistically\nsignificant difference between the means of three or more groups, **but not which group**\n\n      \n  -   one factor split into 3 or more groups (levels) **One-way ANOVA**\n  -   two factors split into 3 or more groups (levels) **Two-way ANOVA**\n  -   more than two factors **Three factor ANOVA** etc...\n\n\n## What's the difference between a t.test and an ANOVA?\n\nThe main difference between a t-test and an ANOVA is in how the two\ntests calculate their test statistic to determine if there is a\nstatistically significant difference between groups.\n\nT-test:\n       \n-   T statistic: ratio of the mean difference between two groups relative to overal standard deviation of the differences\n-   One-sample t-test: $t = \\frac{x-\\mu}{s/\\sqrt n}$\n-   Two-sample t-test: $t = \\frac{(\\bar{x_{1}}-\\bar{x_{2}})-d}{\\frac{s_1}{\\sqrt n} + \\frac{s_2}{\\sqrt n}}$  \n-   Paired t-test: $t=\\frac{\\bar{d}}{\\frac{S_{d}}{\\sqrt n}}$ \n\n$s =$ sample standard deviation\n$d =$ difference\n\nANOVA:\n\n-   F statistic: ratio of the variance between the groups relative to the variance within the groups  \n-   $F = \\frac{s_b^2}{s_w^2}$ \nWhere $s_b^2$ is the between sample variance, and $s_w^2$ is the within sample variance.\n\n- MSF/MSE \n\n## When to use a t.test or an ANOVA?\n\nIn practice, when we want to compare the means of two groups, we use a t-test. When we want to compare the means of three or more groups, we use an ANOVA.\n\nSuppose we have three groups we wish to compare the means between: group A, group B, and group C.\n\nIf you did the following t-tests:\n\n-   A t-test to compare the difference in means between group A and group B\n\n-   A t-test to compare the difference in means between group A and group C\n\n-   A t-test to compare the difference in means between group B and group C\n\n## What if we just did many t-tests?\n\nFor each t-test there is a chance that we will commit a type I error, which is the probability that we reject the null hypothesis when it is actually true. Let's say we set this to 0.05 ($\\alpha = 0.05$). This means that when we perform multiple t-tests, this error rate increases.\n\n-   The probability that we commit a type I error with one t-test is 1 – 0.95 = 0.05.\n-   The probability that we commit a type I error with two t-tests is 1 – (0.95^2) = 0.0975.\n-   The probability that we commit a type I error with three t-tests is 1 – (0.95^3) = 0.1427.\n\nThe type I error just increases!\n\nWe use a post-hoc (after) test to see which group is driving differences, these pairwise comparisons have a correction factor to adjust our Type I error\n\n## What's the difference between a regression and an ANOVA?\n\nA regression is used to understand the relationship between predictor variable(s) and a response variable\n         \n  -   one predictor **Simple Regression**\n  -   two or more predictors **Multiple Regression**\n\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, **but not which group**\n\n      \n  -   one factor split into 3 or more groups (levels) **One-way ANOVA**\n  -   two factors split into 3 or more groups (levels) **Two-way ANOVA**\n  -   more than two factors **Three factor ANOVA** etc...\n\nNot too much difference there\n\n## What's the difference between a regression and an ANOVA?\n\nRegression:\n       \n-   T statistic: test if each predictor's estimate is different from 0\n-   $R^2$: proportion of variance explained $\\frac{SSR}{SST}$\n-   F statistic: overall F statistic for the regression model, $\\frac{MS_{reg}}{MS_{error}}$\nwhere $MS = \\frac{SS}{df}$ \n\nANOVA:\n\n-   F statistic: ratio of the variance between the groups relative to the variance within the groups  \n-   $F = \\frac{s_b^2}{s_w^2}$ \n\nwhere $s_b^2$ is the between sample variance, and $s_w^2$ is the within sample variance.\n\n-   MSF/MSE \n\n\nThe F statistic is the same, thats why you can produce an ANOVA table from your regression\n\n## When to use a regression or an ANOVA?\n\nConventionally, a regression refers to continuous predictors and ANOVAs are used for discrete variables.\n\nBUT ...\n\nYou can code discrete variable as indicator variables and then treat them as continuous and run a regression!\n\nGenerally, if you have 1 continuous and 1 discrete variable you should generate indicator variables and run a multiple regression\n\nIf you are using the continuous predictor as a covariate it would be called an **ANCOVA**. \n\n\nYou can code a discrete variable into continuous but if you do the reverse you lose statistical inference.\n\n## Reminder on Hypotheses\n\nT-Test: difference between the means of two groups\n\n-   Null: $\\mu_{1} - \\mu_{2}$ is equal to $0$ \n        Can be $=$, $\\le$, or $\\ge$\n-   Alt: $\\mu_{1} - \\mu_{2}$ is not equal to $0$ \n        Can be $\\ne$, $<$, or $>$\n        \nRegressions: understand the relationship between predictor variable(s) and a response variable\n\n-   Null: true slope coefficient is $= 0$ (i.e. no relationship)\n-   Alt: true slope coefficient $\\ne 0$ (i.e. relationship)\n\nANOVA: difference between the means of three or more groups\n\n-   Null: group means are equal\n-   Alt: At least one mean is different\n\n## Where to go from here?\n\nLinear models form the basis of a lot more statistics tests. \n\n",
    "supporting": [
      "Chapter08_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}