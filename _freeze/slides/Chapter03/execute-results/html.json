{
  "hash": "0dea4198afe6fa2efe916f2ea70f9918",
  "result": {
    "markdown": "---\ntitle: \"Chapter 3:<br>Probability Concepts & Distributions\"\nimage: img/norms.png\nformat: \n  revealjs:\n    width: 1050\n    height:\t700\n    scrollable: true\n    transition: fade\n    theme: [default, myquarto.scss]\n    slide-number: c/t  \n    logo: img/L_Color.png\n    footer: \"[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)\"\n    styles:\n      - revealjs.dark:\n        background-color: #222\n        color: #fff\n    \nexecute:\n  echo: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n:::\n\n\n# Introduction to Probability {.background-black}\n\n> “Misunderstanding of probability may be the greatest of all impediments to scientific literacy.”\n>\n> — Stephen Jay Gould\n\n::: footer\nIntro to probability\n:::\n\n\n## Probability and randomness\n\n::::{.columns}\n\n:::{.column width=\"70%\"}\n\n::: incremental\n\n- **Probability and randomness are placeholders for incomplete knowledge.** \n\n- After I shuffled a deck of cards, you might consider the identity of the top card to be \"random\".\n\n- But is it really?\n\n- If you knew the starting positions of the cards and a good HD video of my shuffling, you could surely _know_ the positions of the cards, and which is on top.\n\n- Likewise for rolling a die. If we know everything about the starting position, how it was thrown, the texture of the surface, humidity, etc., could we predict what it would roll?\n\n:::\n\n:::\n\n\n:::{.column width=\"30%\"}\n\n![](img/cards.jpg)\n![](img/dice.png)\n\n:::\n\n::::\n\n## Probability as a relative frequency\n\n::: incremental\n\n-   The classical definition of probability is just the relative\n    frequency of an event.\n    -   If a fair die is rolled, there are $n = 6$ possible outcomes.\n        Let the event of interest be *getting a number 4 or more*. The\n        probability of this event is 3 out of 6 or $p=1/2$.\n-   The *sample space* or the set of all possible outcomes need not be\n    finite.\n    -   Example: Tossing a coin until the first head appears will result\n        in an infinite sample space. The probability can be viewed as a\n        limiting or long run fraction of $m/n$ (i.e. when\n        $n \\to \\infty$).\n-   When the sample space is finite and outcomes are equally likely, we\n    can assume that classical probability will be the same as empirical\n    probability.\n    -   Example: To find the probability of a fair coin landing heads it\n        is not necessary to toss the coin repeatedly and observe the\n        proportion of heads.\n-   Probabilities can only be between $0$ (impossible) and $1$\n    (certain).\n-   Probabilities can be subjective (such as expert opinion, or a guess)\n\n:::\n\n## Mutually Exclusive Events\n\nFor mutually exclusive events,\n\n1.  The probability of any two events co-occurring is zero\n\n2.  The probability of one event or another event occurring is the sum\n    of the two respective probabilities.\n\n3.  The probability of any one event not occurring is the sum of those\n    remaining.\n\n::: fragment\nExample: A randomly selected single digit can be either odd (Event $O$)\nor even (Event $E$).\\\nThe events $O$ and $E$ are mutually exclusive because a number cannot be\nboth odd and even.\\\nThe sample space is $\\{0,1,2,3,4,5,6,7,8,9\\}$.\n:::\n\n::: fragment\n1.  $\\rm{Pr(E~\\&~O)=0}$\n2.  $\\rm{Pr(E~ or~O)=1}$\n3.  $\\rm{Pr(E)=1-Pr(O)}$ *and* $\\rm{Pr(O)=1-Pr(E)}$\n:::\n\n## Statistical Independence\n\nIf events $A$ and $B$ are statistically independent, then\n$P(A \\text{ and } B) = P(A) \\times P(B)$.\n\n::: callout-tip\n## Conditional probability\n\n-   $P(A|B)$ is the probability of event $A$ occurring given that event\n    $B$ is has occurred.\n\n-   For example, the probability of a card you've drawn being a 5, given\n    that it is a spade.\n\n-   The sample space is reduced to that where $B$ (e.g. the card is a\n    spade) has occurred.\n:::\n\n::: fragment\nWe say that two events ($A$ and $B$) are **independent** if\n$P(A | B) = P(A)$ **and** $P(B | A) = P(B)$.\n\nObserving event $A$ doesn't make event $B$ any more or less likely, and\nvice versa.\n\nFor any two events $A$ and $B$,\n$P(A \\text{ and } B ) = P(A|B) \\times P(B)$ and\n$P(A \\textbf{ and } B ) = P(B|A) \\times P(A)$.\n:::\n\n## Blood Group Example {.smaller}\n\n::: plot-top-right\n![](img/RhK.png)\n:::\n\n<br> Two systems for categorising blood are:\n\n-   the Rh system (Rh+ and Rh--)\n-   the Kell system (K+ and K--)\n\nFor any person, their blood type in any one system\\\nis **independent** of their blood type in any other.\n\nFor Europeans in New Zealand,\\\nabout 81% are Rh+ and about 8% are K+.\n\n<br>\n\nFrom the table:\n\n-   If a European New Zealander is chosen at random, what is the\n    probability that they are (Rh+ and K+) or (Rh-- and K--)?\n\n    -   0.0648 + 0.1748 = 0.2396\n\n-   Suppose that a murder victim has a bloodstain on him with type (Rh--\n    and K+), presumably from the assailant. What is the probability that\n    a randomly selected person matches this type?\n\n    -   0.0152\n\n## Bayes rule\n\n$$P(A\\mid B)=\\frac {P(B\\mid A)P(A)}{P(B)}~~~~~~~~\\rm{s.t}~~ P(B)>0$$\n\n-   $P(A\\mid B)$ and $P(B\\mid A)$ are conditional probabilities.\n\n-   $P(A)$ and $P(B)$ are marginal or prior probabilities.\n\n## Prevalence, sensitivity, specificity, PPV, and NPV\n\nLet $D$ be the event of a person having the Disease and $H$ be the event\nof a person being Healthy (i.e., not having the disease). The outcome of a test for the disease can be either positive $(T_+)$ or negative $(T_-)$.\n\nConsider the following definitions of conditional probabilities:\n\n-   **prevalence** is the overall probability one has the disease, or $P(D)$.\n-   **sensitivity** the probability that one tests positive given one has the disease, or $P(T_+ | D)$.\n-   **specificity** the probability that one tests negative given one does not have the disease, or $P(T_- | H)$.\n-   **positive predictive value** of a test is the probability one has the disease given that one has tested positive, or $P(D \\mid T_{+})$\n-   **negative predictive value** of a test is the probability that one is healthy given that one has tested negative, or $P(H \\mid T_{-})$\n\n## Example\n\nSay the following were true:\n\n- Prevalence: $P(D) = 0.03$ and $P(H) = 1-0.03=0.97$\n- Sensitivity: $P(T_+\\mid D) = 0.98$ \n- Specificity: $P(T_{-}\\mid H) = 0.95$\n\nWe can use Bayes Rule to answer the following questions:\n\n- What proportion of the overall population will test positive vs negative?\n- What are the implications of a positive or negative test result?\n\n\n## Probability tree {.smaller}\n\nIt can be useful to visualise the probabilities of the four possible states using a tree diagram.\n\n::: left-code-wide\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-ee8091187a6c0383abb9\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ee8091187a6c0383abb9\">{\"x\":{\"diagram\":\"digraph {\\n  \\n  # horizontal alignment\\n  graph [rankdir = LR]\\n  \\n  # Nodes with labels\\n  node [shape = oval]\\n  Start [label = \\\"Start\\\", shape = box]\\n  Healthy [label = \\\"Healthy\\\"]\\n  Disease [label = \\\"Disease\\\"]\\n  PosH [label = \\\"Positive\\\"]\\n  NegH [label = \\\"Negative\\\"]\\n  PosD [label = \\\"Positive\\\"]\\n  NegD [label = \\\"Negative\\\"]\\n  pPosH [label = \\\"P(H & T+) = 0.0485\\\", shape = none]\\n  pNegH [label = \\\"P(H & T-) = 0.9215\\\", shape = none]\\n  pPosD [label = \\\"P(D & T+) = 0.0294\\\", shape = none]\\n  pNegD [label = \\\"P(D & T-) = 0.0006\\\", shape = none]\\n  \\n  # Edges with labels\\n  Start -> Healthy [label = \\\"0.97\\\"]\\n  Start -> Disease [label = \\\"0.03\\\"]\\n  Healthy -> PosH [label = \\\"0.05\\\"]\\n  Healthy -> NegH [label = \\\"0.95\\\"]\\n  Disease -> PosD [label = \\\"0.98\\\"]\\n  Disease -> NegD [label = \\\"0.02\\\"]\\n  PosH -> pPosH \\n  NegH -> pNegH \\n  PosD -> pPosD \\n  NegD -> pNegD \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n**Rules of the Probability Tree**\n\n1.  Within each level, all branches are mutually exclusive events.\n\n2.  The tree covers all possibilities (i.e., the entire sample space).\n\n3.  We multiply as we move along branches.\n\n4.  We add when we move across branches.\n\n\n:::\n\n::: left-plot-narrow\n<br><br><br><br><br><br>\n\n|     | T+     | T-     |      |\n|-----|--------|--------|------|\n| *D* | 0.0294 | 0.0006 | 0.03 |\n| *H* | 0.0485 | 0.9215 | 0.97 |\n\n:::\n\n\n\n## Example continued {.smaller}\n\n**What proportion of the overall population will test positive vs negative?**\n\nThe overall proportion of positive tests will be given by:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n$$\n\\begin{aligned}\nP(T_{+}) &= P(T_{+} \\& D) + P(T_{+} \\& H) \\\\\n&= P(T_{+} \\mid D)P(D) + P(T_{+} \\mid H)P(H) \\\\\n&= 0.98 \\times 0.03 + 0.05 \\times 0.97 \\\\\n&= 0.0779\n\\end{aligned}\n$$\nThe overall proportion of negative tests will be given by:\n\n\n$$\n\\begin{aligned}\nP(T_{-}) &= 1 - P(T_{+}) \\\\ &= 0.9221\n\\end{aligned}\n$$\n\nComplete table of probabilities:\n\n|     | T+     | T-     |      |\n|-----|--------|--------|------|\n| *D* | 0.0294 | 0.0006 | 0.03 |\n| *H* | 0.0485 | 0.9215 | 0.97 |\n|     | 0.0779 | 0.9221 | 1    |\n\n## Example continued {.smaller}\n\n**What are the implications of a positive or negative test result?**\n \nAccording to Bayes rule, the probability of a random person having the disease given they've tested positive is given by:\n\n$$\n\\begin{aligned}\nP(D\\mid T_{+}) &= \\frac {P(T_{+}\\mid D)P(D)} {P(T_{+})} \\\\\n&= \\frac{0.98 \\times 0.03}  {0.0779} \\\\\n&= 0.3774\n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nAccording to Bayes rule, the probability of a random person ***not*** having the disease given they've tested ***negative*** is given by:\n\n$$\n\\begin{aligned}\nP(H \\mid T_{-}) &= \\frac {P(T_{-} \\mid H)P(H)} {P(T_{-})} \\\\ \n&= \\frac{0.95 \\times 0.97}  {0.9221} \\\\\n&= 0.9993\n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\nThe positive predictive value of the test is poor—only 38% of the subjects who tested positive will have the disease. \n\nThe negative predictive value is better—if a random subject tests negative, they're very unlikely to have the disease.\n\n\n# Discrete probability distributions {.background-black}\n\n::: footer\nDiscrete probability distributions\n:::\n\n## Discrete probability distributions\n\nConsider the number of eggs $(X)$ in an Adelie penguin's nest. The\nvalues range from $1$ to $5$, each with a certain probability (or\nrelative frequency) of occurrence.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=384}\n:::\n:::\n\n\n-   Note the probabilities add to $1$ because ${1,2,3,4,5}$ is a\n    complete sample space.\n\nThe population mean $\\mu_X$ is simply the sum of each outcome multiplied\nby its probability.\n\n$$\\mu_X = E(X)= \\sum xP(X=x)=\\sum xP(x)$$\n\nIn R,\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- 1:5\nP <- c(0.1, 0.2, 0.3,0.25,0.15)\n(Mean=sum(X*P))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.15\n```\n:::\n:::\n\n\nThe population variance is given by\n\n$$Var(X)= \\sigma_X^2=\\sum (x-\\mu_X)^2 P(x)$$\n\nThe population SD is simply the square-root of the variance.\n\nIn R,\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- 1:5\nP <- c(0.1, 0.2, 0.3,0.25,0.15)\nMean=sum(X*P)\n(Variance =sum((X-Mean)^2*P))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.4275\n```\n:::\n\n```{.r .cell-code}\n(SD=sqrt(Variance))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.19478\n```\n:::\n:::\n\n\n## Binomial distribution {.smaller}\n\n::: left-code-wide\nConsider a variable that has two possible outcomes\\\n(say *success* and *failure*, with 50% probabilty each).\\\nThis can be described as a \"Bernoulli\" random variable.\n\nA \"Binomial\" is just a collection of Bernoulli trials.\n\nLet $X$ be the number of heads when two coins are tossed.\n\nThe count of the number of *successes* $X$ out of a fixed total of\\\n$n$ *independent* trials follows the binomial distribution.\n\nThat is, $X \\sim Bin(n, p)$, where $p$ the probability of a success.\n\nThe binomial probability function $P(X=x)$ or $P(x)$\\\nis given by $$P(x)={n \\choose x}p^{x}(1-p)^{n-x}$$\n\nFor $n=10$, $p=0.3$, the binomial probabilities,\\\n$P(x)$ for $x=0,1,2, \\dots, 10$, are plotted to the right.\n\nIf each of 10 basketball shots succeeded with probability 0.3, this\ndescribes the probability of your total score out of 10.\n:::\n\n::: right-plot-narrow\n![](img/binom.png)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndfm <- data.frame(\n  x = as.factor(0:10), \n  Probability = dbinom(x = 0:10, size = 10, prob = 0.3))\nggplot(dfm) + aes(x = x, y = Probability) + geom_col() +\n  xlab(\"Number of successes (x)\") +\n  annotate(geom = \"table\", label = list(dfm), x=11, y=.05)\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n\n## Example\n\nA microbiologist plates out certain bacteria on a plate, and picks out\n10 colonies. She knows that the probability of successfully creating a\nrecombinant is 0.15.\n\nWhat is the probability that if she mixes all 10 colonies in a growth\nmedium with penicillin, something (anything) will grow?\n\nIn other words:\n\nIf $X \\sim Bin(n = 10, p = 0.15)$, what is $P(x > 0)$?\n\nNote $P(x > 0)=1-P(x = 0)$. So in R, compute this as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n1 - dbinom(x=0, size=10, prob=.15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8031256\n```\n:::\n:::\n\n\nor\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n1-pbinom(q=0, size=10, prob=.15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8031256\n```\n:::\n:::\n\n\n## Binomial\n\nThe code `pbinom(k,size=n,prob=p)` gives the cumulative probabilities up to and including the quantile $k$.\n\nThe Probability Mass Function (PMS) for a binomial random variable is:\n\n$$P(X\\leq k)=\\sum _{i=0}^{k}{n \\choose x}p^{x}(1-p)^{n-x}$$\n\nThe mean and variance of the binomial random variable is given by\n\n$$\\mu_X=np~~~~ \\sigma^2_X=np(1-p)$$\n\nIn the last example, the *expected number* of recombinant strain of\nbacteria is\n\n$$\\mu_X=np=10*0.15=1.5$$\n\nwith standard deviation\n\n$$\\sigma_X=\\sqrt {np(1-p)}=1.129159$$\n\n## Poisson distribution \n\nThe Poisson distribution is used to obtain the probabilities of counts\nof relatively rare events that occur independently in space or time.\n\nSome Examples:\n\n-   The number of snails in a quadrat $(1~m^2)$\n\n-   Fish counts in a visual transect (25m x 5m)\n\n-   Bacterial colonies in 2 litres of milk\n\n\n## Poisson distribution \n\nThe random variable $X$, the number of occurrences (count), often\nfollows the Poisson distribution whose probability function is given by\n\n$$\\Pr(x)= \\frac{\\lambda^x e^{-\\lambda}}{x!}~~~ x=0,1,2,\\dots, \\infty$$\n\nThe parameter $\\lambda$ is the mean which is also equal to the variance.\n\n$$\\mu_X=\\lambda~~~~ \\sigma^2_X=\\lambda$$\n\nMain assumptions:\n\n1.  The events occur at a constant average rate of $\\lambda$ per unit\n    time or space.\n\n2.  Occurrences are independent of one another as well as they do not\n    happen at exactly the same unit time or space.\n\n## Poisson example {.smaller}\n\n::: plot-top-right\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n\n<br> Consider the number of changes that accumulate along a\\\nstretch of a neutrally evolving gene over a given period of time.\n\nThis is a Poisson random variable with a\\\npopulation mean of $\\lambda=kt$, where\\\n$k$ is the number of mutations per generation, and\\\n$t$ is the time in generations that has elapsed.\n\n<br> <br> <br> <br> <br> <br>\n\nAssume that $k = 1\\times10^{-4}$ and $t = 500$.\n\nFor $\\lambda=kt=0.05$, the Poisson probabilities are shown in the\nfollowing plot.\n\nWhat is the probability that at least one mutation has occurred over\nthis period?\n\n$P(x > 0)=1-P(x = 0)$ is found in R as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n1 - dpois(x=0, lambda=0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04877058\n```\n:::\n:::\n\n\n# Continuous probability distributions {.background-black}\n\n::: footer\nContinuous probability distributions\n:::\n\n## Continuous probability distributions\n\nA discrete random variable takes values which are simply points on a\nreal line. In other words, there is an inherent discontinuity in the\nvalues a discrete random variable can take.\n\nIf a random variable, $X$, can take any value (i.e., not just integers)\nin some interval of the real line, it is called a *continuous* random\nvariable.\n\nE.g., height, weight, length, percentage protein\n\nFor a discrete random variable $X$, the associated probabilities\n$P(X=x)$ are also just points or masses, and hence the probability\nfunction $P(x)$ is also called as the probability mass function (PMF).\n\nFor continuous random variables, probabilities can be computed when the\nvariable falls in an interval such as $5$ to $15$, but not when it takes\na fixed value such as $10$ (which is equal to zero). \n\nThe Probability Density Function (PDF) gives the *relative likelihood* of any particular value.\n\n## Continuous probability distributions\n\n::: left-code-wide\nFor example, consider a random proportion $(X)$ between $0$ and $1$.\nHere $X$ follows a (standard) continuous uniform distribution whose\n(probability) density function $f(x)$ is defined as follows:\n\n$$f(x)=\\begin{cases}{1}~~~\\mathrm {for} \\ 0\\leq x\\leq 1,\\\\[9pt]0~~~\\mathrm {for} \\ x<0\\ \\mathrm {or} \\ x>1\\end{cases}$$\nThis constant density function is the simple one in the graph to the\nright.\n:::\n\n::: right-plot-narrow\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = seq(-.5, 1.5, length=1000),\n       `f(x)` = dunif(x, min=0, max=1)) |> \n  ggplot() + \n  aes(x = x, y = `f(x)`) +\n  geom_area(colour = 1, alpha = .2)\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n\n::: footer\nContinuous probability distributions\n:::\n\n## Continuous probability distributions\n\nThe *density* is the relative likelihood of any value of $x$; that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2. \n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nd <- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np <- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n  \np\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThe **black line** is the PDF, or $f(x)$. The orange area underneath the whole PDF is 1. \n\n## Continuous probability distributions\n\nThe *density* is the relative likelihood of any value of $x$; that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2. \n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nd <- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np <- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  annotate(geom = \"path\", \n    x = c(19.3, 19.3, 13), \n    y = c(0, rep(dnorm(19.3,20,2),2) ),\n    arrow = arrow(),\n    colour = \"dodgerblue4\", size = 1.1)\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThe **black line** is the PDF, or $f(x)$. The orange area underneath the whole PDF is 1. \n\nThe **density** at 19.3 is $f(19.3) = 0.1876$). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndnorm(19.3, mean = 20, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1876202\n```\n:::\n:::\n\n\n## Continuous probability distributions\n\nThe *density* is the relative likelihood of any value of $x$; that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2. \n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nd <- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np <- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  geom_area(\n    data = d |> filter(x <= 19.3),\n    fill = \"dodgerblue4\",\n    size  = 1.1, alpha = .6)\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThe **black line** is the PDF, or $f(x)$. The orange area underneath the whole PDF is 1. \n\nThe area under the curve to the left of the value 19.3 is given by the Cumulative Density Function (CDF), or $F(x)$. It gives the probability that x < 19.3; $F(19.3) = 0.3632$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(19.3, mean = 20, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3631693\n```\n:::\n:::\n\n\n\n\n## Continuous probability distributions\n\nThe cumulative distribution function, CDF, $F(x)$ gives the left tail\narea or probability up to $x$. This is probability is found as\n\n$$F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt$$\nThe relationship between\nthe density function $f(x)$ and the distribution function $F(x)$ is\ngiven by the Fundamental Theorem of Calculus.\n\n$$f(x)={dF(x) \\over dx}$$\n\n## Continuous probability distributions\n\nThe total area under the PDF curve is $1$. The probability of obtaining\na value between two points ($a$ and $b$) is the area under the PDF curve\nbetween those two points. This probability is given by $F(b)-F(a)$.\n\nFor the uniform distribution $U(0,1)$, $f(x)=1$. So\n\n$$F_{X}(x)=\\int _{-\\infty }^{x}\\,dt=x$$\n\nFor example, the probability of a randomly drawn fraction from the\ninterval $[0,1]$ to fall below $x=0.5$ is 50%.\n\nThe probability of a random fraction falling between $a=0.2$ and $b=0.8$\nis\n\n$$F(b)-F(a)=0.8-0.2=0.6$$\n\n::: footer\nContinuous probability distributions\n:::\n\n## The Normal (Gaussian) Distribution {.smaller}\n\n![](img/gauss_deutschemark.png){.absolute top=\"1\" right=\"0\" width=\"300\"}\n\n::: left-code-wide\nThe **Gaussian** or **Normal Distribution** is parameterised in terms of\nthe mean $\\mu$ and the variance $\\sigma ^{2}$ and its Probability\nDensity Function (PDF) is given by\n\n$$f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}$$\nA **Standard Normal Distribution** has mean $\\mu=0$ and standard\ndeviation $\\sigma=1$. It has a simpler PDF:\n\n$$f(z)={\\frac {1}{ {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^{2}}$$ If\n$X \\sim N(\\mu, \\sigma)$, you can convert the $X$ values into $Z$-scores\nby subtracting the mean $\\mu$ and dividing by the standard deviation\n$\\sigma$.\n\n$$Z={\\frac {X-\\mu }{\\sigma }}$$\n\nWe often deal with the standard normal because the *symmetric* bell\nshape of the normal distribution remains the same for all $\\mu$ and\n$\\sigma$.\n:::\n\n::: right-code-narrow\n<br> <br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndfn <- tibble(x=seq(-4,4,length=1000), \n              `f(x)` = dnorm(x), \n              `F(x)` = pnorm(x))\np1 <- ggplot(dfn) + aes(x=x,y=`f(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np2 <- ggplot(dfn) + aes(x=x,y=`F(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Cumulative Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np1/p2 \n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n\n::: footer\nContinuous probability distributions\n:::\n\n\n## Example of a normal {.smaller}\n\n![](img/snail.jpg){.absolute top=\"1\" right=\"120\" width=\"200\"}\n\nThe weight of an individual of *Amphibola crenata*, a marine snail,\\\nis normally distributed with a mean of $40g$ and variance of $20g^2$.\n\n::: right-plot-narrow\n<br> <br> <br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndfs <- tibble(x=seq(20, 60, length=1000), \n    `f(x)` = dnorm(x, mean=40, sd=sqrt(20)))\n\nps <- ggplot(dfs) + aes(x = x, y = `f(x)`) + \n  geom_area(fill=\"gray\") +\n  geom_vline(xintercept=40) \n\nps\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n\n::: {.absolute top=\"20%\" left=\"0%\" width=\"60%\"}\n::: {.fragment .fade-in fragment-index=\"1\"}\n::: {.fragment .fade-out fragment-index=\"4\"}\nWhat is the probability of getting a snail that weighs between $35g$ and\n$50g$?\n:::\n:::\n\n::: {.fragment .fade-in fragment-index=\"2\"}\n::: {.fragment .fade-out fragment-index=\"4\"}\nIn R, the function `pnorm()` gives the CDF.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(50, mean=40, sd=sqrt(20)) - \n  pnorm(35, mean=40, sd=sqrt(20)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8555501\n```\n:::\n:::\n\n:::\n:::\n\n::: {.fragment .fade-in fragment-index=\"3\"}\n::: {.fragment .fade-out fragment-index=\"4\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nps + \n  geom_area(data = dfs |> filter(x < 50 & x > 35),\n            fill=\"coral1\", alpha=.5)\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n:::\n:::\n\n::: {.absolute top=\"20%\" left=\"0%\" width=\"60%\"}\n::: {.fragment .fade-in fragment-index=\"5\"}\nWhat is the probability of getting a snail that weighs below $35g$ or\nover $50g$?\n:::\n\n::: {.fragment .fade-in fragment-index=\"6\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npnorm(35, mean=40, sd=sqrt(20)) + \n  pnorm(50, mean=40, sd=sqrt(20), lower.tail=FALSE) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1444499\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nps + \n  geom_area(data = dfs |> filter(x > 50),\n            fill=\"coral1\", alpha=.5) + \n  geom_area(data = dfs |> filter(x < 35),\n            fill=\"coral1\", alpha=.5)\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=480}\n:::\n:::\n\n:::\n:::\n\n## Areas (probabilities) under the standard normal\n\nUnder standard normal, the areas under the PDF curve are shown below for\nvarious situations.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n::: footer\nContinuous probability distributions\n:::\n\n\n## Small sample effect\n\nFor small samples, the shape might be difficult to judge.\n\n\n::: {.cell .custom4060 layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nset.seed(1234)\ndfm <- data.frame(\n  x=rnorm(50, \n          mean=80, \n          sd=12)\n  )\n\np1 <- ggplot(dfm) + \n  geom_histogram(\n    aes(x=x, y=after_stat(density)), \n    colour=1\n    ) + \n  stat_function(\n    fun = dnorm, \n    args = list(mean = 80, sd = 12), \n    geom = \"line\"\n    ) +\n  xlim(min(dfm), max(dfm))\n\np2 <- ggplot(dfm) + aes(x) + \n  geom_boxplot() +\n  xlim(min(dfm), max(dfm)) +\n  theme_void()\n\nlibrary(patchwork)\np1 / p2 + plot_layout(heights = c(5, 1))\n```\n\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-27-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n::: footer\nSkewed continuous probability distributions\n:::\n\n## Normal quantile plots\n\nIn a normal quantile plot, the quantiles of the sample are plotted\nagainst the theoretical quantiles of the fitted normal distribution.\n\nThe points should roughly lie on a straight line\n\nWe can also compare the empirical and theoretical CDFs.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n**TV viewing time data**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter03_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n::: footer\nSkewed continuous probability distributions\n:::\n\n \n## Which distribution is most appropriate?\n\n-   Remember, theoretical distributions aren't real—they're just models—but they can be useful. Keep your purpose in mind. \n\n-   Choose the simplest distribution that provides an adequate fit. \n\n-   Data may be best served by a mixture of two or more distributions rather than a single distribution.\n\n\n",
    "supporting": [
      "Chapter03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\r\n<script src=\"../site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/grViz-binding-1.0.11/grViz.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}