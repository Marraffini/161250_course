{
  "hash": "53aaf13cd72e4f363e83b537691e1587",
  "result": {
    "markdown": "---\ntitle: \"Chapter 7:<br>Models with Multiple Predictors\"\nimage: img/3d.png\nformat: \n  revealjs:\n    width: 1050\n    height:\t700\n    scrollable: true\n    transition: fade\n    theme: [default, myquarto.scss]\n    slide-number: c/t  \n    logo: img/L_Color.png\n    footer: \"[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)\"\n    styles:\n      - revealjs.dark:\n        background-color: #222\n        color: #fff\n    \nexecute:\n  echo: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Learning Objectives:\n\n-   Understand and describe a multiple linear regression, the difference from a simple linear regression, and when each are appropriate\n-   Fit and display a multiple regression\n-   Understand and interpret different types of Sums of Squares\n-   Understand and test for multicollinearity and other assumptions\n\n\n## What is a multiple Regression?\n-   In a simple regression, there is only one predictor.\n-   Multiple regression modelling involves many predictors.\n\n\n\n## When to use multiple predictors\n\n-   *Statistical control of a confound*: controlling treatment for some unwanted variability\n\n-   *Multiple causation*: multiple things are thought to cause changes in the outcome variable\n\n-   *Interactions*: we are interested in how two variables may combine to change our outcome (will cover this in the next chapter)\n\n## Multiple Regression\nWhere to start:\n\n-   Question and hypothesis + domain knowledge\n\n    -   What are we interested in testing?\n    -   What data is collected? do those variables make sense?\n    -   What kind of data are my response and predictor variables?\n    \n-   Perform EDA first\n\n    -   A scatter plot matrix can show nonlinear relationships\n    -   A correlation matrix will only show the strength of pairwise\n        linear relationships\n\n-   Look for the predictors having the largest correlation with response\n\n    -   Look for inter-correlations between the predictors and choose\n        the one with high correlation with response variable but\n        uncorrelated with the rest.\n\n\n## Data example\nBasketball team summary for 2020 regular season\\\n\nWe have 2118 data points which equates to 1059 games (2 teams per game), 2020 was a short season\n\n\nEach team in a standard season plays 82 games. That means in total, an NBA season is comprised of 1,230 games. \n\n\n\\small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  Team     Game Spread Outcome MIN PTS       P2p       P3p       FTp OREB DREB\n1  ATL 21900014     17       W  48 117 0.6000000 0.3548387 0.8571429    8   34\n2  ATL 21900028      4       W  48 103 0.6296296 0.3000000 0.5333333    9   43\n3  ATL 21900043     -2       L  48 103 0.4736842 0.3333333 0.6875000    8   37\n4  ATL 21900052    -15       L  48  97 0.5454545 0.2820513 0.6666667    9   24\n5  ATL 21900066     -9       L  48  97 0.5370370 0.2058824 0.6923077   16   34\n6  ATL 21900099      8       W  48 108 0.5333333 0.3666667 0.6875000    9   39\n  AST TOV STL BLK PF\n1  27  13   9   2 15\n2  22  18   5   9 26\n3  23  21  12   3 25\n4  28  20  14   7 29\n5  20  16   5   5 15\n6  22  18   9   5 20\n```\n:::\n:::\n\n\n`Spread`: Point difference between winning and losing team\\\n`PTS`: Total points scored by a team in a game\\\n`P2p`: Percent of 2-pointers made\\\n`P3p`: Percent of 3-pointers made\\\n`FTp`: Percent of free-throws made\\\n`OREB`: Offensive rebounds\\\n`DREB`: Defensive rebounds\\\n`AST`: Assists\\\n`STL`: Steals\\\n`BLK`: Blocks\\\n\n\n\n\n## Data example\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptions(digits=3)\ncor(teams[,c(3, 6:12, 14:15)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Spread     PTS      P2p      P3p     FTp     OREB     DREB     AST\nSpread  1.0000  0.5730  0.36338  0.42719  0.1063 -0.00240  0.46470  0.3477\nPTS     0.5730  1.0000  0.48713  0.55094  0.1897 -0.01487  0.14131  0.5103\nP2p     0.3634  0.4871  1.00000 -0.00601 -0.0105 -0.28585  0.02484  0.3633\nP3p     0.4272  0.5509 -0.00601  1.00000  0.0297 -0.18736  0.00164  0.4054\nFTp     0.1063  0.1897 -0.01053  0.02967  1.0000 -0.08960 -0.01724 -0.0297\nOREB   -0.0024 -0.0149 -0.28585 -0.18736 -0.0896  1.00000  0.03882 -0.0996\nDREB    0.4647  0.1413  0.02484  0.00164 -0.0172  0.03882  1.00000  0.0468\nAST     0.3477  0.5103  0.36333  0.40540 -0.0297 -0.09960  0.04681  1.0000\nSTL     0.1246  0.0352  0.01897 -0.04479  0.0115 -0.00389 -0.18027  0.0518\nBLK     0.1837  0.0623  0.03757  0.00540 -0.0185  0.00311  0.20700  0.0411\n            STL      BLK\nSpread  0.12460  0.18367\nPTS     0.03525  0.06234\nP2p     0.01897  0.03757\nP3p    -0.04479  0.00540\nFTp     0.01153 -0.01849\nOREB   -0.00389  0.00311\nDREB   -0.18027  0.20700\nAST     0.05178  0.04112\nSTL     1.00000  0.03267\nBLK     0.03267  1.00000\n```\n:::\n:::\n\n\n## Inter-relationships\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Full regression in R\n\nPlaces all of the predictors in the model\n\nEquivalent of throwing everything in and hoping something sticks\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Spread ~ ., data = teams_forlm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.29  -5.72  -0.15   5.66  30.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -142.4223     2.8728  -49.58  < 2e-16 ***\nPTS            0.0624     0.0269    2.32     0.02 *  \nP2p           74.1170     3.8391   19.31  < 2e-16 ***\nP3p           74.0109     3.3449   22.13  < 2e-16 ***\nFTp           15.4686     2.0265    7.63  3.4e-14 ***\nOREB           0.7191     0.0623   11.54  < 2e-16 ***\nDREB           1.2033     0.0367   32.81  < 2e-16 ***\nAST           -0.0350     0.0479   -0.73     0.47    \nSTL            1.0685     0.0677   15.79  < 2e-16 ***\nBLK            0.3503     0.0784    4.47  8.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.79 on 2108 degrees of freedom\nMultiple R-squared:  0.622,\tAdjusted R-squared:  0.62 \nF-statistic:  385 on 9 and 2108 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n\n\n## Residuals\nJust like with a simple regression we examine residuals to look for patterns\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\nThese look great, probably because we have a ton of data in this example. \n\n\n## Multicollinearity {.increment}\n\n-   *Multicollinearity* is where at least two predictor variables are\n    highly correlated.\n\n-   Multicollinearity does *not* affect the residual SD very much, and\n    doesn't pose a major problem for prediction.\n\n-   The major effects of multicollinearity are:\n\n    -   It changes the estimates of the coefficients.\n    -   It inflates the variance of the estimates of the coefficients.\n        That is, it increases the uncertainty about what the slope\n        parameters are.\n    -   Therefore, it matters when testing hypotheses about the effects\n        of specific predictors.\n\n## Multicollinearity\n\n-   The impact of multicollinearity on the variance of the estimates can\n    be quantified using the Variance Inflation Factor (VIF \\< 5 is\n    considered ok).\n\n-   There are several ways to deal with multicollinarity, depending on\n    context. We can discard one of highly correlated variable, perform\n    ridge regression, or think more carefully about how the variables\n    relate to each other.\n\n## Multicollinearity \n\nIn R we can examine Multicollinearity using the function `vif()`\n\nRemember our basketball regression:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(car)\nvif(full_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n PTS  P2p  P3p  FTp OREB DREB  AST  STL  BLK \n3.07 2.14 2.24 1.14 1.37 1.14 1.50 1.06 1.05 \n```\n:::\n:::\n\n\nThese values are all small (VIF \\< 5) so we can continue on in interpreting the regression.\n\n## Tidy summary\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>t-tests for model parameters</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -142.422 </td>\n   <td style=\"text-align:right;\"> 2.873 </td>\n   <td style=\"text-align:right;\"> -49.575 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PTS </td>\n   <td style=\"text-align:right;\"> 0.062 </td>\n   <td style=\"text-align:right;\"> 0.027 </td>\n   <td style=\"text-align:right;\"> 2.320 </td>\n   <td style=\"text-align:right;\"> 0.020 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> P2p </td>\n   <td style=\"text-align:right;\"> 74.117 </td>\n   <td style=\"text-align:right;\"> 3.839 </td>\n   <td style=\"text-align:right;\"> 19.306 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> P3p </td>\n   <td style=\"text-align:right;\"> 74.011 </td>\n   <td style=\"text-align:right;\"> 3.345 </td>\n   <td style=\"text-align:right;\"> 22.126 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> FTp </td>\n   <td style=\"text-align:right;\"> 15.469 </td>\n   <td style=\"text-align:right;\"> 2.027 </td>\n   <td style=\"text-align:right;\"> 7.633 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> OREB </td>\n   <td style=\"text-align:right;\"> 0.719 </td>\n   <td style=\"text-align:right;\"> 0.062 </td>\n   <td style=\"text-align:right;\"> 11.538 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> DREB </td>\n   <td style=\"text-align:right;\"> 1.203 </td>\n   <td style=\"text-align:right;\"> 0.037 </td>\n   <td style=\"text-align:right;\"> 32.809 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AST </td>\n   <td style=\"text-align:right;\"> -0.035 </td>\n   <td style=\"text-align:right;\"> 0.048 </td>\n   <td style=\"text-align:right;\"> -0.729 </td>\n   <td style=\"text-align:right;\"> 0.466 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> STL </td>\n   <td style=\"text-align:right;\"> 1.069 </td>\n   <td style=\"text-align:right;\"> 0.068 </td>\n   <td style=\"text-align:right;\"> 15.785 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BLK </td>\n   <td style=\"text-align:right;\"> 0.350 </td>\n   <td style=\"text-align:right;\"> 0.078 </td>\n   <td style=\"text-align:right;\"> 4.470 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n## Variance Explained (Review from Chapter 6)\n$R^2$ is the proportion of variance in $y$ explained by $x$ .\n\n$R^2=\\frac{SS~regression}{SS~Total}$\n\n-   $R^2_{adj}$ is adjusted to remove the variation that is explained by\n    chance alone\n\n-   $R^2_{adj}=1-\\frac{MS~Error}{MS~Total}$\\\n$R^2_{adj}$ can also be written as: $R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}$\n\n-   Now we have more sums of square to add in to our regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSS <- anova(full_reg) |> \n   tidy() |> \n  select(term:sumsq) |> \n  janitor::adorn_totals()\nSS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      term   df    sumsq\n       PTS    1 141194.6\n       P2p    1   4001.7\n       P3p    1  14465.4\n       FTp    1    583.8\n      OREB    1   7082.2\n      DREB    1  78359.3\n       AST    1     10.1\n       STL    1  20082.0\n       BLK    1   1542.2\n Residuals 2108 162702.8\n     Total 2117 430024.0\n```\n:::\n:::\n\n\n## Additional variation explained\n\n-   Variation in $Y$ is separated into two parts SSR and SSE.\n\n    -   The shaded overlap of two circles represent the variation in $Y$\n        explained by the $X$ variables.\n\n-   The total overlap of $X_1$ and $X_2$, and $Y$ depends on\n\n    -   relationship of $Y$ with $X_1$ and $X_2$\n    -   correlation between $X_1$ and $X_2$\n\n    ![](img/5_5.png)\n\n\n\n## Sequential addition of predictors\n\n-   Addition of variables decreases SSE and increases SSR and $R^2$.\n-   $s^2$ = MSE = SSE/df decreases to a minimum and then increases since\n    addition of variable decreases SSE but adds to df.\n\n![](img/5_7.png)\n\n## Significance of Type I or Seq.SS\n\n-   The Type I SS is the SS of a predictor after adjusting for the\n    effects of the *preceding* predictors in the model. \n    \n    -   Sometimes order matters, particularly with unequal sample sizes\n- For unbalanced data, this approach tests for a difference in the weighted marginal means. In practical terms, this means that the results are dependent on the realized sample sizes. In other words, it is testing the first factor without controlling for the other factor, which may not be the hypothesis of interest. \n\n-   F test for the significance of the additional variation explained\n-   R function `anova()` calculates sequential or Type-I SS\n\n![](img/type1.png)\n\n## Type II \n\n-   Type II SS is based on the principle of marginality.\n\n    -   Each variable effect is adjusted for all other appropriate\n        effects.\n        -   equivalent to the Type I SS when the variable is the last\n            predictor entered the model.\n    -   Order matters for Type I SS but not for Type II SS\n\n![](img/type2.png)\n\n## Type III SS\n-   Type III SS is the SS added to the regression SS after *ALL* other\n    predictors including an interaction term.\n    \n-   This type tests for the presence of a main effect after other main effects and interaction. This approach is therefore valid in the presence of significant interactions.\n\n-   If the interaction is significant SS for main effects should not be interpreted\n    \n## SS types in action\n\nConsider a model with terms A and B:\n\nType 1 SS:\\\nSS(A) for factor A.\\\nSS(B | A) for factor B.\n\nType 2 SS:\\\nSS(A | B) for factor A.\\\nSS(B | A) for factor B.\n\nType 3 SS:\\\nSS(A | B, AB) for factor A.\\\nSS(B | A, AB) for factor B.\n\n-   When data are balanced and the design is simple, types I, II, and III will give the same results.  \n\n-   SS explained is not always a good criterion for selection of variables\n\n## SS types in action\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(full_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: Spread\n            Df Sum Sq Mean Sq F value  Pr(>F)    \nPTS          1 141195  141195 1829.34 < 2e-16 ***\nP2p          1   4002    4002   51.85 8.3e-13 ***\nP3p          1  14465   14465  187.42 < 2e-16 ***\nFTp          1    584     584    7.56   0.006 ** \nOREB         1   7082    7082   91.76 < 2e-16 ***\nDREB         1  78359   78359 1015.23 < 2e-16 ***\nAST          1     10      10    0.13   0.718    \nSTL          1  20082   20082  260.18 < 2e-16 ***\nBLK          1   1542    1542   19.98 8.2e-06 ***\nResiduals 2108 162703      77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(car)\nAnova(full_reg,  type=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnova Table (Type II tests)\n\nResponse: Spread\n          Sum Sq   Df F value  Pr(>F)    \nPTS          416    1    5.38    0.02 *  \nP2p        28767    1  372.71 < 2e-16 ***\nP3p        37787    1  489.57 < 2e-16 ***\nFTp         4497    1   58.26 3.4e-14 ***\nOREB       10276    1  133.13 < 2e-16 ***\nDREB       83083    1 1076.43 < 2e-16 ***\nAST           41    1    0.53    0.47    \nSTL        19232    1  249.17 < 2e-16 ***\nBLK         1542    1   19.98 8.2e-06 ***\nResiduals 162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nAnova(full_reg,  type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnova Table (Type III tests)\n\nResponse: Spread\n            Sum Sq   Df F value  Pr(>F)    \n(Intercept) 189696    1 2457.72 < 2e-16 ***\nPTS            416    1    5.38    0.02 *  \nP2p          28767    1  372.71 < 2e-16 ***\nP3p          37787    1  489.57 < 2e-16 ***\nFTp           4497    1   58.26 3.4e-14 ***\nOREB         10276    1  133.13 < 2e-16 ***\nDREB         83083    1 1076.43 < 2e-16 ***\nAST             41    1    0.53    0.47    \nSTL          19232    1  249.17 < 2e-16 ***\nBLK           1542    1   19.98 8.2e-06 ***\nResiduals   162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Summary so far\n-   Defined multiple regression\n-   Why we might run a multiple regression\n-   How to perform a multiple regression, examine residuals, multicollinearity\n-   Variation and R squared\n-   Sums of Squares types\n\n    \n## Learning Objectives:\n\n-   Cautionary tales: when correlation misleads us \n-   How to make models\n-   Comparing models\n-   Principles of model selection\n\n\n\n## Does Waffle Houses cause divorce??\n\n![](img/waffleH2divorce.png)\n\n## Or is it butter?\n\n![](img/Divorce_margarine.png)\n\nAnd if you want more to impress your friends at a BBQ, the source is: http://www.tylervigen.com/spurious-correlations\n\n## Spurious association\n-   You've heard that before: correlation does not imply causation. BUT it doesn't discard it either\n-   Hope you are seated: causation does not imply correlation.\n-   Causation implies conditional correlation (up to linearity issues).\n-   We need more than just statistical models to answer causal questions.\n\n## How do we deal with spurious associations?\n-   Domain knowledge: you know that waffles and butter don't *cause* divorce so why might they be correlated?\n\n    -   Is there another predictor that would be better?\n-   Multiple regressions can disentangle the association between two predictors and an outcome \n\n    -   Statistical control of a confound\n  \n![](img/waffleH2divorce.png)\n\n## Masked associations\n\n-   Association between a predictor variable and an outcome can be\nmasked by another variable.\n-   You need to observe both variables to see the “true” influence of either on the\noutcome.\n-   How do we account for the masking variable (seen as a nuisance)?\n\n## Masking situations tend to arise when:\n\n- Both predictors are associated with one another.\n- Have opposite relationships with the outcome.\n\n![](img/venetian-mask.jpg)\n\n\n## Headline: Higher ice cream sales increase shark attacks!\n\nWe’ll predict ice cream sales from the temperature and the number of shark attacks\n\n![](img/ice-cream-shark.jpeg)\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## First we will consider simple regressions \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Single regression\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Temperature Only</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.064 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> temp </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n   <td style=\"text-align:right;\"> 0.064 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Shark Only</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> 0.083 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shark </td>\n   <td style=\"text-align:right;\"> 0.557 </td>\n   <td style=\"text-align:right;\"> 0.084 </td>\n   <td style=\"text-align:right;\"> 6.64 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Multiple regression\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Temperature Only</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.064 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> temp </td>\n   <td style=\"text-align:right;\"> 0.77 </td>\n   <td style=\"text-align:right;\"> 0.064 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Shark Only</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> 0.083 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shark </td>\n   <td style=\"text-align:right;\"> 0.557 </td>\n   <td style=\"text-align:right;\"> 0.084 </td>\n   <td style=\"text-align:right;\"> 6.64 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Temperature and Sharks</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> 0.064 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> temp </td>\n   <td style=\"text-align:right;\"> 0.776 </td>\n   <td style=\"text-align:right;\"> 0.094 </td>\n   <td style=\"text-align:right;\"> 8.217 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shark </td>\n   <td style=\"text-align:right;\"> -0.008 </td>\n   <td style=\"text-align:right;\"> 0.094 </td>\n   <td style=\"text-align:right;\"> -0.084 </td>\n   <td style=\"text-align:right;\"> 0.933 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n## Masking situation\n\nTend to arise when \n\n- Both predictors are associated with one another.\n- Have different relationships with the outcome\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/icecream pairs-1.png){fig-align='center' width=95% height=50%}\n:::\n:::\n\n\n## How do we deal with this?\n\n-   Statistically there is not really an answer\n-   The answer lies in the causes and the causes are not in the data; Shark attacks dont cause ice cream sales or vice versa\n-   Remember that interpreting the (regression) parameter estimates\nalways depends upon what you believe the causal model\n\n\n## Model Selection \n\n- The first step before selection of the best subset of predictors is to study the correlation matrix\n- We then perform stepwise additions (forward) or subtractions (backward) from the model and compare them\n\nBUT...\n\n- We saw with the illustration of SS how the significance or otherwise of a variable in a multiple regression model depends on the other variables in the model\n- Therefore, we cannot fully rely on the t-test and discard a variable because its coefficient is insignificant\n\n## Selection of predictors\n\n-   Heuristic (short-cut) procedures based on criteria such as $F$,\n    $R^2_{adj}$, $AIC$, $C_p$ etc\n    -   `Forward Selection`: Add variables sequentially\n\n        -   convenient to obtain the simplest feasible model\n\n    -   `Backward Elimination`: Drop variables sequentially\n\n        -   If difference between two variables is significant but not\n            the variables themselves, forward regression would obtain\n            the wrong model since both may not enter the model.\n\n            -   Known as *suppressor* variables case (like masking variables discussed earlier)\n\nExample: (try)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1691 -0.6791 -0.0033  0.6441  1.1299 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.98876    1.26689    9.46  3.4e-07 ***\nx1           0.00375    0.41608    0.01     0.99    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.832 on 13 degrees of freedom\nMultiple R-squared:  6.24e-06,\tAdjusted R-squared:  -0.0769 \nF-statistic: 8.11e-05 on 1 and 13 DF,  p-value: 0.993\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x2, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0900 -0.6334  0.0002  0.6146  1.0403 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   10.632      0.811   13.11  7.2e-09 ***\nx2             0.195      0.113    1.74     0.11    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.75 on 13 degrees of freedom\nMultiple R-squared:  0.188,\tAdjusted R-squared:  0.126 \nF-statistic: 3.02 on 1 and 13 DF,  p-value: 0.106\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = suppressor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.01363 -0.00945 -0.00228  0.00863  0.01632 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.51541    0.06114   -73.8   <2e-16 ***\nx1           3.09701    0.01227   252.3   <2e-16 ***\nx2           1.03186    0.00368   280.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0107 on 12 degrees of freedom\nMultiple R-squared:     1,\tAdjusted R-squared:     1 \nF-statistic: 3.92e+04 on 2 and 12 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n## Ockham's razor\n*Nunca ponenda est pluralitas sine necesitate* \\\n(Plurality should never be posited without necessity)\n\n-   Problem: fit to sample always (*multi-level models can be counter-examples) improves as we add parameters.\n-   Dangers of \"stargazing\": selecting variables with low p-values (aka 'lots of stars'). P-values are not designed to cope with over-/under-fitting.\n\n\n## Overfitting\n-   Overfitting learning too much from the data, where you are almost just connecting the points rather than estimating.\n-   Underfitting is the opposite, i.e. being insensitive to the data.\n-   aka Models with fewer assumptions are to be preferred.\n-   In practice, we have to choose between models that differ both in\naccuracy and simplicity. The razor is not really a useful guidance for\nthis trade off. \n\nWe need tools\n\n## All possible models:\nAn exhaustive screening of all possible regression models can also be done using software\n\n-   `Best Subsets`: Stop at each step and check whether predictors, in\n    the model or outside, are the best combination for that step.\\\n    -   time consuming to perform when the predictor set is large\n    \nRemember permutations\n\nFor example:\n\nIf we fix the number of predictors as 3, then 20 regression models are possible\\\n\n\n## What criteria do we use?\nMultiple options\\\n- R squared\\\n- Sums of squared (different types, for testing predictor significance)\\\n- Information criteria (AIC, BIC etc)\\\n- For prediction: MSD/MSE, MAD, MAPE\\\n- $C_p$\\\n\nRemember its about balance and what you are looking for (fit vs prediction, complexity vs generality)\\\n\n-   **`Note`**\n\n    -   If a model stands out, it will perform well in terms of all\n        summary measures.\n    -   If a model does not stand out, summary measures will contradict.\n\n\n## When $R^2$ becomes absurd\n\n![](img/brain_homin_R2ter.png)\n\n## Model selection\n\n-   Residual SD depends on its degrees of freedom\n\n    -   So comparison of models based on Residual SD is not fully fair\n\n-  The following three measures are popular prediction modelling and similar to residual SD \n\n-   Mean Squared Deviation (MSD): mean of the squared errors (i.e., deviations) (also called MSE)\n\n$$\\frac{\\sum \\left({\\rm observation-fit}\\right)^{{\\rm 2}} }{{\\rm number~of~ observations}}$$\n\n-   Mean Absolute Deviation (MAD)\n\n$$\\frac{\\sum \\left|{\\rm observation-fit}\\right| }{{\\rm number~of~observations}}$$\n\n-   Mean Absolute Percentage Error (MAPE)\n\n$$\\frac{\\sum \\frac{\\left|{\\rm observation-fit}\\right|}{{\\rm observation}} }{{\\rm number~of~observations}} {\\times100}$$\n\n## Model selection (continued)\n\n-   Avoid over-fitting.\n\n-   So place a penalty for excessive model parameters\n\n-   Akaike Information Criterion (AIC; *smaller is better*)\n\n$$AIC  =  n\\log \\left(\\frac{SSE}{n} \\right) + 2p$$\n-   Bayesian Information Criterion (BIC) places a higher penalty that depends on, the number of observations. \n\n-   As a result BIC fares well for selecting a model that explains the relationships well while AIC fares well when selecting a model for prediction purposes.\n\n- Other variations: WAIC, AICc, etc (we will not cover them)\n\n\n## Software\n\n-   In $R$, `lm()` and `step()` function will perform the tasks\n\n    -   `leaps()` and `HH` packages contain additional functions\n    -   `dredge()` in `MuMIn` will produce all the subset models given a full model\n    -   Also `MASS`, `car`, `caret`, and `SignifReg` R packages\n\n-   R base package step-wise selection is based on $AIC$ only.\n\n\n\n\n## Cross validation (review from Chapter 6)\nIn sample error vs prediction error\n\n-   For simpler models, increasing the number of parameters improves the fit to the sample.\n-   But it seems to reduce the accuracy of the out-of-sample predictions.\n-   Most accurate models trade off flexibility (complexity) and overfitting\n\nGeneral idea:\n-   Leave out some observations.\n-   Train the model on the remaining samples; score on those left out.\n-   Average over many left-out sets to get the out-of-sample (future) accuracy.\n\n## Cross validated selection: Data example\nConsider the pinetree data set which contains the circumference measurements of pine trees at four positions (First is bottom)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Cross validated selection\n-   Model selection can be done focusing on prediction\n    -   method = \"leapForward\" & method = \"leapBackward\" options\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(caret);  library(leaps)\nset.seed(123)\nfitControl  <-  trainControl(method  =  \"repeatedcv\",\n                             number  =  5,  repeats  =  100)\nleapBackwardfit  <-  train(Top  ~  .,  data  =  pinetree[, -1],\ntrControl  =  fitControl,  method  =  \"leapBackward\")\nsummary(leapBackwardfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubset selection object\n3 Variables  (and intercept)\n       Forced in Forced out\nThird      FALSE      FALSE\nSecond     FALSE      FALSE\nFirst      FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         Third Second First\n1  ( 1 ) \" \"   \" \"    \"*\"  \n2  ( 1 ) \"*\"   \" \"    \"*\"  \n```\n:::\n:::\n\n\n## Polynomial models\n\n-   A polynomial model includes the square, cube of predictor variables\n    as additional variables.\n-   High correlation (multicollinearity) between the predictor variables\n    may be a problem in polynomial models, but not always.\n\n## Polynomial models: Data example\n\nWe can fit a simple linear regression using the Pine tree data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npine1 <- lm(Top ~ First, data = pinetree) \nsummary(pine1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Top ~ First, data = pinetree)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.854 -0.881 -0.195  0.630  3.176 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -6.334      0.765   -8.28  2.1e-11 ***\nFirst          0.763      0.024   31.78  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.29 on 58 degrees of freedom\nMultiple R-squared:  0.946,\tAdjusted R-squared:  0.945 \nF-statistic: 1.01e+03 on 1 and 58 DF,  p-value: <2e-16\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Look closer \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nLooks non-linear\n\n\n## Polynomial models: Data example\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                                  Estimate Std. Error t value Pr(>|t|)\n(Intercept)                         44.121      7.039    6.27        0\npoly(First, degree = 3, raw = T)1   -3.972      0.695   -5.71        0\npoly(First, degree = 3, raw = T)2    0.142      0.022    6.39        0\npoly(First, degree = 3, raw = T)3   -0.001      0.000   -5.95        0\n```\n:::\n:::\n\n\n\n    ```         \n    - For the pinetree example, all the slope coefficients are highly significant for the cubic regression\n    - Not so for the quadratic regression\n    ```\n\n\\tiny\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                                  Estimate Std. Error t value Pr(>|t|)\n(Intercept)                           3.85      2.450   1.569    0.122\npoly(First, degree = 2, raw = T)1     0.10      0.155   0.646    0.521\npoly(First, degree = 2, raw = T)2     0.01      0.002   4.319    0.000\n```\n:::\n:::\n\n\n-   Raw polynomials do not preserve the coefficient estimates but\n    orthogonal polynomials do.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                 17.40      0.146  119.26        0\npoly(First, degree = 2)1    41.01      1.130   36.29        0\npoly(First, degree = 2)2     4.88      1.130    4.32        0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                 17.40      0.115  151.03        0\npoly(First, degree = 3)1    41.01      0.892   45.96        0\npoly(First, degree = 3)2     4.88      0.892    5.47        0\npoly(First, degree = 3)3    -5.31      0.892   -5.95        0\n```\n:::\n:::\n\n\n## Residual diagnostics\n\n-   For multiple regression fits, including polynomial fits, examine the\n    residuals as usual to-\n\n    ```         \n    - Validate the model assumptions\n    - Look for model improvement clues\n    ```\n\n-   Quadratic regression for pinetree data is not satisfactory based on\n    the residual plots shown below:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-27-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Categorical predictors\n\n-   Models can include categorical predictors such as **Area** in the\n    pinetree dataset\n\n-   Make sure that you use the *factor()* function when numerical codes\n    are assigned to categorical variables.\n\n-   Area effect on Top circumference is clear from the following plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter07_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Indicator variables\n\n-   Factors are employed in a multiple regression using indicator\n    variables which are simply binary variables taking either zero or\n    one\n\n-   For for males and females, indicator variables are defined as\n    follows:\n\n    -   Indicator variable of males:\n        $~~~~~~~~\\begin{array}{cccc} I_{\\text {male}} & = & 1 & \\text{for males}\\\\ & & 0& \\text{for females} \\end{array}$\n    -   Indicator variable of females\n        $~~~~~~~~\\begin{array}{cccc} I_{\\text{female}} & = & 1 & \\text{for females}\\\\ & & 0& \\text{for males} \\end{array}$\n\n-   There are three different areas of the forest in the pinetree\n    dataset. So we can define three indicator variables.\n\n-   Only two indicator variables are needed because there is only 2\n    degrees of freedom for the 3 areas.\n\n## Regression output\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Regression of Top Circumference on Area Indicator Variables</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 20.02 </td>\n   <td style=\"text-align:right;\"> 1.11 </td>\n   <td style=\"text-align:right;\"> 17.98 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> I2 </td>\n   <td style=\"text-align:right;\"> -1.96 </td>\n   <td style=\"text-align:right;\"> 1.57 </td>\n   <td style=\"text-align:right;\"> -1.24 </td>\n   <td style=\"text-align:right;\"> 0.22 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> I3 </td>\n   <td style=\"text-align:right;\"> -5.92 </td>\n   <td style=\"text-align:right;\"> 1.57 </td>\n   <td style=\"text-align:right;\"> -3.76 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n-   The y-intercept is the mean of the response for the omitted category\n    -   `20.02` is the mean Top circumference for the first Area\n-   slopes are the difference in the mean response\n    -   `-1.96` is the drop in the mean top circumference in Area 2 when\n        compared to Area 1 (which is not a significant drop)\n    -   `-5.92` is the drop in the mean top circumference in Area 3 when\n        compared to Area 1 (which is a highly significant drop)\n\nAnalysis of Covariance model employs both numerical and categorical\npredictors (covered later on).\n\n-   We specifically include the interaction between them\n\n\n## Summary\n\n-   Regression methods aim to fit a model by least squares to explain\n    the variation in the dependent variable $Y$ by fitting explanatory\n    $X$ variables.\n-   Matrix plots (EDA) and correlation coefficients provide important clues to\n    the interrelationships.\n-   For building a model, the additional variation explained is\n    important. Summary criterion such as $AIC$ is also useful.\n\n-   A model is not judged as the **best** purely on statistical grounds.\n\n\n<!-- download.file(\"http://www.massey.ac.nz/~kgovinda/220exer/Chap5moreexamples.R\", destfile=\"Chap5moreexamples.R\") -->\n\n<!-- download.file(\"https://www.massey.ac.nz/~kgovinda/220exer/chapter-5-exercises.html\", destfile=\"chapter-5-exercises.html\") -->\n\n<!-- install.packages(\"remotes\") -->\n\n<!-- remotes::install_github(\"ricompute/ricomisc\") -->\n\n<!-- ricomisc::rstudio_viewer(\"chapter-4-exercises.html\", file_path = NULL) -->\n",
    "supporting": [
      "Chapter07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}