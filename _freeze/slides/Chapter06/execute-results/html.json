{
  "hash": "3151f73b14b9a11ebb6471ba0ab45155",
  "result": {
    "markdown": "---\ntitle: \"Chapter 6:<br>Models with a Single Predictor\"\nimage: img/reg.png\nformat: \n  revealjs:\n    width: 1050\n    height:\t700\n    scrollable: true\n    transition: fade\n    theme: [default, myquarto.scss]\n    slide-number: c/t  \n    logo: img/L_Color.png\n    footer: \"[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)\"\n    styles:\n      - revealjs.dark:\n        background-color: #222\n        color: #fff\n    \nexecute:\n  echo: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n:::\n\n\n## What is a statistical model?\n\n::: {align=\"center\"}\n**OBSERVATION = FIT + RESIDUAL**\n:::\n\n**FIT**- to explain systematic (non-random) variation in the data\n\n**RESIDUAL** - to explain random variation in the data\n\n## Analysis of two quantitiative variables\n\n-   With paired (related) data (X,Y)\n-   Two variables: one (Y) is random (response) and the other (X) is considered fixed (predictor)\n-   Interested in the functional relationship between these two variables $Y=f(X)$ to predict Y, given X \n-   Interested in estimates of coefficients \n    \n      \n## Regression\n\n-   Statistical Model\n    -   Fitted Model: $\\hat{Y}=a+bX$ (True Model:\n        $Y=\\alpha+\\beta X+\\epsilon$)\n    -   residual error: $e= Y-\\hat{Y}$ ($\\epsilon$ is not the same as\n        $e$)\n\n        \n## Regression\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n       \n-   A **regression equation** is a function that indicates how the\n    **average** value of one response variable for given values of one\n    or more predictor variables varies with these predictor variables;\n    that is, $E(Y|X_1, X_2, ..., X_k)$\n\n## Simple regression\n\n-   The term simple means there is a single predictor\n\n-   The fitted model remains as: $\\hat{Y}=a+bX_{i}$\\\n\n    - $\\hat{Y}$ is the predicted value of $y_{i}$ given $x_{i}$. Also called 'fitted' values, they are the linear funtion of X.\n    - $a$ the intercept on the y-axis; value of $\\hat{Y}$ when $x_{i}=0$.\n    - $b$ the slope of the line; the change in $\\hat{Y}$ with a 1-unit increase in $x_{i}$.\n\n\n## Fitting a regression\n\n-   Want a line that is as 'close' as possible to the existing data points we have\n\n\n-   The method of least squares is employed to obtain the estimates $a$\n    and $b$\n\n    -   The sum of squared residuals is minimized in the least squares\n        method.\n  \n\n## Example (Alcohol consumption data)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n country Alcohol Death\n       1    22.9  30.0\n       2    15.2  23.6\n       3    12.3  18.9\n       4    11.9   5.0\n       5    10.8  12.3\n       6     9.9  14.2\n       7     8.3   7.4\n       8     7.2   3.0\n       9     6.6   7.2\n      10     5.8  10.6\n      11     5.7   3.7\n      12     5.6   3.4\n      13     4.2   4.3\n      14     3.9   3.6\n      15     3.1   5.4\n```\n:::\n:::\n\n\n## Plot of Alcohol consumption data\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Plot of Alcohol consumption data\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n$min \\left( \\sum_{i=1}(y_{i}-\\hat{y_{i}})^2 \\right)$\n\n-   $i$ number of observations\n-   $y_{i}$ observed value of y\n-   $\\hat{y_{i}}$ predicted value of y\n\n\n## R Base Output \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod1 <- lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,\tAdjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n```\n:::\n:::\n\n\n-   Let us `tidy` it.\n\n## Fitted model and testing its coefficients\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nlibrary(broom)\ntidy(mod1) |> mutate_if(is.numeric, round, 3) -> out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %>% \n  kable_classic(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>t-tests for model parameters</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -2.318 </td>\n   <td style=\"text-align:right;\"> 2.065 </td>\n   <td style=\"text-align:right;\"> -1.123 </td>\n   <td style=\"text-align:right;\"> 0.282 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alcohol </td>\n   <td style=\"text-align:right;\"> 1.405 </td>\n   <td style=\"text-align:right;\"> 0.202 </td>\n   <td style=\"text-align:right;\"> 6.954 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n-   Focus on the model, its coefficients\n\n    -   What is the scale of these variables? Are these coefficient estimates meaningful in the context?\n    -   What is the error around these estimates?\n\n\n## Assumptions\n\nModel forming assumptions\n\n1.  $X$ is known without error\n2.  $Y$ is linearly related to $X$ \n3.  There is a random variability of $Y$ about this line\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Assumptions\n**More** assumptions to form $t$ and $F$ statistics:\n\n1.  Variability in $Y$ about the line is constant and independent of $X$ variable.\n2.  The variability of $Y$ about the line follows normal distribution.\n3.  The distribution of $Y$ given $X = X_i$ is independent of $Y$ given\n    $X = X_j$.\n\n## Residuals and assumptions\n-   Most assumptions are on the errors (residuals)\n    -   independent\n    -   normal\n    -   random\n-   Is there a pattern in my residuals? \n-   Do they suggest what to do?\n\n## Residuals\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Types of residuals\n\n-   raw or ordinary residual is just (observation-fit)\n\n-   `Standardized Residual`= Residual/Std.Dev of residual\n\n-   The regression model is influenced by outliers or unusual points\n    because the slope estimate of the regression line is sensitive to\n    these outliers.\n\n-   So we define `Studentised or deleted t Residual`\n\n-   Similar to Standardized Residual without the observation under\n    consideration. That is,\n\n    -   Studentised residual = residual/std. dev of residual (after\n        omitting the particular observation).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Residual plot for `Alcohol~deaths` model\n\n-   For small sample sizes, residual diagnostics is difficult\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Residual plot for `Alcohol~deaths` model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow=(c(2,2)))\nplot(mod1)\n```\n\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Residuals showing need for transformation\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Non-constant Residual Variation](Chapter06_files/figure-revealjs/resipattern1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Adding more predictors\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/resipattern2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Subgrouping patterns\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/resipattern3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Outliers\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/resipattern4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Autocorrelation\n\n-   Neighbouring residuals depend on each other\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/resipattern5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Improving simple regression\n\n-   Use a different predictor or explanatory variable\n-   Transform the $Y$ variable\n-   Add other explanatory variables to the model (next week)\n-   Deletion of invalid (as opposed to outlier) observations\n-   Reconsider the linear relationship\n\n\n## Tests of significance\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nlibrary(broom)\ntidy(mod1) |> mutate_if(is.numeric, round, 3) -> out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %>% \n  kable_classic(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>t-tests for model parameters</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -2.318 </td>\n   <td style=\"text-align:right;\"> 2.065 </td>\n   <td style=\"text-align:right;\"> -1.123 </td>\n   <td style=\"text-align:right;\"> 0.282 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alcohol </td>\n   <td style=\"text-align:right;\"> 1.405 </td>\n   <td style=\"text-align:right;\"> 0.202 </td>\n   <td style=\"text-align:right;\"> 6.954 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n    -   $t$ tests of intercept and slope\n\n    -   $H_0=true~slope=0$; $H_0=true~intercept=0$\n\n        -   For `Alcohol~deaths` model, the slope is significant\n            but not the intercept\n\n\n## Model quality measures\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,\tAdjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n```\n:::\n:::\n\n-   Model summary (or Quality) measures\n\n    -   $R^2$ is the proportion of variation explained by the fitted\n        model\n\n        -   A meaningful model must have at least 50% $R^2$\n        -   A large $R^2$ is important to explain the relationship(s)\n\n    -   Residual standard deviation (error) $S$ has to be small\n\n        -   How small? Difficult to say. Compare $S$ with the overall\n            spread in $Y$ or with the mean of $Y$\n        -   A small $S$ is important for prediction\n\n## Model quality measures\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nout1 <- glance(mod1) |> \n  select(r.squared, sigma, statistic, p.value) |> \n  mutate_if(is.numeric, round, 2)\n\nout1 |> t() |> \n  kable(caption = \"Model summary measures\") |> \n  kable_classic(full_width = T) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; margin-left: auto; margin-right: auto;'>\n<caption>Model summary measures</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\">  </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> r.squared </td>\n   <td style=\"text-align:right;\"> 0.79 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sigma </td>\n   <td style=\"text-align:right;\"> 3.94 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> statistic </td>\n   <td style=\"text-align:right;\"> 48.35 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p.value </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Variance Explained \n$R^2$ is the proportion of variance in $y$ explained by $x$ .\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,\tAdjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n```\n:::\n:::\n\n\n## Variance Explained \n\n$R^2=\\frac{SS~regression}{SS~Total}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSS <- anova(ols) |> \n   tidy() |> \n  select(term:sumsq) |> \n  janitor::adorn_totals()\nSS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      term df    sumsq\n   Alcohol  1 751.4408\n Residuals 13 202.0286\n     Total 14 953.4693\n```\n:::\n:::\n\n\n-   $R^2_{adj}$ is adjusted to remove the variation that is explained by\n    chance alone\n\n-   $R^2_{adj}=1-\\frac{MS~Error}{MS~Total}$\\\nwhere:\\\n$MS~Error$ = $frac{SSE}{n-k-1}$\\\n$MS~Total$ = $frac{SST}{n-1}$\\\ntherefore $R^2_{adj}$ can also be written as: $R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}$\n\n## ANOVA $F$-test\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nmod1  <-  lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,\tAdjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nanova(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: Death\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nAlcohol    1 751.44  751.44  48.353 1.001e-05 ***\nResiduals 13 202.03   15.54                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nmod1.aov  <-  aov(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1.aov) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)    \nAlcohol      1  751.4   751.4   48.35  1e-05 ***\nResiduals   13  202.0    15.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n-   For the straight line model, the F-test is equivalent to testing the\n    hypothesis that the true slope is zero.\n\n\n## ANOVA $F$-test\n    -   Significant F ratio need not always imply that the\n        straight line is the best fit to the data.\n    -   For `Alcohol~deaths` model, the $F$ statistic is \n        significant which means that the fitted model explains significant variation\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nout1 <- anova(mod1) |> tidy() |> mutate_if(is.numeric, round, 2)\n\noptions(knitr.kable.NA = \" \")\n\nkable(out1, caption = \"ANOVA table\") |> \n  kable_classic(full_width = F) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>ANOVA table</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> sumsq </th>\n   <th style=\"text-align:right;\"> meansq </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Alcohol </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 751.44 </td>\n   <td style=\"text-align:right;\"> 751.44 </td>\n   <td style=\"text-align:right;\"> 48.35 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 202.03 </td>\n   <td style=\"text-align:right;\"> 15.54 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## ANOVA table construction\n\n-   Each source has an associated degrees of freedom.\n\n-   For regression, DF = $1$ as there are two parameters $a$ and $b$\n    fitted in the model\n\n-   For total, DF = $n-1$ since there are n observations\n\n-   For error, DF = by subtraction = $(n-1)-1 = n-2$\n\n-   Mean Square (MS) values are obtained as MS = SS/DF\n\n-   F ratio for regression =MS(Regression)/MS(Error)\n\n-   F ratio follows the $F$ distribution with $(1, n-2)$ d.f and\n    provides the significance of the model fitted.\n\n    -   the ratio of two sample variances (or MS) follows the $F$\n        distribution (normal case)\n\n## Prediction\n\n-   The predicted response at value $x=x_0$ is obtained using the fitted\n    regression equation.\n-   Confidence & prediction intervals can also be constructed.\n-   Note that prediction intervals are for individual observations\n    whereas the confidence intervals are for the expected (mean)\n    response for a given $x_0$\n\n\\scriptsize\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(mod1, new = data.frame(Alcohol=10), interval=\"confidence\", level =0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 11.72778 9.476416 13.97915\n```\n:::\n\n```{.r .cell-code}\npredict(mod1, new = data.frame(Alcohol=10), interval=\"prediction\", level =0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 11.72778 2.918701 20.53686\n```\n:::\n:::\n\n\n## Outlier effect on regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Scatter plot of People vs Vehicle](Chapter06_files/figure-revealjs/rangiHi-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Leverage and Cook's distance\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n-   A distant $x$ value has a higher ***leverage***.\n\n    -   This leverage is often measured by the $h_{ii}$ or `hi` value\n    -   Check $h_{ii}$ \\> $\\frac{3p}{n}$ or not\n\n-   Influence of a point on the regression is measured using the\n    **Cook's distance** $D_i$\n\n    -   related to difference between the regression coefficients with\n        and without the $i^{th}$ data point.\n    -   $D_{i} >0.7$ can be deemed as being influential (for $n>15$)\n\n## Leverage and Cook's distance\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Robust regression models\n\nRobust regression methods are designed to limit the effect that violations of assumptions by the underlying data-generating process have on regression estimates.\n\nFor example, least squares estimates for regression models are highly sensitive to outliers: an outlier with twice the error magnitude of a typical observation contributes four (two squared) times as much to the squared error loss, and therefore has more leverage over the regression estimates.\n\n## Robust models\n\nWe can fit a robust linear model using the functions `MASS::rlm()`\n`robustbase::lmrob()`.\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n<!-- ```{r, echo=TRUE, results='hide'} -->\n\n<!-- summary(rlm(y~x)) -->\n\n<!-- summary(lmrob(y~x)) -->\n\n<!-- ``` -->\n## Robust Models\n\nTo determine if a robust regression model offers a better fit to the data compared to the OLS model, we can calculate the residual standard error of each model.\n\nThe residual standard error (RSE) is a way to measure the standard deviation of the residuals in a regression model. The lower the value for RSE, the more closely a model is able to fit the data.\n\n## Robust Models\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#fit least squares regression model\nols <- lm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of ols model\nsummary(ols)$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.942164\n```\n:::\n\n```{.r .cell-code}\n#fit robust regression model\nrobust <- rlm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of robust model\nsummary(robust)$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.417522\n```\n:::\n:::\n\n\n\n## Cross Validation (CV) \n\n-   A tool to avoid overfitting (more important with multiple predictors)\n-   Evaluate performance of model\n\n## Cross Validation (CV)\n-   Split the sample data randomly into (equal) k subsets (parts) by\n    resampling.\n\n-   Fit the model for $(k − 1$) subsets of the data\n\n-   Predict for the omitted subsets\n\n-   Compare prediction errors\n\n-   Repeat for each subsets of the data\n\n## Cross Validation Types\n\n-   K-fold\n-   Stratified K-fold \n\n    - Same as K-fold but splitting data governed by criteria so that each subset has the same proportion of obervations of a given categorical value\n    \n-   Leave One Out (LOO)\n\n    - Leaves out a single data point, then uses the same process as K-fold CV\n\n-   R has many packages for cross validation\n\n## Example (Alcohol consumption data)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# CV using Root Mean Sq Error as measure of prediction error\nlibrary(caret);  library(MASS,  exclude  =  \"select\")\nset.seed(123)\n\nfitControl <- trainControl(method = \"repeatedcv\", number = 5, repeats = 100)\nlmfit <- train(Death ~Alcohol,  data= cirrhosis, \n                 trControl = fitControl, method=\"lm\")\nlm.rmses <- lmfit$resample[,1]\nrlmfit <- train(Death ~Alcohol,  data = cirrhosis, \n                  trControl=fitControl, method = \"rlm\")\nrlm.rmses <- rlmfit$resample[,1]\ndfm  <-  cbind.data.frame(lm.rmses,rlm.rmses)\nlibrary(patchwork)\nqplot(data=dfm,  lm.rmses,  geom=\"boxplot\")  /\nqplot(data=dfm,  rlm.rmses,  geom=\"boxplot\")\n```\n\n::: {.cell-output-display}\n![](Chapter06_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Choosing the `best` model\n\n-   The `best` model is not decided purely on statistical grounds.\n-   If the main aim is to describe relationships, include all the\n    relevant variables.\n-   If the main aim is to predict, prefer the simplest feasible\n    (parsimonious) model with smaller number of predictors.\\\n-   Examine the literature to discover similar examples, see how they\n    are tackled, discuss the matter with the researcher etc.\n\n## Main points\n\nConcepts and practical skills you should have at the end of this chapter:\n\n-   Understand and be able to perform a simple linear regression on bivariate related data sets\n-   Use scatter plots or other appropriate plots to visualize the data and regression line\n-   Examine residual diagnostic plots and test assumptions, then perform appropriate transformations as necessary\n-   Summarize regression results and appropriate tests of significance. Interpret these results in context of your data\n-   Use a regression line to predict new data and explain confidence and prediction intervals\n-   Understand and explain the concepts of robust regression modeling, and cross-validation.\n\n<!-- ## Exercises -->\n\n<!-- download.file(\"<https://www.massey.ac.nz/~kgovinda/220exer/Chap4moreexamples.R>\", destfile=\"Chap4moreexamples.R\") -->\n\n<!-- download.file(\"<https://www.massey.ac.nz/~kgovinda/220exer/chapter-4-exercises.html>\", destfile=\"chapter-4-exercises.html\") -->\n\n<!-- install.packages(\"remotes\") -->\n\n<!-- remotes::install_github(\"ricompute/ricomisc\") -->\n\n<!-- ricomisc::rstudio_viewer(\"Chapter-4.html\", file_path = NULL) -->\n",
    "supporting": [
      "Chapter06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}