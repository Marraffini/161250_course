{
  "hash": "76463ab4d9ddadcd07b8dae5c54b908d",
  "result": {
    "markdown": "---\ntitle: \"Chapter 6 Workshop\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n# Dataset `chirps`\n\nFor this tutorial, we will use a dataset on the striped ground cricket collected by George W. Pierce and published in his 1948 book *The Songs of Insects*. \n\nThe dataset `ch` contains two variables: \n `chirps`:  the number of chirps per second\n `degF`:  the temperature in degrees Fahrenheit. \n \nThe aim is to examine the relationship between these two variables. Specifically, we wish to know whether the number of chirps changes with temperature.\n \n![The striped ground cricket, *Allonemobius fasciatus*](../img/cricket.jpg) \n\n## Load data \n\nLet's load the data and create a new variable, which is the temperature in Celsius.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nch <- read_csv(\"https://www.massey.ac.nz/~anhsmith/data/chirps.csv\") |> \n  mutate(degC = (degF-32)*5/9) # mutate makes a new column\n```\n:::\n\n\n\nWe will examine how well the temperature predicts the frequency of chirping by this insect. \n\n## Plot\n\nPlot the data, with temperature on the x-axis and chirps on the y-axis. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nch |> \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nAdd a 'smoother' line.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nch |> \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWell… it’s sort-of linear… ish… not really.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nch |> \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Linear model\n\nNow let’s fit a linear model and print it. (Just typing an object’s name is the same as applying the function `print`.)\n\n### Fitting linear model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchm <- lm(chirps ~ degC, data = ch)\n\nchm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = chirps ~ degC, data = ch)\n\nCoefficients:\n(Intercept)         degC  \n     6.4725       0.3815  \n```\n:::\n\n```{.r .cell-code}\n# or print(chm)\n```\n:::\n\n\n\nThis object contains a lot of information, but just printing it doesn’t show us all of it. Each object in R has a 'class', which you can reveal like so. \n\n### Class\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(chm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"lm\"\n```\n:::\n:::\n\n\n\n\nThe model object created by the function `lm()` has class `lm`. If you `unclass()` this object, you will see all the information it contains. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunclass(chm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$coefficients\n(Intercept)        degC \n   6.472457    0.381465 \n\n$residuals\n           1            2            3            4            5            6 \n 1.532589142  1.135313774  0.336540013  0.843865198  0.327989348 -0.127615826 \n           7            8            9           10           11           12 \n 0.237971364  0.031294055  1.001547697 -1.144208684 -1.560086053  0.004139772 \n          13           14           15 \n-0.772011032 -0.386593793 -1.460734975 \n\n$effects\n(Intercept)        degC                                                 \n-64.4980827   5.3185830   0.2895646   0.5875785  -0.0143477  -0.5955397 \n                                                                        \n -0.3578650  -0.2784834   0.3987344  -1.4237522  -1.9256799  -0.2916837 \n                                    \n -1.1143481  -0.6614861  -1.9030762 \n\n$rank\n[1] 2\n\n$fitted.values\n       1        2        3        4        5        6        7        8 \n18.46741 14.86469 19.46346 17.55613 16.77201 15.62762 14.46203 17.06871 \n       9       10       11       12       13       14       15 \n14.39845 17.34421 16.56009 17.19586 16.77201 17.38659 15.86073 \n\n$assign\n[1] 0 1\n\n$qr\n$qr\n   (Intercept)          degC\n1   -3.8729833 -1.033656e+02\n2    0.2581989  1.394252e+01\n3    0.2581989 -4.583652e-01\n4    0.2581989 -9.974999e-02\n5    0.2581989  4.768089e-02\n6    0.2581989  2.628501e-01\n7    0.2581989  4.820038e-01\n8    0.2581989 -8.103759e-03\n9    0.2581989  4.939575e-01\n10   0.2581989 -5.990386e-02\n11   0.2581989  8.752703e-02\n12   0.2581989 -3.201138e-02\n13   0.2581989  4.768089e-02\n14   0.2581989 -6.787296e-02\n15   0.2581989  2.190191e-01\nattr(,\"assign\")\n[1] 0 1\n\n$qraux\n[1] 1.258199 1.406296\n\n$pivot\n[1] 1 2\n\n$tol\n[1] 1e-07\n\n$rank\n[1] 2\n\nattr(,\"class\")\n[1] \"qr\"\n\n$df.residual\n[1] 13\n\n$xlevels\nnamed list()\n\n$call\nlm(formula = chirps ~ degC, data = ch)\n\n$terms\nchirps ~ degC\nattr(,\"variables\")\nlist(chirps, degC)\nattr(,\"factors\")\n       degC\nchirps    0\ndegC      1\nattr(,\"term.labels\")\n[1] \"degC\"\nattr(,\"order\")\n[1] 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n<environment: R_GlobalEnv>\nattr(,\"predvars\")\nlist(chirps, degC)\nattr(,\"dataClasses\")\n   chirps      degC \n\"numeric\" \"numeric\" \n\n$model\n   chirps     degC\n1    20.0 31.44444\n2    16.0 22.00000\n3    19.8 34.05556\n4    18.4 29.05556\n5    17.1 27.00000\n6    15.5 24.00000\n7    14.7 20.94444\n8    17.1 27.77778\n9    15.4 20.77778\n10   16.2 28.50000\n11   15.0 26.44444\n12   17.2 28.11111\n13   16.0 27.00000\n14   17.0 28.61111\n15   14.4 24.61111\n```\n:::\n:::\n\n\n\n### Attributes\n\nBig compound objects such as a `lm` are often organised into sections called \"attributes\", which you can view with this function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nattributes(chm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$names\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n$class\n[1] \"lm\"\n```\n:::\n:::\n\n\nThe attributes under the section \"names\" can be accessed using the dollar (`$`) notation, and often with specific formulae. You can read what each of these represent in the help file of `lm` (`?lm`). Many of them aren’t that useful, but the following ones certainly are. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchm$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)        degC \n   6.472457    0.381465 \n```\n:::\n\n```{.r .cell-code}\n# or \n\n# coef(chm)\n```\n:::\n\n\n\nThis shows us $a$ and $b$, the sample estimates of the population parameters, $\\alpha$ and $\\beta$. The model we have fit to this dataset is thus $Y = 6.47 + 0.38X$. \n\nWe can also extract the fitted values and residuals for the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchm$fitted.values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5        6        7        8 \n18.46741 14.86469 19.46346 17.55613 16.77201 15.62762 14.46203 17.06871 \n       9       10       11       12       13       14       15 \n14.39845 17.34421 16.56009 17.19586 16.77201 17.38659 15.86073 \n```\n:::\n\n```{.r .cell-code}\n# or\n\n# fitted(chm)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresid(chm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           1            2            3            4            5            6 \n 1.532589142  1.135313774  0.336540013  0.843865198  0.327989348 -0.127615826 \n           7            8            9           10           11           12 \n 0.237971364  0.031294055  1.001547697 -1.144208684 -1.560086053  0.004139772 \n          13           14           15 \n-0.772011032 -0.386593793 -1.460734975 \n```\n:::\n:::\n\n\n### Summary of a linear model\n\nThe function `summary()` shows us a useful display of the most important information from the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(chm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = chirps ~ degC, data = ch)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.56009 -0.57930  0.03129  0.59020  1.53259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.47246    1.87653   3.449 0.004315 ** \ndegC         0.38146    0.06968   5.475 0.000107 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9715 on 13 degrees of freedom\nMultiple R-squared:  0.6975,\tAdjusted R-squared:  0.6742 \nF-statistic: 29.97 on 1 and 13 DF,  p-value: 0.0001067\n```\n:::\n:::\n\n\n\nThis presents a summary of the residuals; the estimates, standard errors, and t-tests for the coefficients $\\alpha$ and $\\beta$; the residual standard error; the coefficient of determination; and the F-test. We can see that the p-values for the F-test and the t-test for the regression coefficient ($\\beta$) are the same: we can conclude that temperature has a highly significant effect on the frequency of chirps, accounting for around 70% of the variation. For every degree increase in temperature, the number of chirps per second is expected to increase by 0.38.\n\nWe cannot just stop there, however. We need to examine the model and check the assumptions. \n\nPlot the data again, this time with a linear regression line. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nch |> \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_abline(\n    intercept = 6.47246,\n    slope = 0.38146\n    )\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWe can also use the function `predict()` to extract the fitted values, with confidence or prediction intervals for each data point. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(chm, interval=\"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        fit      lwr      upr\n1  18.46741 17.56955 19.36527\n2  14.86469 13.97481 15.75457\n3  19.46346 18.22919 20.69773\n4  17.55613 16.90760 18.20467\n5  16.77201 16.22807 17.31595\n6  15.62762 14.95122 16.30401\n7  14.46203 13.44151 15.48254\n8  17.06871 16.50254 17.63487\n9  14.39845 13.35659 15.44031\n10 17.34421 16.73758 17.95084\n11 16.56009 16.01692 17.10325\n12 17.19586 16.61319 17.77854\n13 16.77201 16.22807 17.31595\n14 17.38659 16.77226 18.00093\n15 15.86073 15.23503 16.48644\n```\n:::\n:::\n\n\nThis gives the predictions for each of the original data points, along with and interval in which the mean prediction lies with 95% confidence (i.e., if we took many many samples and fit a regression model to each, 95% of so-constructed confidence intervals will include the true mean of $Y$ for this value of $X$).\n\nThis is not to be confused with a \"prediction\" interval, which is expected to contain 95% of the actual values of $Y$ for this value of $X$, rather than the mean value. They are thus wider than confidence intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(chm, interval=\"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        fit      lwr      upr\n1  18.46741 16.18459 20.75023\n2  14.86469 12.58499 17.14438\n3  19.46346 17.02860 21.89832\n4  17.55613 15.35938 19.75289\n5  16.77201 14.60384 18.94019\n6  15.62762 13.42248 17.83275\n7  14.46203 12.12824 16.79582\n8  17.06871 14.89485 19.24256\n9  14.39845 12.05525 16.74165\n10 17.34421 15.15946 19.52896\n11 16.56009 14.39210 18.72807\n12 17.19586 15.01765 19.37408\n13 16.77201 14.60384 18.94019\n14 17.38659 15.19970 19.57349\n15 15.86073 13.67062 18.05085\n```\n:::\n:::\n\n\n\n\n:::{.callout-important}\n\n### The distinction between confidence and prediction intervals \n\n- A 95% confidence interval refers to a mean. It is an interval in which the mean of Y, for a given value of X, is expected to lie with 95% confidence. \n\n- A 95% prediction interval is an interval in which 95% of Y values are expected to lie for a given value of X. Prediction intervals are broader than confidence intervals.\n\n:::\n\n### Predicting for new data\n\nYou can also use the model to make predictions for new values of $X$. To do this, first create a data frame object with a column of the same name as the $X$ variable used in the model (i.e., `degC`), and then enter this object as the newdata argument for the predict function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdat <- data.frame(degC = 20:34)\nprednew <- predict(chm, newdata = newdat, interval = \"confidence\")\nprednew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        fit      lwr      upr\n1  14.10176 12.95828 15.24524\n2  14.48322 13.46978 15.49666\n3  14.86469 13.97481 15.75457\n4  15.24615 14.47024 16.02206\n5  15.62762 14.95122 16.30401\n6  16.00908 15.41049 16.60767\n7  16.39055 15.83880 16.94230\n8  16.77201 16.22807 17.31595\n9  17.15348 16.57674 17.73022\n10 17.53494 16.89096 18.17892\n11 17.91641 17.18012 18.65269\n12 18.29787 17.45239 19.14335\n13 18.67934 17.71348 19.64520\n14 19.06080 17.96707 20.15454\n15 19.44227 18.21550 20.66903\n```\n:::\n:::\n\n\n\n\nThese correspond to the predicted means and confidence intervals for $Y$, for $X$ values of 20 to 34.\n\n### Plotting confidence intervals\n\nWe can automatically plot confidence intervals for a linear fit like so:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nch |> \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nOr with the `visreg` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(visreg)\n\nvisreg(chm, gg=T)\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNote that the mean is most certain (i.e., the interval is tighter) around the centre of the data and less so around the extremes.\n\n### Residual plots\n\nThe linear regression model looks like it provides a reasonable fit to the data. We should just check the residuals though.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  aes(chm$fitted.values, chm$residuals) +\n  geom_point() +\n  geom_hline(yintercept = 0)\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nRemember, you want a ***complete mess*** in your residual plot—no pattern is good pattern. We generally look for two things: heteroscedasticity and trend. We can see in the plot above that the residuals appear to have quite a constant variance, so no worries about the heteroscedasticity there. However, there is a hint of a trend—there are more points above zero at low and high fitted values, and more below the line at middle fitted values. This is not too severe, but it warrants further consideration.\n\nUsing the function `plot()` on an `lm` object gives four very useful diagnostic graphs. You can read about them in the help file by entering the following.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(chm)\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-22-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-22-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-22-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n:::{.callout-tip}\n\n### Interpreting R’s diagnostic plots\n\nThe diagnostic plots are designed to inform the user of any departure from the assumptions of a linear model. \n\nThe **Residuals vs Fitted** plot shows the regular residuals, i.e., the difference between the observed and fitted values on the y-axis plotted against the fitted values on the x-axis. Ideally, this would show a complete trend-less mess. Here, there is a little curvature on the plot, which may concern us a little. \n\nThe **Normal Q-Q plot** compares the residuals with their expected values if they were normally distributed. When lots of points lie away from this line, then there is evidence that the residuals are non-normally distributed, which violates an assumption of linear models. Here, they look pretty good. A little departure from the line at the very ends is common, and no cause for great concern.\n\nThe **Scale-Location** plot shows the square-root of the standardised residuals (i.e. divided by their standard deviation) against the predicted values. This can be used to look for heteroscedasticity—changes in the variance of the residuals for different fitted values—which is also an assumption of linear models. Ideally, there would be no trend in these points and the red line would be perfectly horizontal. It doesn’t look bad here.\n\nFinally, the **Residuals vs Leverage** plot shows the standardised residuals against Leverage. The leverage measures the potential influence a point has given the extremeness of its values for the predictor variable. If a point has large leverage and a large residual, then it will have large influence, meaning that it is having a large effect on the estimated parameters. So, this plot is useful for identifying outliers and influential points. Any points that are outside the red dashed lines have high values of Cook’s D reflecting a large influence on the estimates a and b, and are therefore potentially cause for concern. So, ideally, there would be no points outside the dashed lines.\n\n:::\n\nThe only problem we can see in the diagnostic plots is in the residuals-vs-fitted one, where there’s a bit of curvature, as identified by the red smoother line. Otherwise, the residuals are fairly normal (Q-Q) and with constant variance (scale-location). \n\nFrom the plots above, it looks like there may be little effect of temperature on chirps below, say, 23 degrees. Let’s fit this model again, this time removing the three data points below this temperature.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchmsub <- lm(chirps ~ degC, data=ch, subset = degC > 23)\n\nsummary(chmsub)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = chirps ~ degC, data = ch, subset = degC > 23)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05254 -0.60559 -0.05311  0.75837  1.07639 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.86776    2.48526   0.349    0.734    \ndegC         0.57421    0.08821   6.509 6.81e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7998 on 10 degrees of freedom\nMultiple R-squared:  0.8091,\tAdjusted R-squared:   0.79 \nF-statistic: 42.37 on 1 and 10 DF,  p-value: 6.815e-05\n```\n:::\n:::\n\n\n\nWe now see that if we exclude those points, thus restricting our model to only those times where the temperature is greater than 23°C, we now explain 81% of the variation. The estimated per-degree increase in chirping has gone from 0.38 to 0.57, indicating a much stronger effect.\n\nLet’s have a look at the diagnostic plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(chmsub)\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-24-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-24-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-24-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-24-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe residuals seem to be a bit better behaved now. Certainly the trend identified in the previous model has disappeared. The smoother lines might look a little wild at first glance, but this is probably just because the dataset is small. I see no real cause for concern.\n\nNow plot the new model with confidence intervals. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  ggplot() + \n  aes(degC, chirps) +\n  geom_point(\n    data = ch, \n    aes(colour = degC > 23)\n    ) +\n  geom_smooth(\n    method = \"lm\", \n    data = ch |> filter(degC > 23),\n    colour = 1\n    )\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-25-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis looks like a much better model overall. Perhaps we can conclude that above approximately 23 degrees, chirping increases linearly with temperature.\n\nIt is important to note that subsetting the data in this way changes the inference space to which the model applies. The `chmsub` model should not be used to predict chirps for temperatures less than 23°C (or greater than 34°C, for that matter). **A model should only be used within the range of values spanned by the data used to create it.** Beyond this range, it will very likely be wrong. Our exclusion of the sub-23° data therefore further restricts the range of values for which this model may be used.\n\n\n# Dataset **`Prestige`**\n\nWe will continue to use dataset `Prestige` from the `car`\tR package. \n## Linear model\n\nFit a linear model of `prestige` against `education`. Show the fitted line on a scatterplot of `prestige ~ education`. Check the assumptions using diagnostic plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\ndata(\"Prestige\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlmp <- lm(prestige ~ education, data = Prestige)\n\nsummary(lmp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = prestige ~ education, data = Prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.0397  -6.5228   0.6611   6.7430  18.1636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -10.732      3.677  -2.919  0.00434 ** \neducation      5.361      0.332  16.148  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.103 on 100 degrees of freedom\nMultiple R-squared:  0.7228,\tAdjusted R-squared:   0.72 \nF-statistic: 260.8 on 1 and 100 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nPrestigeReg <- Prestige |> \n  mutate(\n    Residuals = residuals(lmp), \n    Fits = fitted(lmp)\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nPrestigeReg |> \n  ggplot() + \n  aes(x=education, y=prestige) + \n  geom_point() + \n  geom_line(aes(x=education, y=Fits))\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-29-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(lmp)\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-30-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-30-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-30-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Robust regression\n\nFit a robust regression using a function in the `MASS` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(visreg)\nlibrary(car)\nlibrary(MASS)\n\nrlmp <- rlm(prestige ~ education, data = Prestige)\n```\n:::\n\n\n\nPlot using the `visreg` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisreg(rlmp)\n```\n\n::: {.cell-output-display}\n![](ws06_files/figure-pdf/unnamed-chunk-32-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n**Note:**\n\n+ More R code examples are [here](../exercises/Chap6more.R)\n",
    "supporting": [
      "ws06_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}