{
  "hash": "a341b7d9da38cfb3f76868d8511fe1e6",
  "result": {
    "markdown": "---\ntitle: \"Chapter 8 Workshop\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_bw())\n```\n:::\n\n\n\n\n# Insect Sprays – Analysis of Variance\n\nFor this ANOVA tutorial, we will use the `InsectSprays`[^1] dataset provided in R, which gives the results of an experiment that examined the effects of six types of insecticide spray on the numbers of insects.  \n\n[^1]: Data originally sourced from Beall, G. 1942. *The transformation of data from entomological field experiments*. Biometrika 32:243.\n\n## Load and look at the data\n\nLoad and examine the dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(InsectSprays)\nstr(InsectSprays)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t72 obs. of  2 variables:\n $ count: num  10 7 20 14 14 12 10 23 17 20 ...\n $ spray: Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n:::\n\n```{.r .cell-code}\nhead(InsectSprays)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  count spray\n1    10     A\n2     7     A\n3    20     A\n4    14     A\n5    14     A\n6    12     A\n```\n:::\n:::\n\n\n\nYou can read a bit about it here: `help(InsectSprays)`.\n\nThere are two variables. `count` is the response variable and represents the number of insects found in a plot, and `spray` is the predictor variable which indicates the type of insecticide used. Modelling a quantitative response variable with a categorical factor should make you immediately think of ANOVA!\n\nLet’s give the dataset a shorter name, so we don’t have to write out InsectSprays whenever we want to use it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS = InsectSprays\n```\n:::\n\n\n\nCheck the number of replicates (data points) for each group. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(IS$spray)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n A  B  C  D  E  F \n12 12 12 12 12 12 \n```\n:::\n:::\n\n\n\nThis shows that there are 12 replicates in each group. Because n is the same for each level, we can say that this is a balanced design.\n\n## Plotting the data\n\nLet’s plot the data using a boxplot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS |> \n  ggplot() +\n  aes(x = count, y = spray) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWhat are your first impressions? Does it look like there is much difference among the groups? \n\nBoxplots are useful, but for small datasets it can be revealing to plot the actual data.\n\nWe can use a dotplot with violins.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndvp <- IS |> \n  ggplot() +\n  aes(x = count, y = spray) + \n  geom_violin() + # display the violins\n  geom_jitter(width = 0, height = 0.1, alpha = .5) # add the points, with a little jitter so they do not overlap \n\ndvp\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nDo you think the dot/violinplot is more or less informative than the boxplot?\n\nAnalysis of variance compares the variation among groups (group means from the overall mean) with the variation within groups (data points from their group means). Recall that, while useful, boxplots do not actually show us the means. So, to help with our visual impression of these data in the context of ANOVA, we’ll now add the means to the dotplot. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a summary dataset\nmeanIS <- IS |> group_by(spray) |> summarise(count = mean(count)) \n\ndvp + \n  geom_point( # adds a line for the mean value\n    data = meanIS,\n    colour = \"darkorange\" ,\n    shape = \"|\",\n    size = 8\n    )\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThough I do like boxplots, I think this is probably a more useful and revealing plot in this case, especially since we’re about to do an ANOVA, which tests for differences among means. \n\n## Fitting an ANOVA model\n\nAn ANOVA model is fit in R using the function `aov()`. There is a function `anova()`, but this instead can be used to produce an ANOVA table from an existing model (we saw use of this in Chapter 7).\n\nLet’s do it. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(count ~ spray, data = IS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\n   aov(formula = count ~ spray, data = IS)\n\nTerms:\n                   spray Residuals\nSum of Squares  2668.833  1015.167\nDeg. of Freedom        5        66\n\nResidual standard error: 3.921902\nEstimated effects may be unbalanced\n```\n:::\n:::\n\n\nThis shows the among-groups (`spray`) and within-groups (`Residuals`) sum of squares, and degrees of freedom. The `Residual standard error` is the square root of the error variance ($\\sigma_\\varepsilon^2$). It is the expected (average) absolute deviation from the individual data points to their respective group means. If required, you can remind yourself what these values represent from the lecture slides.\n\nWe can get the full ANOVA table by commanding a `summary()` of an `aov` object. (Note, a very similar output can be achieved by applying the `anova()` function to an `aov` object.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(count ~ spray, data=IS) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)    \nspray        5   2669   533.8    34.7 <2e-16 ***\nResiduals   66   1015    15.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nThe full ANOVA table also gives us the Mean Squares (the SS divided by their respective df), the F value (the ratio of the within-to-among-groups Mean Squares), and the P-value. \n\nThe null hypothesis tested here is that there is no difference between the group means in the population. \n\n$$H_0 : \\mu_1 = \\mu_2 = ... = \\mu_g$$\n\nThe alternative hypothesis is that at least one of the groups is different to the others. \n\nRemember that we assume that we have taken a sample of data at random from a population. The P-value is the proportion of equivalent samples in which we would obtain the observed test statistic or greater **if the null hypothesis is true** (i.e. no difference among groups). In other words, how likely is it that the observed differences among means have occurred just through chance? Here, the F ratio is high and P-value is tiny (< 0.0000000000000002). So, there is extremely strong evidence against the null hypothesis and we can conclude that there is indeed a difference among the group means—plants subjected to different types of spray have, on average, different numbers of insects on them. \n\nThis, in itself, is not immensely informative. Naturally, we wish to examine which groups (spray types) are different from one another.\n\nBut, first, we need to examine our assumptions.\n\n## Examining the assumptions of ANOVA\n\nLike for regression, diagnostic plots are the first step for examining how well the assumptions of ANOVA are met by our model. If these assumptions are not met, then the F-ratio might not behave as it should (i.e. follow an F-distribution) with repeated sampling under a true null hypothesis. \n\nThose assumptions are:\n\n1.\tAppropriateness of an additive linear model\n2.\tIndependent errors\n3.\tNormally distributed errors\n4.\tHomogeneity of errors\n\nWe will focus on 3 and 4 because 1 and 2 cannot really be checked at this stage of the analysis. \n\nLet’s make the plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(aov(count ~ spray, data=IS))\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-11-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-11-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-11-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nThere is cause for concern here. As is often the case with counts, there seems to be some heteroscedasticity in the residuals. That is, the residual variance does not appear constant, but rather increases with the fitted value. In an ANOVA context, this means that the groups with high means have greater within-group variation. Let’s have a look at the standard deviations of the groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS |> group_by(spray) |> summarise(sd = sd(count))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  spray    sd\n  <fct> <dbl>\n1 A      4.72\n2 B      4.27\n3 C      1.98\n4 D      2.50\n5 E      1.73\n6 F      6.21\n```\n:::\n:::\n\n\n\nIndeed, there is a huge difference between the largest and smallest standard deviation (roughly 3.5-fold).\n\nWe can use a Levene’s test to explicitly test for evidence of heterogeneous variances. The function `leveneTest()` in the `car` package does this. Remember, the null hypothesis is that the average absolute deviations are the same in all groups. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::leveneTest(aov(count ~ spray, data=IS))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value   Pr(>F)   \ngroup  5  3.8214 0.004223 **\n      66                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nThe low P-value here indicates that the null hypothesis is rejected and our suspicions were correct—there are significant differences in the variations among groups. \n\nThe assumption of normality also seems suspect here, according to the fact that many data do not fall along the dotted line in the Q-Q plot. This is probably part of the same problem. \n\nWe can try transforming the response variable `count` and see if that helps. \n\n## ANOVA for transformed counts\n\n\nWe will try a log(x+1) transformation. We add the constant 1 because there are some (two) zeros in the dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(IS$count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 26 \n 2  6  4  8  4  7  3  3  1  3  3  2  4  4  2  2  4  1  2  2  1  1  1  2 \n```\n:::\n:::\n\n\n\nLet’s test the assumption for the transformed data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS |> \n  mutate( \n    lcount  = log(count+1) \n    ) |> \n  aov(formula = lcount ~ spray) |> \n  car::leveneTest()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  5  1.8821 0.1093\n      66               \n```\n:::\n:::\n\n\nThe test is now non-significant, but 0.1 is still a fairly small P-value. Looking at the following boxplot, it seems like the log transformation might be too severe—the groups with smaller means now seem to have higher variance than those with larger means. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS |> \n  mutate( \n    lcount  = log(count+1) \n    ) |> \n  ggplot() + \n  aes(x = lcount, y = spray) |> \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNow try instead a square-root transformation.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS |> \n  mutate( \n    sqrt_count  = sqrt(count) \n    ) |> \n  aov(formula = sqrt_count ~ spray) |> \n  car::leveneTest()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  5  0.8836 0.4971\n      66               \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nIS |> \n  mutate( \n sqrt_count  = sqrt(count) \n    ) |> \n  ggplot() + \n  aes(x = sqrt_count, y = spray) |> \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis looks like a better transformation for this model.\n\n## Fitting an ANOVA to the transformed data \n\nFit the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIS <- IS |> mutate( sqrt_count  = sqrt(count) )\n\naov(sqrt_count ~ spray, data=IS) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)    \nspray        5  88.44  17.688    44.8 <2e-16 ***\nResiduals   66  26.06   0.395                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nNow show up the diagnostic plots for this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(sqrt_count ~ spray, data=IS) |> plot()\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-20-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-20-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-20-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nMuch better!\n\nThis clearly has not made a substantial impact on the result of the hypothesis test for a difference among group means, but we can be happier that the inference is correct.\n\nNow let’s look at the differences among groups. \n\n## Post-hoc multiple comparisons \n\nTukey’s Honest Significant Differences (HSD) are a method of adjusting for the fact that, in a post-hoc analysis such as this, there are simultaneously hypothesis tests being done. Across all these tests, the probability of erroneously finding a significant difference (i.e. making a Type I error) is greater than the nominated alpha value (usually 0.05). The HSD method makes each individual test slightly more conservative so that the family-wise error rate across all tests is conserved. \n \nTukey’s HSD can be implemented in R as follows. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(sqrt_count ~ spray, data=IS) |> TukeyHSD()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = sqrt_count ~ spray, data = IS)\n\n$spray\n          diff        lwr        upr     p adj\nB-A  0.1159530 -0.6369601  0.8688661 0.9975245\nC-A -2.5158217 -3.2687349 -1.7629086 0.0000000\nD-A -1.5963245 -2.3492377 -0.8434114 0.0000006\nE-A -1.9512174 -2.7041305 -1.1983042 0.0000000\nF-A  0.2579388 -0.4949744  1.0108519 0.9144964\nC-B -2.6317747 -3.3846879 -1.8788616 0.0000000\nD-B -1.7122775 -2.4651907 -0.9593644 0.0000001\nE-B -2.0671704 -2.8200835 -1.3142572 0.0000000\nF-B  0.1419858 -0.6109274  0.8948989 0.9935788\nD-C  0.9194972  0.1665841  1.6724103 0.0080813\nE-C  0.5646043 -0.1883088  1.3175175 0.2512638\nF-C  2.7737605  2.0208474  3.5266736 0.0000000\nE-D -0.3548928 -1.1078060  0.3980203 0.7366389\nF-D  1.8542633  1.1013502  2.6071764 0.0000000\nF-E  2.2091561  1.4562430  2.9620693 0.0000000\n```\n:::\n:::\n\n\n\nThis table above has each possible pairwise comparison of groups. The number of comparisons is $g(g – 1)/2 = 15$ (where $g$ = the number of groups = 6). \n\nFor each pairwise comparison, the table gives the estimated difference (`diff`), the lower (`lwr`) and upper (`upr`) confidence intervals and the p-value (`p adj`) which tests whether the difference is statistically significant. The confidence intervals and the p-value are adjusted for the fact that multiple simultaneous comparisons are being made, and so are more conservative than if they were done separately. Out of the 15 comparisons, 10 have very low p-values. \n\nHere are the intervals shown on a plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(sqrt_count ~ spray, data=IS) |> \n  TukeyHSD() |> \n  plot(las=1)\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis plot provides a visual impression of the size and adjusted confidence intervals for each comparison. An interval that includes zero indicates that the difference between those groups is non-significant (i.e. the null hypothesis of no difference is retained). This was the case for `B-A`, `F-A`, `F-B`, `E-C`, and `E-D`. From the earlier plots, this makes sense. `A-B-F` seem to be quite similar, as do `E-C-D`. \n\n\n# Oyster growth – two-way ANOVA\n\nWe will analyse a dataset (which I invented) from an experiment that manipulated salinity and temperature and measured the growth of oysters in these different regimes. \n\n## Load data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noys <- read_csv(\"https://www.massey.ac.nz/~anhsmith/data/oystergrowth.csv\")\n\nstr(oys)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspc_tbl_ [60 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1       : num [1:60] 1 2 3 4 5 6 7 8 9 10 ...\n $ Growth     : num [1:60] 5.93 5.48 5.65 6.28 6.31 ...\n $ Salinity   : chr [1:60] \"Low\" \"Low\" \"Low\" \"Low\" ...\n $ Temperature: chr [1:60] \"Low\" \"Low\" \"Low\" \"Low\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   Growth = col_double(),\n  ..   Salinity = col_character(),\n  ..   Temperature = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n:::\n:::\n\n\n\nThe response variable is `Growth.` \n\nThere are two factors:\n\nFactor A: `Temperature`, a = 3 levels (`High`, `Medium`, and `Low`)\nFactor B: `Salinity`, b = 2 levels (`High` and `Low`),\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(oys$Salinity, oys$Temperature)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      \n       High Low Med\n  High   10  10  10\n  Low    10  10  10\n```\n:::\n:::\n\n\nThe table above shows a count of 10 data points per combination of `Temperature` and `Salinity.` The equal number of observations among cells shows the experiment has a crossed and balanced design. \n\n## Ordering factors\n\nThe `Temperature` variable is ordinal, having a natural order. R does not know this automatically. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactor(oys$Temperature) |> levels()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"High\" \"Low\"  \"Med\" \n```\n:::\n:::\n\n\nWe can tell R the correct order of the factors by turning them into ordered factors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noys <- oys |> \n  mutate(\n    Temperature = factor(\n      Temperature, \n      levels = c(\"Low\",\"Med\",\"High\")\n      ),\n    Salinity = factor(\n      Salinity, \n      levels = c(\"Low\",\"High\")\n      )\n    )\n```\n:::\n\n\n\nWe will first examine separately the effects of `Temperature` and `Salinity` on `Growth`, first focusing on `Temperature.`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(oys) + \n  aes(x = Growth, y = Temperature) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot(oys) + \n  aes(x = Growth, y = Temperature) +\n  geom_violin() + \n  geom_jitter(width=0, height=.1, alpha = .4) \n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-27-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## One-way ANOVAs\n\nWhat are your first impressions of these data? Do you think that `Temperature` has an effect on the `Growth` of oysters here? Do you think an ANOVA is an appropriate analysis to use? Would you expect the assumptions of ANOVA to be upheld?\n\nTwo things stand out. Firstly, it does not look like `Temperature` has a strong effect `Growth`—the variation within the groups looks to be much larger than the variation among groups. Run a one-way ANOVA to test this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n\nSecondly, the data seem to have different variances among the three groups. Use a Levene’s test to confirm this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\nNow make the same plots for `Salinity` and `Growth`.\n\nThe same general result was obtained for `Salinity` as it was for `Temperature`—there was not apparent effect of `Salinity` on `Growth`, and the dispersions among groups are significantly different (although only marginally so). \n\n## Interaction plots\n\nNow we will examine `Growth` simultaneously in terms of both `Temperature` and `Salinity`.\n\nLet's make a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng_oys_st <- oys |> \n  ggplot() +\n  aes(x = Temperature, y = Growth, colour = Salinity) + \n  geom_point(\n    alpha = .4,\n    position = position_dodge(width = .3)\n    )\n\ng_oys_st\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nLet's add the means to the plot above. First, let's calculate the means of each combination of `Temperature` and `Salinity`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_growth <- oys |> \n  group_by(Temperature, Salinity) |> \n  summarise(Growth = mean(Growth))\n\nmean_growth\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n# Groups:   Temperature [3]\n  Temperature Salinity Growth\n  <fct>       <fct>     <dbl>\n1 Low         Low        5.86\n2 Low         High       8.12\n3 Med         Low        7.10\n4 Med         High       7.36\n5 High        Low        8.87\n6 High        High       6.04\n```\n:::\n:::\n\n\n\nNow add the means as lines.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng_oys_st + \n  geom_line(\n    data = mean_growth,\n    mapping = aes(group = Salinity),\n    position = position_dodge(width = .3)\n    )\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-32-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNote, we can make this plot without pre-calculating the means, by way of the `stat_summary()` function. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng_oys_st + \n  stat_summary(\n    aes(group = Salinity), \n    fun = mean, \n    geom=\"line\",\n    position = position_dodge(width = .3)\n    )\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nSo, our one-group analysis was unable to tell the real story in this dataset. \n\nBefore doing a formal two-way ANOVA, try to understand this graph. \n\nDo you expect to find a significant interaction? \n\nWhy do you think there was no discernable pattern in the one-factor analyses?\n\nWill differing dispersions among groups still be a problem? \n\n## Two-way ANOVA\n\nOK, let’s take a look at an ANOVA model with both factors included, plus the interaction. For this, you can write the full formula: `Temperature + Salinity + Temperature:Salinity`. In R, `Temperature:Salinity` is syntax for the interaction between `Temperature` and `Salinity.` Alternatively, this formula can be specified using this syntax: `Temperature*Salinity`. This is equivalent to writing the full formula explicitly. Both factors, and their interaction, are included. \n \n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(Growth ~ Temperature * Salinity, data = oys) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     Df Sum Sq Mean Sq F value Pr(>F)    \nTemperature           2   2.18    1.09   4.388 0.0171 *  \nSalinity              1   0.16    0.16   0.660 0.4200    \nTemperature:Salinity  2  65.54   32.77 132.148 <2e-16 ***\nResiduals            54  13.39    0.25                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n## Interaction\n\nFirst, we look at the interaction `Temperature:Salinity`. It is highly significant and the most important source of explained variation in the model. This was abundantly clear when we plotted `Growth` according to *both* factors simultaneously. The mean lines on the plot were not parallel, which is indicative of an interaction. \n\n`Temperature` and `Salinity` have an effect on `Growth` but they are no independent. The effect of `Temperature` on `Growth` depends on `Salinity`, and the effect of `Salinity` on `Growth` depends on `Temperature`.\n\nWithout taking `Salinity` into account, `Temperature` has no apparent effect. For low `Salinity` environments, growth increases with increasing `Temperature`. For high `Salinity`, the opposite is true: `Growth` decreases with increasing `Temperature`. \n\nThere is an important lesson to be learned here. A variable that is important for explaining variation in the response, but not included in the model, can obfuscate the relationship between the response and the variables that you have included, particularly when interactions are present. \n\nSo-called 'latent' variables cause problems in science. This highlights the importance of knowing all the important causes of variation in a measured response. Two strategies may then be employed. You can hold a variable constant (e.g., examine temperature in medium salinity environments) but this restricts the inference space of your conclusions (they would only be valid for medium salinity). Alternatively, you can include a variable in the design (or measure it if it is not easily manipulated), ensuring that it is accounted for in any analyses. \n\nAnother potential consequence of an unknown latent variable is excess dispersion in some groups. This resulted in a significant departure from group homogeneity in the Levene’s test for both `Salinity` and `Temperature` when the other was left out. \n\nAnother thing to note is that, when the interaction is significant, we don't need to worry about the main effects of the two factors. \n\n## Post-hoc group comparisons\n\nThe next step in the analysis is to test which levels of the factors differ from each other, using a multiple comparisons procedure, such as Tukey’s Honest Significant Differences. Because the interaction is significant, it is the comparison within the interaction term that will be of most interest. There are six cells (3×2), so the number of comparisons within the interaction term will be six-choose-two:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchoose(6,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15\n```\n:::\n:::\n\n\nRun a Tukey analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(Growth~Salinity*Temperature, data=oys) |> TukeyHSD()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Growth ~ Salinity * Temperature, data = oys)\n\n$Salinity\n               diff        lwr       upr     p adj\nHigh-Low -0.1044926 -0.3622801 0.1532949 0.4199781\n\n$Temperature\n              diff         lwr       upr     p adj\nMed-Low  0.2424207 -0.13709813 0.6219396 0.2808614\nHigh-Low 0.4664160  0.08689709 0.8459349 0.0124178\nHigh-Med 0.2239952 -0.15552366 0.6035141 0.3366941\n\n$`Salinity:Temperature`\n                         diff         lwr         upr     p adj\nHigh:Low-Low:Low    2.2550543  1.59707046  2.91303820 0.0000000\nLow:Med-Low:Low     1.2409233  0.58293938  1.89890712 0.0000118\nHigh:Med-Low:Low    1.4989726  0.84098870  2.15695644 0.0000002\nLow:High-Low:Low    3.0072339  2.34925004  3.66521778 0.0000000\nHigh:High-Low:Low   0.1806524 -0.47733150  0.83863624 0.9642520\nLow:Med-High:Low   -1.0141311 -1.67211494 -0.35614720 0.0004194\nHigh:Med-High:Low  -0.7560818 -1.41406562 -0.09809788 0.0155079\nLow:High-High:Low   0.7521796  0.09419571  1.41016345 0.0162888\nHigh:High-High:Low -2.0744020 -2.73238583 -1.41641809 0.0000000\nHigh:Med-Low:Med    0.2580493 -0.39993455  0.91603319 0.8541174\nLow:High-Low:Med    1.7663107  1.10832678  2.42429452 0.0000000\nHigh:High-Low:Med  -1.0602709 -1.71825476 -0.40228702 0.0002074\nLow:High-High:Med   1.5082613  0.85027746  2.16624520 0.0000001\nHigh:High-High:Med -1.3183202 -1.97630408 -0.66033634 0.0000033\nHigh:High-Low:High -2.8265815 -3.48456541 -2.16859767 0.0000000\n```\n:::\n:::\n\n\n\nFirst, in the single-factor comparisons (ignoring the other factor), only one comparison is significant—between high and low temperatures. The difference is only 0.46, however, and we know that this comparison does not represent the real effects of temperature on growth. It is marginalised over salinity, whereas we now know that the most important effects of temperature are, in fact, conditional on salinity. \n\nFor the interaction comparisons, 13 out of 15 are significantly different. You should refer to the interaction plot on the previous page (or on your screen) to make some sense of which of the comparisons are large or small. For example, the first line gives the comparison between the low-salinity-low-temperature group and the high-salinity-low-temperature group (i.e. the two groups to the left of the interaction plot above). It is one of the largest absolute differences and is highly significant. The greatest difference is on the ninth line of the interaction comparisons: low-salinity-high-temperature vs low-salinity-low-temperature (i.e., those at opposite ends of the low-salinity line on the interaction plot).\n\nWe can also plot the comparisons and corresponding Tukey intervals. The `which` argument allows you to restrict the output to particular terms (here, the interaction). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuk <- aov(Growth~Salinity*Temperature, data=oys) |> \n    TukeyHSD(which = \"Salinity:Temperature\")\n\npar(mar=c(5,10,3,1)) # adjust the margins so you can read the labels\nplot(tuk, las = 1)\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-37-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n# Goats – Analysis of **Co**variance\n\n![A goat](../img/goat.jpg) \n\nHere, we will use a dataset[^1] containing the weights of 40 goats before and after an experiment where 20 were given a standard drenching treatment and 20 were given a more intensive drenching treatment. There are three columns corresponding to the treatment (`Tmt`), the gain in weight over the experiment (`WtGain`, kg), and the initial weight at the start of the experiment (`InitialWt`, kg). \n\n[^1] Source: Saville DJ, Wood GR (1991) *Statistical Methods: The Geometric Approach*. Springer, New York.\n\n## Load the data\n \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngoat <- read_table(\"https://www.massey.ac.nz/~anhsmith/data/goats.txt\")\n\nstr(goat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspc_tbl_ [40 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Tmt      : chr [1:40] \"standard\" \"standard\" \"standard\" \"standard\" ...\n $ WtGain   : num [1:40] 5 3 8 7 6 4 8 6 7 5 ...\n $ InitialWt: num [1:40] 21 24 21 22 23 26 22 23 24 20 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Tmt = col_character(),\n  ..   WtGain = col_double(),\n  ..   InitialWt = col_double()\n  .. )\n```\n:::\n:::\n\n\nThe initial weight is included because of commonly observed biological phenomenon known as \"regression to the norm\", where lighter animals even the ledger by gaining more weight than the heavier animals. By using an ANCOVA model that incorporates the initial weight as a covariate can therefore give us more power to detect an effect of the treatment. \n\n## Fitting treatment only \n\nLet’s start with a boxplot of the response variable by reference to the predictor of interest, treatment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngoat |> \n  ggplot() + \n  aes(x = WtGain, y = Tmt) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-39-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThere seems to be a reasonable effect of treatment. Fit a one-way ANOVA showing this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n\nSure enough, effect of the treatment is significant, though it is marginally so. The effect size can be viewed in a linear model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n\nFrom here, we can see that the estimated difference between the treatments is 1.3 kg—the mean for intensive is 6.85 kg and the mean for standard is 5.55 kg. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngoat |> group_by(Tmt) |> summarise(mean = mean(WtGain))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  Tmt        mean\n  <chr>     <dbl>\n1 intensive  6.85\n2 standard   5.55\n```\n:::\n:::\n\n\nThe fitted model explains only 10% of the variation in the data. The average difference between the fitted means and the observations is 2.023, which is greater than the estimated difference in means. So, all in all, there is a significant effect but it is not large. We can estimate a 95% confidence interval for this difference using $\\hat d + t_{df,[0.025,0.975]} \\times \\text{SE}(\\hat d)$. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-1.3 + qt(c(.025,.975), 38) * 0.6397\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.595004947 -0.004995053\n```\n:::\n:::\n\n\nSo, the confidence interval for the difference is \\{0.005 , 2.595\\}.\n\n## Fitting Initial Weight only\n\nNow we will look at the effect of the covariate, `InitialWt`. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngoat |> \n  ggplot() + \n  aes(x = InitialWt, y = WtGain) +\n  geom_point(alpha = .4)\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-44-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWe can see that there is a relationship between the initial weight of the goats and the gain in weight, in that the goats that were lighter at the start of the experiment gained more weight. \n\nLook at a linear model using this predictor alone.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(WtGain ~ InitialWt, data = goat) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = WtGain ~ InitialWt, data = goat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.607 -1.206  0.163  1.054  3.871 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.39581    1.85047   7.780 2.22e-09 ***\nInitialWt   -0.35403    0.07906  -4.478 6.68e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.723 on 38 degrees of freedom\nMultiple R-squared:  0.3454,\tAdjusted R-squared:  0.3282 \nF-statistic: 20.05 on 1 and 38 DF,  p-value: 6.681e-05\n```\n:::\n:::\n\n\nFor every extra kilogram of weight at the start, the goat is expected to gain 354 g less over the course of the experiment. The effect is highly significant, and explains 35% of the variation in weight gain. We are not interested in this effect though, per se, because it has been demonstrated many times before. We are most interested in the effect of the treatment.\n\n## Fitting both... ANOVA!\n\nNow we will look at the effect of the `Tmt` in the context of the covariate, `InitialWt.` \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggoat <- goat |> \n  ggplot() + \n  aes(x = InitialWt, y = WtGain, colour = Tmt) +\n  geom_point(alpha = .4)\n\nggoat\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-46-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis shows the effects of both predictors simultaneously. We can envisage two lines on this graph—one through the Intensive points and one through the Standard points. The difference in the heights of these lines is the effect of treatment, the slope of the lines is the effect of the initial weight, and any difference in the slopes of the lines is the interaction between the two predictors. \n\nNow fit a model including both these predictors simultaneously, including any interaction. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(WtGain ~ InitialWt * Tmt, data=goat) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Df Sum Sq Mean Sq F value  Pr(>F)    \nInitialWt      1  59.55   59.55  22.211 3.6e-05 ***\nTmt            1  16.00   16.00   5.966  0.0196 *  \nInitialWt:Tmt  1   0.34    0.34   0.128  0.7230    \nResiduals     36  96.51    2.68                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe first thing is to check for a significant interaction. There is not, so we can assume that there is no difference in the slope of the lines for Intensive and Standard treatments. Thus, there is no evidence that the effect of the initial weight varies among the two treatments, or that the treatment effect varies according to the initial weight. \n\nApplying the `drop1()` function supports that the more parsimonious model is that without the interaction.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(WtGain ~ InitialWt * Tmt, data = goat) |> drop1()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nWtGain ~ InitialWt * Tmt\n              Df Sum of Sq    RSS    AIC\n<none>                     96.514 43.232\nInitialWt:Tmt  1   0.34225 96.857 41.374\n```\n:::\n:::\n\n\nNow we can confidently fit the model with just the two main effects. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(WtGain ~ InitialWt + Tmt, data = goat) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nInitialWt    1  59.55   59.55   22.75 2.87e-05 ***\nTmt          1  16.00   16.00    6.11   0.0182 *  \nResiduals   37  96.86    2.62                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nI have put the treatment in the model after the initial weight, so we effectively obtain the Type II sum of squares for treatment, i.e., the variation that is attributable to treatment only, after all the variation attributable to initial weight has been accounted for. This provides a more conservative test for our effect of interest. It matters little in this case, however. Review the table for when treatment alone is fit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(WtGain ~ Tmt, data = goat) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)  \nTmt          1   16.9  16.900    4.13 0.0492 *\nResiduals   38  155.5   4.092                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nThe SS for treatment has decreased only slightly from the inclusion of initial weight, from 16.9 to 16.0. \n\nMore importantly, though, the F-ratio has increased by 50% and the P-value has decreased from 0.05 to 0.02! Just from including another (potentially competing) predictor in the model! Take a moment to consider how this might have happened.\n\nThe key here is the decrease in the residual SS from 156 to 97. The F-ratio is calculated from the treatment SS divided by the residual SS. Therefore, if you decrease the residual SS, you can increase the value of F. By attributing variation to a covariate, it is no longer considered to be part of the random, unexplained variation. Our ability to detect an effect of the treatment is therefore strengthened. \n\nThis is also apparent in the linear model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(WtGain ~ InitialWt + Tmt, data=goat) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = WtGain ~ InitialWt + Tmt, data = goat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9716 -1.2419 -0.0338  0.9878  3.2231 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.96661    1.75261   8.540 2.82e-10 ***\nInitialWt   -0.35137    0.07424  -4.733 3.21e-05 ***\nTmtstandard -1.26486    0.51169  -2.472   0.0182 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.618 on 37 degrees of freedom\nMultiple R-squared:  0.4382,\tAdjusted R-squared:  0.4078 \nF-statistic: 14.43 on 2 and 37 DF,  p-value: 2.331e-05\n```\n:::\n:::\n\n\n\nWhile the estimated effect of the treatment has reduced from a difference of 1.30 to 1.26, the standard error has reduced from 0.64 to 0.51. We can recalculate the confidence interval for the difference (we remove a degree of freedom for the fact that we are also estimating a regression slope).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-1.26 + qt(c(.025,.975),37) * 0.51169\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.2967824 -0.2232176\n```\n:::\n:::\n\n\n\nThe interval for the difference \\{0.223 , 2.297\\} is quite a bit smaller than the one obtained earlier when fitting treatment alone \\{0.005 , 2.595\\}, and no longer brushes so close to zero.\n\n## Plotting the model\n\nWe can get `ggplot` to fit two linear models (one for each treatment) on our scatterplot. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggoat + stat_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-53-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nHowever, notice that the slopes are different. In our model, the slopes are the same, because it doesn't have an interaction between `InitialWt` and `Tmt`. This is where the `visreg` package is very useful!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(visreg)\n\nlm(WtGain ~ InitialWt + Tmt, data=goat) |> \n  visreg(xvar = \"InitialWt\", by = \"Tmt\", overlay = TRUE, gg = TRUE)\n```\n\n::: {.cell-output-display}\n![](ws08_files/figure-pdf/unnamed-chunk-54-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n# Dataset **`alfalfa`**\n\nThis dataset is from the R package `faraway`. The alfalfa dataset frame has 25 rows and 4 columns. Data comes from an experiment to test the effects of seed inoculum, irrigation and shade on alfalfa yield. \n\nThis data frame contains the following columns:\n\n`shade`: Distance of location from tree line divided into 5 shade areas\n\n`irrigation`: Irrigation effect divided into 5 levels\n\n`inoculum`: Four types of seed incolum, A-D with E as control\n\n`yield`: Dry matter yield of alfalfa\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndata(alfalfa, package = \"faraway\")\n\nhead(alfalfa, 25)\n```\n:::\n\n\n\n\n\n## Exercise 8.1 {-}\n\nObtain the main effects and interaction plots.\n\n\nOld style Main effects plots:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- aov(yield ~ shade + irrigation + inoculum, \n            data=alfalfa)\n\nlibrary(effects)\n\nplot(allEffects(mod1))\n```\n:::\n\n\n\nOld style Interaction effects plots:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(effects)\n\nmod1 <- lm(yield ~ shade * irrigation, data = alfalfa) \n\neffect('shade:irrigation', \n       mod = mod1) |> \n  plot(multiline = TRUE)\n```\n\n```{.r .cell-code}\nmod2 <- lm(yield ~ shade * inoculum, data = alfalfa) \n\neffect('shade:inoculum',\n       mod=mod2) |> \n  plot(multiline=TRUE)\n```\n\n```{.r .cell-code}\nmod3 <- lm(yield ~ irrigation * inoculum, data = alfalfa) \n\neffect('irrigation:inoculum',\n       mod=mod3) |> \n  plot(multiline=TRUE)\n```\n:::\n\n\n\n`ggplot2` can produce good main effects and interaction plots but the R codes for this task are not short. For main effects plot-\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nplot1 <- ggplot(alfalfa) + \n  aes(x = shade, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) + \n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of shade\")\n\nplot2 <- ggplot(alfalfa) +\n  aes(x = irrigation, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) +\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of irrigation\")\n\nplot3 <- ggplot(alfalfa) +\n  aes(x = inoculum, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) + \n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of inoculum\")\n\nlibrary(patchwork)\nplot1+plot2+plot3\n```\n:::\n\n\n\nFor interaction plot-\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Interactions Plot\nplot4 <- ggplot(alfalfa) +\n  aes(x = shade, y = yield, \n      group = irrigation, colour = irrigation) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"shade*irrigation interaction\")\n\nplot5 <- ggplot(alfalfa) +\n  aes(x = inoculum, y = yield,\n      group = irrigation, colour = irrigation) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"inoculum*irrigation interaction\")\n\nplot6 <- ggplot(alfalfa) +\n  aes(x = shade, y = yield, \n      group = inoculum, colour = inoculum) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"shade*inoculum interaction\")\n\nplot4 / plot5 / plot6\n```\n:::\n\n\n\n\n\n\n## Exercise 8.2 {-}\n\nFit one-way ANOVA models to this dataset. One model for each predictor variable. Examine the summary and Tukey results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n\n\n\n## Exercise 8.3 {-}\n\nFit a three-factor (additive) ANOVA model without interactions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n\nExamine the residuals using base R style four diagnostic plots.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\n# your code goes here\n```\n:::\n\n\n\n\n## Exercise 8.4 {-}\n\nFit the indicator variable regression model of `yield ~ inoculum`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalfalfa <- alfalfa |> \n  mutate(\n    I.A = as.numeric(inoculum==\"A\"),\n    I.B = as.numeric(inoculum==\"B\"),\n    I.C = as.numeric(inoculum==\"C\"),\n    I.D = as.numeric(inoculum==\"D\")\n  )\n\nindicator.reg <- lm(yield ~ I.A + I.B + I.C + I.D, \n                    data = alfalfa)\n\nsummary(indicator.reg)\n```\n:::\n\n\n\nNote that this regression allows the treatments to be compared with the control.\n\n\n## Exercise 8.5 {-}\n\n`shade` is a categorical variable of factor codes but let us (incorrectly) treat it as numerical (and if the actual distances are given, then the `distance` variable becomes a covariate). Fit ANCOVA of yield on the `inoculum` factor and `shade` covariate.\n\n\n**R:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# your code goes here\n```\n:::\n\n\n\n\n## Non-parametric ANOVA\n\nMann-Whitney test: allows to compare 2 groups under the non-normality assumption.\n\nKruskal-Wallis test: allows to compare three or more groups\n\n\n## Return to the Insect Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(count ~ spray, data=IS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\n   aov(formula = count ~ spray, data = IS)\n\nTerms:\n                   spray Residuals\nSum of Squares  2668.833  1015.167\nDeg. of Freedom        5        66\n\nResidual standard error: 3.921902\nEstimated effects may be unbalanced\n```\n:::\n\n```{.r .cell-code}\naov(count ~ spray, data=IS) |> TukeyHSD()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = count ~ spray, data = IS)\n\n$spray\n           diff        lwr       upr     p adj\nB-A   0.8333333  -3.866075  5.532742 0.9951810\nC-A -12.4166667 -17.116075 -7.717258 0.0000000\nD-A  -9.5833333 -14.282742 -4.883925 0.0000014\nE-A -11.0000000 -15.699409 -6.300591 0.0000000\nF-A   2.1666667  -2.532742  6.866075 0.7542147\nC-B -13.2500000 -17.949409 -8.550591 0.0000000\nD-B -10.4166667 -15.116075 -5.717258 0.0000002\nE-B -11.8333333 -16.532742 -7.133925 0.0000000\nF-B   1.3333333  -3.366075  6.032742 0.9603075\nD-C   2.8333333  -1.866075  7.532742 0.4920707\nE-C   1.4166667  -3.282742  6.116075 0.9488669\nF-C  14.5833333   9.883925 19.282742 0.0000000\nE-D  -1.4166667  -6.116075  3.282742 0.9488669\nF-D  11.7500000   7.050591 16.449409 0.0000000\nF-E  13.1666667   8.467258 17.866075 0.0000000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(IS$count~IS$spray)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  IS$count by IS$spray\nKruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10\n```\n:::\n:::\n\n\n## Return to the Oyster Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(Growth~Salinity, data = oys)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\n   aov(formula = Growth ~ Salinity, data = oys)\n\nTerms:\n                Salinity Residuals\nSum of Squares   0.16378  81.11150\nDeg. of Freedom        1        58\n\nResidual standard error: 1.182571\nEstimated effects may be unbalanced\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(oys$Growth~oys$Salinity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  oys$Growth by oys$Salinity\nKruskal-Wallis chi-squared = 0.013989, df = 1, p-value = 0.9058\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(oys$Growth~oys$Salinity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon rank sum exact test\n\ndata:  oys$Growth by oys$Salinity\nW = 458, p-value = 0.9124\nalternative hypothesis: true location shift is not equal to 0\n```\n:::\n:::\n\n\n\n+ More R code examples are [here](../exercises/Chap8more.R)",
    "supporting": [
      "ws08_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}