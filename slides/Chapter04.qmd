---
title: "Chapter 4:<br>Introduction to Statistical Inference"
image: img/inference.png
format: 
  revealjs:
    width: 1050
    height:	700
    scrollable: true
    transition: fade
    theme: [default, myquarto.scss]
    slide-number: c/t  
    logo: img/L_Color.png
    footer: "[161250 Data Analysis](https://anhsmith.quarto.pub/161250-data-analysis/slides.html)"
    styles:
      - revealjs.dark:
        background-color: #222
        color: #fff
    
execute:
  echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = FALSE, 
                      include=TRUE, 
                      message=FALSE, 
                      comment = NA, 
                      warn=-1, 
                      warn.conflicts = FALSE, 
                      quietly=TRUE, 
                      fig.align="center")
library(tidyverse)
library(patchwork)
theme_set(theme_minimal())
```

## Statistical Inference

::::{.columns}

:::{.column width="65%"}

The term *statistical inference* means that we are making estimates and/or claims about the population of interest based on sample data.

In other words, can we generalise the patterns seen in the sample data or not is the issue.

Recall that the the variability sample data depends on the population variability as well as the size of the sample.

So sample estimates are affected by the size of the sample in particular.

Confidence intervals and test of hypotheses are popular tools for statistical inference.

:::

:::{.column width="35%"}

![](img/sampling_variation2.gif)

:::

::::

## Normality tests and EDA

-   EDA approaches

    -   normal quantile plot, Boxplots,mids vs. spreads plot etc.

-   Hypothesis tests

    -   Kolmogorov-Smirnov test (based on the biggest difference between
        the empirical and theoretical cumulative distributions)
    -   Shapiro-Wilk test (based on variance of the difference)

-   Example: N(100,1) random data of size $n=50$

    -   **The null hypothesis of normality must be justified on
        empirical grounds**

\small

```{r, warning=FALSE, message=FALSE}
set.seed(1234)
shapiro.test(rnorm(50, mean=100))
ks.test(rnorm(50), "pnorm")
```

## Example

```{r, warning=FALSE, fig.height=3, fig.width=4}
tv = read_csv("https://www.massey.ac.nz/~anhsmith/data/tv.csv")

p1 <- ggplot(tv, aes(sample = TELETIME)) + 
  stat_qq() + stat_qq_line()
p1 + 
  labs(title = "Normal quantile plot for TV viewing times") 
```

## Sampling distributions

The term *sampling distribution* means the distribution of the computed
statistic such as the sample mean when sampling is repeated many times.

For a normal population,

-   Student's $t$ distribution is the sampling distribution of the mean
    (after rescaling).

-   $\chi^2$ distribution is the sampling distribution of the sample
    variance $S^2$.

-   $(n-1)S^2/\sigma^2$ follows $\chi^2$ distribution.

$F$ distribution is ratio of two $\chi^2$ distributions.

-   It becomes the sampling distribution of the ratio of two sample
    variances $S_1^2/S_2^2$ from two normal populations (after scaling).

$t$ distribution is symmetric but $\chi^2$ and $F$ distributions are
right skewed. - For large samples, they become normal - For $n>30$, the
skew will diminish

For the three sampling distributions, the sample size $n$ becomes the
proxy parameter, called the degrees of freedom (df).

-   $t_{n-1}$, $\chi_{n-1}^2$ & $F_{(n_1-1),(n_2-1) }$

## Student's $t$ distribution

-   Sample means from normal population are normally distributed ie.
    $\bar{X} \sim N(\mu, \frac {\sigma}{\sqrt{n}})$
-   Even when the population is not normal,
    $\bar{X} \sim N(\mu, \frac {\sigma}{\sqrt{n}})$ holds for large $n$
    (Central Limit Theorem)
-   When $\sigma$ is unknown (which true for small sample sizes), the
    distribution of $\left(\frac{\bar{x}-\mu}{S/\sqrt{n}} \right)$ is no
    longer normal unlike the distribution of
    $\left(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}} \right) \sim N(0, 1)$
-   The distribution of the statistic
    $t=\left(\frac{\bar{x}-\mu}{S/\sqrt{n}} \right)$ is called the
    Student's t-distribution with $(n-1)$ degrees of freedom.
-   Student's t-distribution is symmetric but more spread than the
    standard normal.

## Density curves

$t_2$ density

```{r, fig.height=3, fig.width=4}
library(tidyverse)
p <-ggplot(data.frame(x = c(-4, 4)), aes(x = x))+xlab("t")+ylab("density")
p <- p+geom_vline(xintercept = 0)
p1 <- p + stat_function(fun = dnorm, args=list(mean=0, sd=1))
p1 <- p1 + stat_function(fun = dt, args=list(df=2), colour = "red")
p1 <- p1 + labs(title = "Student's t with 2 df compared with standard normal")
p1+theme_minimal()
```

$\chi_{5}^2$ density

```{r, fig.height=3, fig.width=4}
library(tidyverse)
p <-ggplot(data.frame(x = c(0, 25)), aes(x = x))+xlab("ChiSq")+ylab("density")
p <- p+geom_vline(xintercept = 0)
p1 <- p + stat_function(fun = dchisq, args=list(df=5), colour = "red")
p1 <- p1 + labs(title = "ChiSq density for 5 df")
p1+theme_minimal()
```

$F_{2, 10}$ density

```{r, fig.height=3, fig.width=4}
library(tidyverse)
p <-ggplot(data.frame(x = c(0, 10)), aes(x = x))+xlab("F")+ylab("density")
p <- p+geom_vline(xintercept = 0)
p1 <- p + stat_function(fun = df, args=list(df1=2,df2=10), colour = "red")
p1 <- p1 + labs(title = "F density for (2,10) dfs")
p1+theme_minimal()
```

<https://shiny.massey.ac.nz/anhsmith/demos/demo.critical.values/>

## Confidence interval for the population mean

-   A confidence interval (CI) is nothing but an interval estimate of an
    unknown population quantity

    -   CI is of the form `estimate` $\pm$ `margin of error`

-   Student's t quantiles are used to construct the confidence interval
    for the population mean as $\bar{y}$ $\pm$ $t \times (S/\sqrt n)$

    -   $S/\sqrt n$ is the `estimated standard error` of the mean

```{r}
library(tidyverse)
p1 <-ggplot(data.frame(x = c(-4, 4)), aes(x = x))+xlab("t")+ylab("density")
p1 <- p1+geom_vline(xintercept = 0)
p1 <- p1 + stat_function(fun = dt, args=list(df=10), colour = "red")
p1 <- p1 + labs(title = "Student's t quantiles- 0.025 & 0.975 for 10 df")
lowert <- qt(.025, 10)
p1 <- p1+geom_vline(xintercept=lowert)
uppert <- qt(.975, 10)
p1 <- p1+geom_vline(xintercept=uppert)
p1+theme_minimal()

```

## One-sample t-test

-   Null hypothesis $H_0:\mu=\mu_0$. Two-sided Alternative hypothesis
    $H_1:\mu \neq \mu_0$

-   Compute the Student's t-statistic
    $t=\left(\frac{\bar{x}-\mu_0}{S/\sqrt{n}} \right)$

-   Obtain the P value from n-1 degrees of freedom

    -   P (Probability) value is the tail area above $t$ and below $-t$

-   If the P value is below the set significance level $\alpha$, reject
    the null hypothesis

-   <https://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/>

-   Learn how to perform t-tests in R

    -   function `t.test`

## Concept of power

A hypothesis test has a certain **power** (probability) to reject the
null hypothesis when it is false.

The power of the t test can be evaluated using R for given effect size
$\delta$

```{r, echo=TRUE}
power.t.test(n = 30, delta = 1, sd = 1, sig.level = 0.05)
power.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)
```

## Two sample t-test

-   In a two-sample t-test, the equality of two population means is
    tested. Null hypothesis $H_0:\mu=\mu_1=\mu_2$. Two-sided Alternative
    hypothesis $H_1:\mu_1 \neq \mu_2$

-   If EDA suggests that the two populations have the same spread, we
    perform a `pooled t-test` in which the variance common to the two
    population is estimated as
    $S_{p}^{2} = w_{1} S_{1}^{2} +w_{2} S_{2}^{2}$ where the *weights*
    are $w_{1} =\frac{n_{1}-1}{n_{1} +n_{2}-2}$ and
    $w_{2} =\frac{n_{2}-1}{n_{1} +n_{2}-2}$

-   For the `pooled` case, the $df$ for the $t$-test is $n_{1}+n_{2}-2$
    but becomes smaller for the `unpooled` case to
    $$df=\frac{\left(\frac{S_{1}^{2}}{n_{1}} +\frac{S_{2}^{2} }{n_{2}} \right)^{2} }{\frac{1}{n_{1} -1} \left(\frac{S_{1}^{2}}{n_{1}}\right)^{2} +\frac{1}{n_{2} -1} \left(\frac{S_{2}^{2}}{n_{2} } \right)^{2}} $$

## Validity of equal variance assumption {.smaller}

-   Equal variance assumption is found plausible in the following
    output:

```{r}
bartlett.test(TELETIME~SEX, data=tv)
car::leveneTest(TELETIME~factor(SEX), data=tv)
```

## Paired t-test

-   Paired t-test is nothing but a one-sample t-test on the individual
    differences. That is, we test whether the true mean of the paired
    differences is zero.
-   Paired t-test requires a common pairing variable to relate the two
    population elements.
-   The two sample data must be correlated somewhat highly.
-   If the correlation is low or negative, the paired t-test is less
    powerful (so use the usual two-sample t-test).
-   Eg. studying twins; left and right legs (eyes) etc.
-   Testing equality of mean IQ scores of twins

```{r}
# twins <- read.table("http://www.massey.ac.nz/~anhsmith/data/twins.txt", header=T)

load("../data/twins.RData")
attach(twins)
t.test(x=A, y=B, paired=TRUE)
```

## Shiny apps (not currently working)

<https://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/>

<https://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/>

<https://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/>

-   Covers non-parametric tests, see Study Guide.

## Test of proportions

Testing for a single proportion

A sample survey of size 1000 has had 450 females.

-   Can we treat the survey as unbiased?

```{r, echo=TRUE}
prop.test(450, 1000)
```

An exact version of the test

```{r}
binom.test(c(450, 550), p = 3/4)
```

## Comparing several proportions

Fleiss (1981) *Statistical methods for rates and proportions* data on
smokers in four group of patients

```{r, echo=TRUE}
smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
```

## Transformations

-   Transformations re-express the data

-   The linear transformation $Y^*= a+bY$ result only in a change of
    scale or of origin

-   A linear transformation does not change the shape of the
    distribution i.e. histogram, boxplot etc. remain the same shape

-   We prefer non-linear transformations which alter the shape of the
    distribution

-   Transformations are needed to-

    -   to bring symmetry
    -   normality
    -   stabilise the variance etc

## Example

-   Right skewed distribution is made roughly symmetric using a log
    transformation for no. of vehicles variable (rangitikei.\* dataset)

```{r}
load("../data/rangitikei.Rdata")
p1 <- ggplot(rangitikei, aes(vehicle))+geom_boxplot()
p1 <- p1+labs(title = "Boxplot of Vehicle (Raw data)")
p2 <- ggplot(rangitikei, aes(vehicle^2))+geom_boxplot()
p2 <- p2+labs(title = "Boxplot of Vehicle (Squared data)")
p3 <- ggplot(rangitikei, aes(sqrt(vehicle)))+geom_boxplot()
p3 <- p3+labs(title = "Boxplot of Vehicle (Square Root data)")
p4 <- ggplot(rangitikei, aes(log(vehicle)))+geom_boxplot()
p4 <- p4+labs(title = "Boxplot of Vehicle (log data)")
library(patchwork)
p2+p1+p3+p4  
```

## A Ladder of Powers for Transforming Data

-   Right skewed data needs a shrinking transformation
-   Left skewed data needs a stretching transformation
-   The strength or power of the transformation depends on the degree of
    skew.\

| POWER | Formula               | Name            | Result                 |
|:------|:----------------------|:----------------|:-----------------------|
| 3     | $x^3$                 | cube            | stretches large values |
| 2     | $x^2$                 | square          | stretches large values |
| 1     | $x$                   | raw             | No change              |
| 1/2   | $\sqrt{x}$            | square root     | squashes large values  |
| 0     | $\log{x}$             | logarithm       | squashes large values  |
| -1/2  | $\frac{-1}{\sqrt{x}}$ | reciprocal root | squashes large values  |
| -1    | $\frac{-1}{x}$        | reciprocal      | squashes large values  |

## D-statistics

-   D-statistics are used for checking the adequacy of a transformation

-   Symmetry implies mean = median

-   A simple approach is to plot the distance from the median for points
    below and above the median.

```{r}
symmetryplot = function(x)
{
  m = length(x) %/% 2
  sx = sort(x)
  dat1=cbind.data.frame(x=median(x) - sx[1:m],y=rev(sx)[1:m] - median(x))
  p1=ggplot(dat1, aes(x=x,y=y))+geom_point()+xlab("Distance below median")+ylab("Distance above median")
  p1+ geom_abline(intercept=0, slope=1)
}

symmetryplot(rangitikei$vehicle)

```

-   (mean-median) measures skew but we can standardise it in 3 ways

    -   D1 = \|mean-median\| / std.dev.
    -   D2 = \|mean-median\| / F-spread
    -   D3 = \|mid_F-median\| / F-spread

-   D3 - sensitive to skewness in the middle 50% of data.\

-   D1 & D2 - sensitive to skewness in whole data

-   Smaller the D value, the better the transformation for symmetry

```{r}
D1 = function(x) {(mean(x) - median(x))/sd(x)}
D2 = function(x) {(mean(x) - median(x))/(fivenum(x)[4]-fivenum(x)[2])}
D3 = function(x) {((fivenum(x)[4]+fivenum(x)[2])/2 - median(x))/(fivenum(x)[4]-fivenum(x)[2])}
c(D1=D1(rnorm(30)),D2=D2(rnorm(30)),D3=D3(rnorm(30)))
```

## Box-Cox transformation

-   This is a normalising transformation (i.e more than just symmetry)
    but we may not always find a suitable Box-Cox power $\lambda$.
-   R gives a point estimate of power & the CI.

```{r}
# library(MASS)
# boxcox(rangitikei$vehicle ~ 1)
# title("Log-likelihood curve of boxcox parameter")
library(lindia)
gg_boxcox(lm(rangitikei$vehicle ~ 1))
```

-   Focus on the confidence interval for the estimated $\lambda$ and it
    may be wider

## Statistical Inference Based on Transformed data

-   Confidence intervals found as `estimate` $\pm$ `margin of error`
    require symmetry.

-   Obtain the confidence interval using transformed data and then back
    transform the limits (to keep the original scale)

    -   The confidence limits of log transformed data can be
        exponentiated back (ie. $e^{\text{confidence limit}}$)
    -   The confidence limits of square-root transformed data can be
        squared back (ie. ${\text{confidence limit}^2}$)

-   For hypothesis test, apply the same transformation on the value
    hypothesised under the null

-   Explore
    https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/
    app (not currently working)

-   Example: Brain weight data is extremely skewed to the right. So rely
    on the back-transformed CI.

```{r}
load("../data/brainweight.RData")
attach(brainweight)
t.test(BrainWt)$conf.int[1:2]
t.test(log(BrainWt))$conf.int[1:2]
exp(t.test(log(BrainWt))$conf.int)[1:2]
```

## A Caution on the transformation technique

-   Perform EDA in addition to computing coefficients of skewness or
    D-Statistics etc
-   Small datasets may reveal skewness (So ignore)
-   Avoid a transformation if marginal improvement happens.
-   Transformed data may be difficult to interpret
-   Try other options such as subgrouping etc.
-   Normalising transformations are different from transformations for
    symmetry
-   Read the section from your SG

## Variance stabilisation

-   Two or more batches of data may not have equal variance
-   If each batch is skewed in the same direction, then a common
    transformation can be used.
-   If batches are skewed in different directions, the transformation
    technique fails
-   Explore LOCATION vs SPREAD plot of batches.
-   If there exists a relationship between spreads and averages, a
    single transformation can be applied for all the batches.

## Example

-   Common shrinking transformation can be found in the following case

<!-- ```{r} -->

<!-- library(tidyverse) -->

<!-- hospital <- read.table("https://www.massey.ac.nz/~anhsmith/data/hospital.txt", header=TRUE, sep=",") -->

<!-- dat <- hospital %>% group_by(YEAR) %>%  -->

<!--   pivot_longer(cols =3:8, names_to = "Region", values_to = "Admissions") -->

<!-- dat %>% group_by(interaction(YEAR, Region)) %>%  -->

<!--   summarise(medians=median(Admissions), ranges=max(Admissions)-min(Admissions)) ->dat -->

<!-- p1 <- ggplot(dat, aes(x=medians, y=ranges))+geom_point() -->

<!-- p1 <- p1 +geom_smooth(method="lm", se=FALSE) -->

<!-- p1 <- p1 +labs(title = "Range vs Median Plot") -->

<!-- p1 -->

<!-- ``` -->

![](img/hospital_rangevmedian.png)

## Shiny apps

<https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/>

<https://shiny.massey.ac.nz/anhsmith/demos/perform.transformations/>

## Non-parametric tests

An alternative to using power transformations

Relies on replacing the actual observed data by their ranks

**Spearman's Rank Correlation**

-   Rank the $X$ and $Y$ variables, and then obtain usual Pearson
    correlation coefficient

-   The following plot shows both (Spearman correlation in the upper
    triangle)

```{r spear, echo=FALSE, fig.cap="Comparison of Pearsonian and Spearman's rank correlations"}
library(GGally)
ggpairs(trees, upper = list(continuous = wrap('cor', method = "spearman")), 
  lower = list(continuous = 'cor'))
```

## Wilcoxon signed rank test

A non-parametric alternative to the one-sample t-test

-   $H_0: \eta=\eta_0$ where $\eta$ (Greek letter 'eta') is the
    population median

-   Based on based on ranking $(|Y-\eta_0|)$, where the ranks for data
    with $Y<\eta_0$ are compared to the ranks for data with $Y>\eta_0$

```{r}
wilcox.test(tv$TELETIME, mu=1680, conf.int=T)
```

```{r}
t.test(tv$TELETIME, mu=1680)
```

## Mann-Whitney test

For two group comparison, pool the two group responses and then rank the
pooled data

Ranks for the first group are compared to the ranks for the second group

-   The null hypothesis is that the two group medians are the same:
    $H_0: \eta_1=\eta_2$.

```{r}
wilcox.test(rangitikei$people~rangitikei$time, conf.int=T)
```

```{r}
t.test(rangitikei$people~rangitikei$time)
```

## Another form of test

```{r, echo=TRUE}
kruskal.test(rangitikei$people~rangitikei$time)
```

```{r, echo=TRUE}
wilcox.test(rangitikei$people~rangitikei$time)
```

## Permutation tests

A permutation (or randomisation) test is based on the idea of randomly
permuting the observed data and then answering whether a hypothesis is
negated or not.

One sample hypothesis test example follows:

```{r, echo=TRUE}
library(exactRankTests)
perm.test(tv$TELETIME, null.value=1500)
```

For small samples, this test is not powerful.

## Two group comparison

```{r, echo=TRUE}
perm.test(TELETIME~SEX, distribution ='exact', data=tv)
```

Also using a linear model fit (cover later)

```{r, echo=TRUE}
library(lmPerm)
summary(lmp(TELETIME~SEX, data=tv))
```

Read the study guide example for bootstrap tests (not examined)

## Summary

Inference is less complicated for normal populations

-   Assessment of normality is particularly important for small sample
    sizes
-   Student t-tests are generally robust and can be used for non-normal
    populations (as long as there are no subgrouping or sample sizes are
    large)

Power transformations aim to achieve symmetry

-   Box-Cox transformations aim to normalise the data
-   Improved Inference can be made
-   Subgrouping effect can be dampened

Permutation tests can be done for a second opinion

<!-- ## Exercises -->

<!-- download.file("<https://www.massey.ac.nz/~kgovinda/220exer/Chap3moreexamples.R>", destfile="Chap3moreexamples.R") -->

<!-- download.file("<https://www.massey.ac.nz/~kgovinda/220exer/chapter-3-exercises.html>", destfile="chapter-3-exercises.html") -->

<!-- install.packages("remotes") -->

<!-- remotes::install_github("ricompute/ricomisc") -->

<!-- ricomisc::rstudio_viewer("chapter-3.html", file_path = NULL) -->
